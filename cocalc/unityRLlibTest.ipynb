{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from ray import tune\n",
    "import gym, ray\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "from gym_unity.envs import UnityEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class myBalanceBall(gym.Env):\n",
    "    \n",
    "    currentWorker = 0\n",
    "    \n",
    "    def __init__(self, env_config):\n",
    "        myBalanceBall.currentWorker += 1\n",
    "        \n",
    "        self.env_name = \"/Users/mettinger/github/myMLAgentsTest/3DBallSingleAgent\"\n",
    "        self.unityEnv = UnityEnv(self.env_name, worker_id=myBalanceBall.currentWorker, use_visual=False)\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low= -np.inf, high=np.inf, shape=(8,))\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.unityEnv.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.unityEnv.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 0\n",
    "config[\"eager\"] = False\n",
    "trainer = ppo.PPOTrainer(config=config, env=myBalanceBall)\n",
    "\n",
    "# Can optionally call trainer.restore(path) to load a checkpoint.\n",
    "\n",
    "for i in range(10):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config={\n",
    "        \"env\": myBalanceBall,\n",
    "        \"num_workers\": 0,\n",
    "        \"eager\": False,\n",
    "        \"env_config\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 08:46:58,757\tINFO resource_spec.py:212 -- Starting Ray with 2.83 GiB memory available for workers and up to 1.44 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-02-19 08:46:59,126\tINFO services.py:1093 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03,194\tINFO trainer.py:370 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03,198\tINFO trainer.py:517 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] Color LCD preferred device: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] Metal devices available: 2\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] 0: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] 1: Intel(R) UHD Graphics 630 (low power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.507 Unity Environment[46679:3758436] Using device AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] Color LCD preferred device: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] Metal devices available: 2\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] 0: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] 1: Intel(R) UHD Graphics 630 (low power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.433 Unity Environment[46684:3758658] Using device AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:15,555\tINFO trainable.py:178 -- _setup took 12.357 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:15,555\tWARNING util.py:41 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m /anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m /anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-47-25\n",
      "  done: false\n",
      "  episode_len_mean: 16.666666666666668\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.5666666900118191\n",
      "  episode_reward_min: 1.2000000178813934\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 73.5951582590739\n",
      "      mean_inference_ms: 4.293632507324219\n",
      "      mean_processing_ms: 6.856815020243327\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 100\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 110.345\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.853333333333335\n",
      "    ram_util_percent: 67.25333333333334\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 93.71667805284557\n",
      "    mean_inference_ms: 1.4774893770123472\n",
      "    mean_processing_ms: 6.357089127644454\n",
      "  time_since_restore: 10.38307499885559\n",
      "  time_this_iter_s: 10.38307499885559\n",
      "  time_total_s: 10.38307499885559\n",
      "  timestamp: 1582120045\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.3831</td><td style=\"text-align: right;\">        100</td><td style=\"text-align: right;\"> 1.56667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 15.23076923076923\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4230769442824216\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 13\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 13.0\n",
      "    episode_reward_max: 1.2000000178813934\n",
      "    episode_reward_mean: 1.2000000178813934\n",
      "    episode_reward_min: 1.2000000178813934\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 73.67811092110567\n",
      "      mean_inference_ms: 3.171848696331645\n",
      "      mean_processing_ms: 7.138363150663154\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9902\n",
      "    min_exploration: 0.9902\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 200\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.488\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.158823529411762\n",
      "    ram_util_percent: 67.44117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.008493786086\n",
      "    mean_inference_ms: 1.2327640906427937\n",
      "    mean_processing_ms: 6.598586157812763\n",
      "  time_since_restore: 20.741326093673706\n",
      "  time_this_iter_s: 10.358251094818115\n",
      "  time_total_s: 20.741326093673706\n",
      "  timestamp: 1582120057\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         20.7413</td><td style=\"text-align: right;\">        200</td><td style=\"text-align: right;\"> 1.42308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 15.105263157894736\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4105263368079537\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 19\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 75.02711829492601\n",
      "      mean_inference_ms: 2.4929087040788036\n",
      "      mean_processing_ms: 6.9102271128509\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9804\n",
      "    min_exploration: 0.9804\n",
      "    num_steps_sampled: 300\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 300\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.429\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.90625\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.16545367284678\n",
      "    mean_inference_ms: 1.1133140577033256\n",
      "    mean_processing_ms: 6.610006894394021\n",
      "  time_since_restore: 31.012473106384277\n",
      "  time_this_iter_s: 10.271147012710571\n",
      "  time_total_s: 31.012473106384277\n",
      "  timestamp: 1582120069\n",
      "  timesteps_since_restore: 300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 300\n",
      "  training_iteration: 3\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         31.0125</td><td style=\"text-align: right;\">        300</td><td style=\"text-align: right;\"> 1.41053</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 15.23076923076923\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4230769442824216\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 26\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 75.61818006876352\n",
      "      mean_inference_ms: 2.1341588046099687\n",
      "      mean_processing_ms: 6.738272873131004\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9706\n",
      "    min_exploration: 0.9706\n",
      "    num_steps_sampled: 400\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 400\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.324\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.476470588235294\n",
      "    ram_util_percent: 67.39411764705883\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.34998775555238\n",
      "    mean_inference_ms: 1.021942199046291\n",
      "    mean_processing_ms: 6.597079214092687\n",
      "  time_since_restore: 41.384133100509644\n",
      "  time_this_iter_s: 10.371659994125366\n",
      "  time_total_s: 41.384133100509644\n",
      "  timestamp: 1582120080\n",
      "  timesteps_since_restore: 400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 400\n",
      "  training_iteration: 4\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.3841</td><td style=\"text-align: right;\">        400</td><td style=\"text-align: right;\"> 1.42308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 14.545454545454545\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3545454747297547\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 33\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 75.90126455499885\n",
      "      mean_inference_ms: 1.8781088711170668\n",
      "      mean_processing_ms: 6.74797711747416\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9608\n",
      "    min_exploration: 0.9608\n",
      "    num_steps_sampled: 500\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 500\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.475\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.1625\n",
      "    ram_util_percent: 67.3625\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.46542313888669\n",
      "    mean_inference_ms: 0.9588428071520539\n",
      "    mean_processing_ms: 6.618152570314233\n",
      "  time_since_restore: 51.75628614425659\n",
      "  time_this_iter_s: 10.372153043746948\n",
      "  time_total_s: 51.75628614425659\n",
      "  timestamp: 1582120092\n",
      "  timesteps_since_restore: 500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 500\n",
      "  training_iteration: 5\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         51.7563</td><td style=\"text-align: right;\">        500</td><td style=\"text-align: right;\"> 1.35455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: some intermediate output was truncated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 642 intermediate output messages were discarded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4c…"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         3304.99</td><td style=\"text-align: right;\">      32700</td><td style=\"text-align: right;\">    4.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    config={\n",
    "        \"env\": myBalanceBall,\n",
    "        \"num_workers\": 0,\n",
    "        \"eager\": False,\n",
    "        \"Q_model\": {\"hidden_activation\": \"relu\", \"hidden_layer_sizes\": [256, 256]},\n",
    "        \"policy_model\":{\"hidden_activation\": \"relu\", \"hidden_layer_sizes\": [256, 256]},\n",
    "        \"env_config\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}