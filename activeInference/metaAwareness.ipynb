{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"bb9c4339ca0a487d95b88cd58aebf954","deepnote_cell_type":"text-cell-h3","formattedRanges":[{"fromCodePoint":145,"ranges":[],"toCodePoint":198,"type":"link","url":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8396119/"}]},"source":["### \"Towards a computational phenomenology of mental action: modelling meta-awareness and attentional control with deep parametric active inference\" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8396119/"]},{"cell_type":"markdown","metadata":{"cell_id":"d3508621cf4a464583d1c68a4d7e155a","deepnote_cell_type":"image","deepnote_img_src":"image-20231012-092223.png"},"source":["<img src=\"./image-20231012-092223.png\" width=\"\" align=\"\" />"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"9c9e51567eb74ca1954ed95b1c561748","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9879,"execution_start":1697116521351,"source_hash":null},"outputs":[],"source":["import pymdp\n","from pymdp import utils\n","from pymdp.agent import Agent"]},{"cell_type":"markdown","metadata":{"cell_id":"5574fbee9a724611884cbd0606bb87f9","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Example code for PyMDP"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"0cf62b94708b4834be49039c01bad2dd","deepnote_cell_type":"code"},"outputs":[],"source":["\"\"\" Quickly build a random active inference agent, give it an observation and have it do hidden state and policy inference \"\"\"\n","\n","\n","num_obs = [3, 5] # observation modality dimensions\n","num_states = [4, 2, 3] # hidden state factor dimensions\n","num_controls = [4, 1, 1] # control state factor dimensions\n","A_array = utils.random_A_matrix(num_obs, num_states) # create sensory likelihood (A matrix)\n","B_array = utils.random_B_matrix(num_states, num_controls) # create transition likelihood (B matrix)\n","\n","C_vector = utils.obj_array_uniform(num_obs) # uniform preferences\n","\n","# instantiate a quick agent using your A, B and C arrays\n","my_agent = Agent( A = A_array, B = B_array, C = C_vector)\n","\n","# give the agent a random observation and get the optimized posterior beliefs\n","\n","observation = [1, 4] # a list specifying the indices of the observation, for each observation modality\n","\n","qs = my_agent.infer_states(observation) # get posterior over hidden states (a multi-factor belief)\n","\n","# Do active inference\n","\n","q_pi, neg_efe = my_agent.infer_policies() # return the policy posterior and return (negative) expected free energies of each policy as well\n","\n","action = my_agent.sample_action() # sample an action from the posterior over policies"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_full_width":false,"deepnote_notebook_id":"1ceebc868aec4223a0250344b8f81ee9","kernelspec":{"display_name":"pymdp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
