{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yjXtUHtIlbd",
    "outputId": "33c40a05-03e4-4812-fea5-d8acfc05d66b"
   },
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as drv\n",
    "drv.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yjXtUHtIlbd",
    "outputId": "33c40a05-03e4-4812-fea5-d8acfc05d66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device query (PyCUDA version) \n",
      "\n",
      "Detected 1 CUDA Capable device(s) \n",
      "\n",
      "Device 0: GeForce RTX 2060\n",
      "\t Compute Capability: 7.5\n",
      "\t Total Memory: 6144 megabytes\n",
      "\t ASYNC_ENGINE_COUNT: 3\n",
      "\t CAN_MAP_HOST_MEMORY: 1\n",
      "\t CLOCK_RATE: 1200000\n",
      "\t COMPUTE_CAPABILITY_MAJOR: 7\n",
      "\t COMPUTE_CAPABILITY_MINOR: 5\n",
      "\t COMPUTE_MODE: DEFAULT\n",
      "\t CONCURRENT_KERNELS: 1\n",
      "\t ECC_ENABLED: 0\n",
      "\t GLOBAL_L1_CACHE_SUPPORTED: 1\n",
      "\t GLOBAL_MEMORY_BUS_WIDTH: 192\n",
      "\t GPU_OVERLAP: 1\n",
      "\t INTEGRATED: 0\n",
      "\t KERNEL_EXEC_TIMEOUT: 1\n",
      "\t L2_CACHE_SIZE: 3145728\n",
      "\t LOCAL_L1_CACHE_SUPPORTED: 1\n",
      "\t MANAGED_MEMORY: 1\n",
      "\t MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
      "\t MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
      "\t MAXIMUM_SURFACE1D_WIDTH: 32768\n",
      "\t MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
      "\t MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
      "\t MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
      "\t MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
      "\t MAXIMUM_SURFACE2D_WIDTH: 131072\n",
      "\t MAXIMUM_SURFACE3D_DEPTH: 16384\n",
      "\t MAXIMUM_SURFACE3D_HEIGHT: 16384\n",
      "\t MAXIMUM_SURFACE3D_WIDTH: 16384\n",
      "\t MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
      "\t MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
      "\t MAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
      "\t MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n",
      "\t MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURE1D_WIDTH: 131072\n",
      "\t MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
      "\t MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
      "\t MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
      "\t MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
      "\t MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
      "\t MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
      "\t MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
      "\t MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
      "\t MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURE2D_WIDTH: 131072\n",
      "\t MAXIMUM_TEXTURE3D_DEPTH: 16384\n",
      "\t MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
      "\t MAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
      "\t MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
      "\t MAXIMUM_TEXTURE3D_WIDTH: 16384\n",
      "\t MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
      "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
      "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
      "\t MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
      "\t MAX_BLOCK_DIM_X: 1024\n",
      "\t MAX_BLOCK_DIM_Y: 1024\n",
      "\t MAX_BLOCK_DIM_Z: 64\n",
      "\t MAX_GRID_DIM_X: 2147483647\n",
      "\t MAX_GRID_DIM_Y: 65535\n",
      "\t MAX_GRID_DIM_Z: 65535\n",
      "\t MAX_PITCH: 2147483647\n",
      "\t MAX_REGISTERS_PER_BLOCK: 65536\n",
      "\t MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
      "\t MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
      "\t MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
      "\t MAX_THREADS_PER_BLOCK: 1024\n",
      "\t MAX_THREADS_PER_MULTIPROCESSOR: 1024\n",
      "\t MEMORY_CLOCK_RATE: 7001000\n",
      "\t MULTI_GPU_BOARD: 0\n",
      "\t MULTI_GPU_BOARD_GROUP_ID: 0\n",
      "\t PCI_BUS_ID: 1\n",
      "\t PCI_DEVICE_ID: 0\n",
      "\t PCI_DOMAIN_ID: 0\n",
      "\t STREAM_PRIORITIES_SUPPORTED: 1\n",
      "\t SURFACE_ALIGNMENT: 512\n",
      "\t TCC_DRIVER: 0\n",
      "\t TEXTURE_ALIGNMENT: 512\n",
      "\t TEXTURE_PITCH_ALIGNMENT: 32\n",
      "\t TOTAL_CONSTANT_MEMORY: 65536\n",
      "\t UNIFIED_ADDRESSING: 1\n",
      "\t WARP_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "print('CUDA device query (PyCUDA version) \\n')\n",
    "\n",
    "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "    \n",
    "    gpu_device = drv.Device(i)\n",
    "    print('Device {}: {}'.format( i, gpu_device.name() ) )\n",
    "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
    "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
    "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))\n",
    "    \n",
    "    # The following will give us all remaining device attributes as seen \n",
    "    # in the original deviceQuery.\n",
    "    # We set up a dictionary as such so that we can easily index\n",
    "    # the values using a string descriptor.\n",
    "    \n",
    "    device_attributes_tuples = gpu_device.get_attributes().items() \n",
    "    device_attributes = {}\n",
    "    \n",
    "    for k, v in device_attributes_tuples:\n",
    "        device_attributes[str(k)] = v\n",
    "    \n",
    "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
    "    \n",
    "    # Cores per multiprocessor is not reported by the GPU!  \n",
    "    # We must use a lookup table based on compute capability.\n",
    "    # See the following:\n",
    "    # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
    "    \n",
    "    #cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 6.2 : 128}[compute_capability]\n",
    "    \n",
    "    #print '\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp)\n",
    "    \n",
    "    device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
    "    \n",
    "    for k in device_attributes.keys():\n",
    "        print('\\t {}: {}'.format(k, device_attributes[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBInvRU2Jayn",
    "outputId": "5e97d8df-e2bf-45d4-8f42-d7dc9bc6949e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does our kernel work correctly? : True\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "ker = SourceModule(\"\"\"\n",
    "__global__ void scalar_multiply_kernel(float *outvec, float scalar, float *vec)\n",
    "{\n",
    "     int i = threadIdx.x;\n",
    "     outvec[i] = scalar*vec[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "scalar_multiply_gpu = ker.get_function(\"scalar_multiply_kernel\")\n",
    "\n",
    "testvec = np.random.randn(512).astype(np.float32)\n",
    "testvec_gpu = gpuarray.to_gpu(testvec)\n",
    "outvec_gpu = gpuarray.empty_like(testvec_gpu)\n",
    "\n",
    "scalar_multiply_gpu( outvec_gpu, np.float32(2), testvec_gpu, block=(512,1,1), grid=(1,1,1))\n",
    "\n",
    "print(\"Does our kernel work correctly? : {}\".format(np.allclose(outvec_gpu.get() , 2*testvec) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLtT-apeBBmY",
    "outputId": "f8c04b70-1315-4e99-9e3e-fc5ac8178ba6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-b87deecb5186>:11: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\r\n",
      "\n",
      "  ker = SourceModule(\"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3.211999\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "num_arrays = 200\n",
    "array_len = 1024**2\n",
    "\n",
    "ker = SourceModule(\"\"\"       \n",
    "__global__ void mult_ker(float * array, int array_len)\n",
    "{\n",
    "     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     int num_iters = array_len / blockDim.x;\n",
    "\n",
    "     for(int j=0; j < num_iters; j++)\n",
    "     {\n",
    "         int i = j * blockDim.x + thd;\n",
    "\n",
    "         for(int k = 0; k < 50; k++)\n",
    "         {\n",
    "              array[i] *= 2.0;\n",
    "              array[i] /= 2.0;\n",
    "         }\n",
    "     }\n",
    "\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "mult_ker = ker.get_function('mult_ker')\n",
    "\n",
    "data = []\n",
    "data_gpu = []\n",
    "gpu_out = []\n",
    "\n",
    "# generate random arrays.\n",
    "for _ in range(num_arrays):\n",
    "    data.append(np.random.randn(array_len).astype('float32'))\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "# copy arrays to GPU.\n",
    "for k in range(num_arrays):\n",
    "    data_gpu.append(gpuarray.to_gpu(data[k]))\n",
    "\n",
    "# process arrays.\n",
    "for k in range(num_arrays):\n",
    "    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1))\n",
    "\n",
    "# copy arrays from GPU.\n",
    "for k in range(num_arrays):\n",
    "    gpu_out.append(data_gpu[k].get())\n",
    "\n",
    "t_end = time()\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    assert (np.allclose(gpu_out[k], data[k]))\n",
    "\n",
    "print('Total time: %f' % (t_end - t_start))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pyCudaTest.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
