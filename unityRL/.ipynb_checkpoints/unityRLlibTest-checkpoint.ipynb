{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from ray import tune\n",
    "import gym, ray\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "#sys.path.insert(0,'/Users/mettinger/github/ml-agents/gym-unity')\n",
    "\n",
    "from gym_unity.envs import UnityEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBalanceBall(gym.Env):\n",
    "    \n",
    "    currentWorker = 0\n",
    "    \n",
    "    def __init__(self, env_config):\n",
    "        myBalanceBall.currentWorker += 1\n",
    "        \n",
    "        self.env_name = \"/Users/mettinger/github/myMLAgentsTest/3DBallSingleAgent\"\n",
    "        self.unityEnv = UnityEnv(self.env_name, worker_id=myBalanceBall.currentWorker, use_visual=False)\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low= -np.inf, high=np.inf, shape=(8,))\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.unityEnv.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.unityEnv.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 0\n",
    "config[\"eager\"] = False\n",
    "trainer = ppo.PPOTrainer(config=config, env=myBalanceBall)\n",
    "\n",
    "# Can optionally call trainer.restore(path) to load a checkpoint.\n",
    "\n",
    "for i in range(10):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config={\n",
    "        \"env\": myBalanceBall,\n",
    "        \"num_workers\": 0,\n",
    "        \"eager\": False,\n",
    "        \"env_config\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-19 08:46:58,757\tINFO resource_spec.py:212 -- Starting Ray with 2.83 GiB memory available for workers and up to 1.44 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-02-19 08:46:59,126\tINFO services.py:1093 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03,194\tINFO trainer.py:370 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03,198\tINFO trainer.py:517 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] Color LCD preferred device: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] Metal devices available: 2\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] 0: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.506 Unity Environment[46679:3758436] 1: Intel(R) UHD Graphics 630 (low power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:03.507 Unity Environment[46679:3758436] Using device AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] Color LCD preferred device: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] Metal devices available: 2\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] 0: AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.432 Unity Environment[46684:3758658] 1: Intel(R) UHD Graphics 630 (low power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:08.433 Unity Environment[46684:3758658] Using device AMD Radeon Pro 555X (high power)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:15,555\tINFO trainable.py:178 -- _setup took 12.357 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m 2020-02-19 08:47:15,555\tWARNING util.py:41 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m /anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m /anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=46675)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-47-25\n",
      "  done: false\n",
      "  episode_len_mean: 16.666666666666668\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.5666666900118191\n",
      "  episode_reward_min: 1.2000000178813934\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 73.5951582590739\n",
      "      mean_inference_ms: 4.293632507324219\n",
      "      mean_processing_ms: 6.856815020243327\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 100\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 110.345\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.853333333333335\n",
      "    ram_util_percent: 67.25333333333334\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 93.71667805284557\n",
      "    mean_inference_ms: 1.4774893770123472\n",
      "    mean_processing_ms: 6.357089127644454\n",
      "  time_since_restore: 10.38307499885559\n",
      "  time_this_iter_s: 10.38307499885559\n",
      "  time_total_s: 10.38307499885559\n",
      "  timestamp: 1582120045\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.3831</td><td style=\"text-align: right;\">        100</td><td style=\"text-align: right;\"> 1.56667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 15.23076923076923\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4230769442824216\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 13\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 13.0\n",
      "    episode_reward_max: 1.2000000178813934\n",
      "    episode_reward_mean: 1.2000000178813934\n",
      "    episode_reward_min: 1.2000000178813934\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 73.67811092110567\n",
      "      mean_inference_ms: 3.171848696331645\n",
      "      mean_processing_ms: 7.138363150663154\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9902\n",
      "    min_exploration: 0.9902\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 200\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.488\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.158823529411762\n",
      "    ram_util_percent: 67.44117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.008493786086\n",
      "    mean_inference_ms: 1.2327640906427937\n",
      "    mean_processing_ms: 6.598586157812763\n",
      "  time_since_restore: 20.741326093673706\n",
      "  time_this_iter_s: 10.358251094818115\n",
      "  time_total_s: 20.741326093673706\n",
      "  timestamp: 1582120057\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         20.7413</td><td style=\"text-align: right;\">        200</td><td style=\"text-align: right;\"> 1.42308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 15.105263157894736\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4105263368079537\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 19\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 75.02711829492601\n",
      "      mean_inference_ms: 2.4929087040788036\n",
      "      mean_processing_ms: 6.9102271128509\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9804\n",
      "    min_exploration: 0.9804\n",
      "    num_steps_sampled: 300\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 300\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.429\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.90625\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.16545367284678\n",
      "    mean_inference_ms: 1.1133140577033256\n",
      "    mean_processing_ms: 6.610006894394021\n",
      "  time_since_restore: 31.012473106384277\n",
      "  time_this_iter_s: 10.271147012710571\n",
      "  time_total_s: 31.012473106384277\n",
      "  timestamp: 1582120069\n",
      "  timesteps_since_restore: 300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 300\n",
      "  training_iteration: 3\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         31.0125</td><td style=\"text-align: right;\">        300</td><td style=\"text-align: right;\"> 1.41053</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 15.23076923076923\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4230769442824216\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 26\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 75.61818006876352\n",
      "      mean_inference_ms: 2.1341588046099687\n",
      "      mean_processing_ms: 6.738272873131004\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9706\n",
      "    min_exploration: 0.9706\n",
      "    num_steps_sampled: 400\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 400\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.324\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.476470588235294\n",
      "    ram_util_percent: 67.39411764705883\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.34998775555238\n",
      "    mean_inference_ms: 1.021942199046291\n",
      "    mean_processing_ms: 6.597079214092687\n",
      "  time_since_restore: 41.384133100509644\n",
      "  time_this_iter_s: 10.371659994125366\n",
      "  time_total_s: 41.384133100509644\n",
      "  timestamp: 1582120080\n",
      "  timesteps_since_restore: 400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 400\n",
      "  training_iteration: 4\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.3841</td><td style=\"text-align: right;\">        400</td><td style=\"text-align: right;\"> 1.42308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 14.545454545454545\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3545454747297547\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 33\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 75.90126455499885\n",
      "      mean_inference_ms: 1.8781088711170668\n",
      "      mean_processing_ms: 6.74797711747416\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9608\n",
      "    min_exploration: 0.9608\n",
      "    num_steps_sampled: 500\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 500\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.475\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.1625\n",
      "    ram_util_percent: 67.3625\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.46542313888669\n",
      "    mean_inference_ms: 0.9588428071520539\n",
      "    mean_processing_ms: 6.618152570314233\n",
      "  time_since_restore: 51.75628614425659\n",
      "  time_this_iter_s: 10.372153043746948\n",
      "  time_total_s: 51.75628614425659\n",
      "  timestamp: 1582120092\n",
      "  timesteps_since_restore: 500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 500\n",
      "  training_iteration: 5\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         51.7563</td><td style=\"text-align: right;\">        500</td><td style=\"text-align: right;\"> 1.35455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-24\n",
      "  done: false\n",
      "  episode_len_mean: 14.675\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.367500020377338\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 40\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.31590252830868\n",
      "      mean_inference_ms: 1.682869593302409\n",
      "      mean_processing_ms: 6.683912731352306\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.951\n",
      "    min_exploration: 0.951\n",
      "    num_steps_sampled: 600\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 600\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.413\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.923529411764704\n",
      "    ram_util_percent: 67.50588235294117\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.57268896799386\n",
      "    mean_inference_ms: 0.9114965242594376\n",
      "    mean_processing_ms: 6.620900138555795\n",
      "  time_since_restore: 62.12733721733093\n",
      "  time_this_iter_s: 10.37105107307434\n",
      "  time_total_s: 62.12733721733093\n",
      "  timestamp: 1582120104\n",
      "  timesteps_since_restore: 600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 600\n",
      "  training_iteration: 6\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         62.1273</td><td style=\"text-align: right;\">        600</td><td style=\"text-align: right;\">  1.3675</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 14.617021276595745\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3617021479505174\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 47\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 13.0\n",
      "    episode_reward_max: 1.2000000178813934\n",
      "    episode_reward_mean: 1.2000000178813934\n",
      "    episode_reward_min: 1.2000000178813934\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.05356685185836\n",
      "      mean_inference_ms: 1.5675657886569783\n",
      "      mean_processing_ms: 6.799596851154909\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9412\n",
      "    min_exploration: 0.9412\n",
      "    num_steps_sampled: 700\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 700\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.387\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.000000000000002\n",
      "    ram_util_percent: 67.53529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.66951102551586\n",
      "    mean_inference_ms: 0.8743064613893715\n",
      "    mean_processing_ms: 6.615237790610122\n",
      "  time_since_restore: 72.4989185333252\n",
      "  time_this_iter_s: 10.371581315994263\n",
      "  time_total_s: 72.4989185333252\n",
      "  timestamp: 1582120115\n",
      "  timesteps_since_restore: 700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 700\n",
      "  training_iteration: 7\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         72.4989</td><td style=\"text-align: right;\">        700</td><td style=\"text-align: right;\">  1.3617</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 14.814814814814815\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3814815020671598\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 54\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.3632379361053\n",
      "      mean_inference_ms: 1.4536469729978647\n",
      "      mean_processing_ms: 6.751994588481846\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9314\n",
      "    min_exploration: 0.9314\n",
      "    num_steps_sampled: 800\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 800\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.395\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.96875\n",
      "    ram_util_percent: 67.375\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.75343926510197\n",
      "    mean_inference_ms: 0.8447716091954512\n",
      "    mean_processing_ms: 6.606533264355134\n",
      "  time_since_restore: 82.87094688415527\n",
      "  time_this_iter_s: 10.372028350830078\n",
      "  time_total_s: 82.87094688415527\n",
      "  timestamp: 1582120127\n",
      "  timesteps_since_restore: 800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 800\n",
      "  training_iteration: 8\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         82.8709</td><td style=\"text-align: right;\">        800</td><td style=\"text-align: right;\"> 1.38148</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 14.639344262295081\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3639344465537149\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 61\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.89112111141807\n",
      "      mean_inference_ms: 1.3737098166817112\n",
      "      mean_processing_ms: 6.627995716898065\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9216\n",
      "    min_exploration: 0.9216\n",
      "    num_steps_sampled: 900\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 900\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 108.543\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.54705882352941\n",
      "    ram_util_percent: 67.55882352941177\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.81170068316425\n",
      "    mean_inference_ms: 0.8214207001648767\n",
      "    mean_processing_ms: 6.609669866090844\n",
      "  time_since_restore: 93.24236488342285\n",
      "  time_this_iter_s: 10.371417999267578\n",
      "  time_total_s: 93.24236488342285\n",
      "  timestamp: 1582120139\n",
      "  timesteps_since_restore: 900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 900\n",
      "  training_iteration: 9\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         93.2424</td><td style=\"text-align: right;\">        900</td><td style=\"text-align: right;\"> 1.36393</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 14.82089552238806\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3820895728335452\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 67\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 13.0\n",
      "    episode_reward_max: 1.2000000178813934\n",
      "    episode_reward_mean: 1.2000000178813934\n",
      "    episode_reward_min: 1.2000000178813934\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.63135962052779\n",
      "      mean_inference_ms: 1.3399991122159092\n",
      "      mean_processing_ms: 6.72591816295277\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.9117999999999999\n",
      "    min_exploration: 0.9117999999999999\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1000\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.427\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.375\n",
      "    ram_util_percent: 67.51875000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.84205613116055\n",
      "    mean_inference_ms: 0.8075711552036242\n",
      "    mean_processing_ms: 6.609456213521353\n",
      "  time_since_restore: 103.52118182182312\n",
      "  time_this_iter_s: 10.278816938400269\n",
      "  time_total_s: 103.52118182182312\n",
      "  timestamp: 1582120150\n",
      "  timesteps_since_restore: 1000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 10\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         103.521</td><td style=\"text-align: right;\">       1000</td><td style=\"text-align: right;\"> 1.38209</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-49-22\n",
      "  done: false\n",
      "  episode_len_mean: 14.783783783783784\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3783783989178169\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 74\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 12.0\n",
      "    episode_reward_max: 1.1000000163912773\n",
      "    episode_reward_mean: 1.1000000163912773\n",
      "    episode_reward_min: 1.1000000163912773\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.27961864579196\n",
      "      mean_inference_ms: 1.315632782413461\n",
      "      mean_processing_ms: 6.848907740102649\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.902\n",
      "    min_exploration: 0.902\n",
      "    num_steps_sampled: 1100\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1100\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.065\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.335294117647056\n",
      "    ram_util_percent: 67.70588235294119\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.86196916704247\n",
      "    mean_inference_ms: 0.7959877470360828\n",
      "    mean_processing_ms: 6.616109901617463\n",
      "  time_since_restore: 113.89116597175598\n",
      "  time_this_iter_s: 10.369984149932861\n",
      "  time_total_s: 113.89116597175598\n",
      "  timestamp: 1582120162\n",
      "  timesteps_since_restore: 1100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1100\n",
      "  training_iteration: 11\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         113.891</td><td style=\"text-align: right;\">       1100</td><td style=\"text-align: right;\"> 1.37838</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-49-33\n",
      "  done: false\n",
      "  episode_len_mean: 14.9375\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3937500207684934\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 80\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.68678943927472\n",
      "      mean_inference_ms: 1.2733055995060847\n",
      "      mean_processing_ms: 6.745014435205704\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.8922\n",
      "    min_exploration: 0.8922\n",
      "    num_steps_sampled: 1200\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1200\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.11\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.943749999999998\n",
      "    ram_util_percent: 67.69375\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.87093813739867\n",
      "    mean_inference_ms: 0.788736951815558\n",
      "    mean_processing_ms: 6.618703039466487\n",
      "  time_since_restore: 124.1648600101471\n",
      "  time_this_iter_s: 10.273694038391113\n",
      "  time_total_s: 124.1648600101471\n",
      "  timestamp: 1582120173\n",
      "  timesteps_since_restore: 1200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1200\n",
      "  training_iteration: 12\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         124.165</td><td style=\"text-align: right;\">       1200</td><td style=\"text-align: right;\"> 1.39375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-49-45\n",
      "  done: false\n",
      "  episode_len_mean: 14.965116279069768\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3965116487166216\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 86\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 12.0\n",
      "    episode_reward_max: 1.1000000163912773\n",
      "    episode_reward_mean: 1.1000000163912773\n",
      "    episode_reward_min: 1.1000000163912773\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.39548283268289\n",
      "      mean_inference_ms: 1.2455021125682886\n",
      "      mean_processing_ms: 6.846817219314944\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.8824\n",
      "    min_exploration: 0.8824\n",
      "    num_steps_sampled: 1300\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1300\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 99.225\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.905882352941177\n",
      "    ram_util_percent: 67.80588235294118\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.87495086665258\n",
      "    mean_inference_ms: 0.7830693268756068\n",
      "    mean_processing_ms: 6.6195038518255\n",
      "  time_since_restore: 134.43976712226868\n",
      "  time_this_iter_s: 10.274907112121582\n",
      "  time_total_s: 134.43976712226868\n",
      "  timestamp: 1582120185\n",
      "  timesteps_since_restore: 1300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1300\n",
      "  training_iteration: 13\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">          134.44</td><td style=\"text-align: right;\">       1300</td><td style=\"text-align: right;\"> 1.39651</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-49-56\n",
      "  done: false\n",
      "  episode_len_mean: 15.010752688172044\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.4010752896948526\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 93\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.83653852580923\n",
      "      mean_inference_ms: 1.2143365049784163\n",
      "      mean_processing_ms: 6.727720783875052\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.8726\n",
      "    min_exploration: 0.8726\n",
      "    num_steps_sampled: 1400\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1400\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.013\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.06875\n",
      "    ram_util_percent: 67.94375\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.87714077072484\n",
      "    mean_inference_ms: 0.7776393951949513\n",
      "    mean_processing_ms: 6.623320613140394\n",
      "  time_since_restore: 144.8124611377716\n",
      "  time_this_iter_s: 10.37269401550293\n",
      "  time_total_s: 144.8124611377716\n",
      "  timestamp: 1582120196\n",
      "  timesteps_since_restore: 1400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1400\n",
      "  training_iteration: 14\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         144.812</td><td style=\"text-align: right;\">       1400</td><td style=\"text-align: right;\"> 1.40108</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-50-09\n",
      "  done: false\n",
      "  episode_len_mean: 14.97\n",
      "  episode_reward_max: 2.400000035762787\n",
      "  episode_reward_mean: 1.3970000208169222\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 100\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 11.0\n",
      "    episode_reward_max: 1.0000000149011612\n",
      "    episode_reward_mean: 1.0000000149011612\n",
      "    episode_reward_min: 1.0000000149011612\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 76.52579681782782\n",
      "      mean_inference_ms: 1.197112763481301\n",
      "      mean_processing_ms: 6.794836953722475\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 0.8628\n",
      "    min_exploration: 0.8628\n",
      "    num_steps_sampled: 1500\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1500\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 109.295\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.347058823529416\n",
      "    ram_util_percent: 67.94117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.87694907019319\n",
      "    mean_inference_ms: 0.77315403510191\n",
      "    mean_processing_ms: 6.629650022790449\n",
      "  time_since_restore: 155.18571710586548\n",
      "  time_this_iter_s: 10.373255968093872\n",
      "  time_total_s: 155.18571710586548\n",
      "  timestamp: 1582120209\n",
      "  timesteps_since_restore: 1500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1500\n",
      "  training_iteration: 15\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         155.186</td><td style=\"text-align: right;\">       1500</td><td style=\"text-align: right;\">   1.397</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 14.98\n",
      "  episode_reward_max: 2.500000037252903\n",
      "  episode_reward_mean: 1.3980000208318233\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 106\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 77.03306424478613\n",
      "      mean_inference_ms: 1.1637544817497758\n",
      "      mean_processing_ms: 6.682074023591869\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 6.093\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -3.3570971488952637\n",
      "        critic_loss: 0.02246914617717266\n",
      "        max_q: 2.385404586791992\n",
      "        mean_q: 2.033414840698242\n",
      "        min_q: 1.5180875062942505\n",
      "        model: {}\n",
      "        td_error: 0.04493829235434532\n",
      "    max_exploration: 0.853\n",
      "    min_exploration: 0.853\n",
      "    num_steps_sampled: 1600\n",
      "    num_steps_trained: 25600\n",
      "    num_target_updates: 1600\n",
      "    opt_peak_throughput: 42012.788\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 22.366\n",
      "    sample_time_ms: 80.633\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.247058823529414\n",
      "    ram_util_percent: 68.09411764705882\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.83211770339233\n",
      "    mean_inference_ms: 0.7273537092512864\n",
      "    mean_processing_ms: 6.649225097717322\n",
      "  time_since_restore: 166.0233769416809\n",
      "  time_this_iter_s: 10.83765983581543\n",
      "  time_total_s: 166.0233769416809\n",
      "  timestamp: 1582120220\n",
      "  timesteps_since_restore: 1600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1600\n",
      "  training_iteration: 16\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         166.023</td><td style=\"text-align: right;\">       1600</td><td style=\"text-align: right;\">   1.398</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-50-32\n",
      "  done: false\n",
      "  episode_len_mean: 15.17\n",
      "  episode_reward_max: 2.500000037252903\n",
      "  episode_reward_mean: 1.4170000211149454\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 111\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 77.40100798624087\n",
      "      mean_inference_ms: 1.1303330156346951\n",
      "      mean_processing_ms: 6.571936693432529\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.932\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -3.9104161262512207\n",
      "        critic_loss: 0.02436640113592148\n",
      "        max_q: 3.17488956451416\n",
      "        mean_q: 2.643749237060547\n",
      "        min_q: 2.04834246635437\n",
      "        model: {}\n",
      "        td_error: 0.04873279854655266\n",
      "    max_exploration: 0.8432\n",
      "    min_exploration: 0.8432\n",
      "    num_steps_sampled: 1700\n",
      "    num_steps_trained: 51200\n",
      "    num_target_updates: 1700\n",
      "    opt_peak_throughput: 51900.67\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.267\n",
      "    sample_time_ms: 73.9\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.91176470588235\n",
      "    ram_util_percent: 67.6470588235294\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.69193966696011\n",
      "    mean_inference_ms: 0.7118905633940961\n",
      "    mean_processing_ms: 6.639430258026327\n",
      "  time_since_restore: 176.22208285331726\n",
      "  time_this_iter_s: 10.198705911636353\n",
      "  time_total_s: 176.22208285331726\n",
      "  timestamp: 1582120232\n",
      "  timesteps_since_restore: 1700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1700\n",
      "  training_iteration: 17\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         176.222</td><td style=\"text-align: right;\">       1700</td><td style=\"text-align: right;\">   1.417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 15.45\n",
      "  episode_reward_max: 2.500000037252903\n",
      "  episode_reward_mean: 1.4450000215321779\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 117\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 71.0\n",
      "    episode_reward_max: 7.000000104308128\n",
      "    episode_reward_mean: 7.000000104308128\n",
      "    episode_reward_min: 7.000000104308128\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 80.89595964585227\n",
      "      mean_inference_ms: 1.0410540405361133\n",
      "      mean_processing_ms: 5.544469959434422\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.433\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -4.409358024597168\n",
      "        critic_loss: 0.026543810963630676\n",
      "        max_q: 3.885894298553467\n",
      "        mean_q: 3.180525302886963\n",
      "        min_q: 2.6174051761627197\n",
      "        model: {}\n",
      "        td_error: 0.05308762192726135\n",
      "    max_exploration: 0.8334\n",
      "    min_exploration: 0.8334\n",
      "    num_steps_sampled: 1800\n",
      "    num_steps_trained: 76800\n",
      "    num_target_updates: 1800\n",
      "    opt_peak_throughput: 57753.827\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.804\n",
      "    sample_time_ms: 88.065\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.429411764705883\n",
      "    ram_util_percent: 67.68823529411765\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.44912508579024\n",
      "    mean_inference_ms: 0.6997154008948189\n",
      "    mean_processing_ms: 6.633752531119922\n",
      "  time_since_restore: 186.5271017551422\n",
      "  time_this_iter_s: 10.305018901824951\n",
      "  time_total_s: 186.5271017551422\n",
      "  timestamp: 1582120244\n",
      "  timesteps_since_restore: 1800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1800\n",
      "  training_iteration: 18\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         186.527</td><td style=\"text-align: right;\">       1800</td><td style=\"text-align: right;\">   1.445</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-51-01\n",
      "  done: false\n",
      "  episode_len_mean: 15.62\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.4620000217854976\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 122\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 80.99650494430377\n",
      "      mean_inference_ms: 1.018392650977425\n",
      "      mean_processing_ms: 5.521331792292387\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.179\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -4.8905029296875\n",
      "        critic_loss: 0.03064509481191635\n",
      "        max_q: 4.307615756988525\n",
      "        mean_q: 3.6876585483551025\n",
      "        min_q: 3.0511584281921387\n",
      "        model: {}\n",
      "        td_error: 0.061290182173252106\n",
      "    max_exploration: 0.8236\n",
      "    min_exploration: 0.8236\n",
      "    num_steps_sampled: 1900\n",
      "    num_steps_trained: 102400\n",
      "    num_target_updates: 1900\n",
      "    opt_peak_throughput: 61261.807\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.346\n",
      "    sample_time_ms: 89.21\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.804166666666667\n",
      "    ram_util_percent: 67.69166666666668\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 94.19296156790628\n",
      "    mean_inference_ms: 0.6943066271057526\n",
      "    mean_processing_ms: 6.630559449426816\n",
      "  time_since_restore: 196.7138476371765\n",
      "  time_this_iter_s: 10.186745882034302\n",
      "  time_total_s: 196.7138476371765\n",
      "  timestamp: 1582120261\n",
      "  timesteps_since_restore: 1900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 1900\n",
      "  training_iteration: 19\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         196.714</td><td style=\"text-align: right;\">       1900</td><td style=\"text-align: right;\">   1.462</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 16.01\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.5010000223666429\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 126\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 81.21982132992171\n",
      "      mean_inference_ms: 1.0003994797806606\n",
      "      mean_processing_ms: 5.461751957378729\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.3\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -5.361021995544434\n",
      "        critic_loss: 0.017689082771539688\n",
      "        max_q: 4.926124572753906\n",
      "        mean_q: 4.210587501525879\n",
      "        min_q: 3.5588035583496094\n",
      "        model: {}\n",
      "        td_error: 0.035378165543079376\n",
      "    max_exploration: 0.8138\n",
      "    min_exploration: 0.8138\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 128000\n",
      "    num_target_updates: 2000\n",
      "    opt_peak_throughput: 59531.937\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.996\n",
      "    sample_time_ms: 87.881\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.805882352941175\n",
      "    ram_util_percent: 67.75294117647059\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 93.95107999512348\n",
      "    mean_inference_ms: 0.6909694172645843\n",
      "    mean_processing_ms: 6.625019096930979\n",
      "  time_since_restore: 206.80527591705322\n",
      "  time_this_iter_s: 10.091428279876709\n",
      "  time_total_s: 206.80527591705322\n",
      "  timestamp: 1582120273\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 20\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         206.805</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">   1.501</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 16.27\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.5270000227540732\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 131\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 81.43315453460251\n",
      "      mean_inference_ms: 0.9856039775166534\n",
      "      mean_processing_ms: 5.409122665147275\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.346\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -5.840593338012695\n",
      "        critic_loss: 0.017822304740548134\n",
      "        max_q: 5.280223846435547\n",
      "        mean_q: 4.706599712371826\n",
      "        min_q: 3.7978789806365967\n",
      "        model: {}\n",
      "        td_error: 0.03564460948109627\n",
      "    max_exploration: 0.804\n",
      "    min_exploration: 0.804\n",
      "    num_steps_sampled: 2100\n",
      "    num_steps_trained: 153600\n",
      "    num_target_updates: 2100\n",
      "    opt_peak_throughput: 58901.978\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.886\n",
      "    sample_time_ms: 78.422\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.505882352941175\n",
      "    ram_util_percent: 67.7764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 93.61122402986543\n",
      "    mean_inference_ms: 0.6889408321633372\n",
      "    mean_processing_ms: 6.608715245222166\n",
      "  time_since_restore: 217.0010380744934\n",
      "  time_this_iter_s: 10.195762157440186\n",
      "  time_total_s: 217.0010380744934\n",
      "  timestamp: 1582120286\n",
      "  timesteps_since_restore: 2100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2100\n",
      "  training_iteration: 21\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         217.001</td><td style=\"text-align: right;\">       2100</td><td style=\"text-align: right;\">   1.527</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-51-38\n",
      "  done: false\n",
      "  episode_len_mean: 16.68\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.5680000233650206\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 136\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 40.0\n",
      "    episode_reward_max: 3.9000000581145287\n",
      "    episode_reward_mean: 3.9000000581145287\n",
      "    episode_reward_min: 3.9000000581145287\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.25501791496109\n",
      "      mean_inference_ms: 0.9616592382019312\n",
      "      mean_processing_ms: 5.1752601951229416\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.178\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -6.274888038635254\n",
      "        critic_loss: 0.024412851780653\n",
      "        max_q: 6.09833288192749\n",
      "        mean_q: 5.172520637512207\n",
      "        min_q: 4.196640491485596\n",
      "        model: {}\n",
      "        td_error: 0.048825703561306\n",
      "    max_exploration: 0.7942\n",
      "    min_exploration: 0.7942\n",
      "    num_steps_sampled: 2200\n",
      "    num_steps_trained: 179200\n",
      "    num_target_updates: 2200\n",
      "    opt_peak_throughput: 61275.442\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.615\n",
      "    sample_time_ms: 88.065\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.205555555555556\n",
      "    ram_util_percent: 67.8\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 93.2355478209152\n",
      "    mean_inference_ms: 0.6876561450895514\n",
      "    mean_processing_ms: 6.591908250069282\n",
      "  time_since_restore: 227.19289898872375\n",
      "  time_this_iter_s: 10.191860914230347\n",
      "  time_total_s: 227.19289898872375\n",
      "  timestamp: 1582120298\n",
      "  timesteps_since_restore: 2200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2200\n",
      "  training_iteration: 22\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         227.193</td><td style=\"text-align: right;\">       2200</td><td style=\"text-align: right;\">   1.568</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-51-52\n",
      "  done: false\n",
      "  episode_len_mean: 16.96\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.5960000237822534\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 140\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.26679347235442\n",
      "      mean_inference_ms: 0.944782912982667\n",
      "      mean_processing_ms: 5.173909513256218\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.108\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -6.764459609985352\n",
      "        critic_loss: 0.017272278666496277\n",
      "        max_q: 6.673060417175293\n",
      "        mean_q: 5.687520980834961\n",
      "        min_q: 4.756630897521973\n",
      "        model: {}\n",
      "        td_error: 0.034544553607702255\n",
      "    max_exploration: 0.7844\n",
      "    min_exploration: 0.7844\n",
      "    num_steps_sampled: 2300\n",
      "    num_steps_trained: 204800\n",
      "    num_target_updates: 2300\n",
      "    opt_peak_throughput: 62323.22\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.293\n",
      "    sample_time_ms: 80.95\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.915\n",
      "    ram_util_percent: 67.79999999999998\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 92.91206862085403\n",
      "    mean_inference_ms: 0.6869764110225725\n",
      "    mean_processing_ms: 6.575926126552535\n",
      "  time_since_restore: 237.28209924697876\n",
      "  time_this_iter_s: 10.089200258255005\n",
      "  time_total_s: 237.28209924697876\n",
      "  timestamp: 1582120312\n",
      "  timesteps_since_restore: 2300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2300\n",
      "  training_iteration: 23\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         237.282</td><td style=\"text-align: right;\">       2300</td><td style=\"text-align: right;\">   1.596</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-52-04\n",
      "  done: false\n",
      "  episode_len_mean: 17.25\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.625000024214387\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 146\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.35344915620742\n",
      "      mean_inference_ms: 0.9321695373904321\n",
      "      mean_processing_ms: 5.152357682105033\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.464\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -7.164904594421387\n",
      "        critic_loss: 0.023175301030278206\n",
      "        max_q: 7.065755844116211\n",
      "        mean_q: 6.136728763580322\n",
      "        min_q: 5.151094436645508\n",
      "        model: {}\n",
      "        td_error: 0.04635060578584671\n",
      "    max_exploration: 0.7746\n",
      "    min_exploration: 0.7746\n",
      "    num_steps_sampled: 2400\n",
      "    num_steps_trained: 230400\n",
      "    num_target_updates: 2400\n",
      "    opt_peak_throughput: 57344.525\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.612\n",
      "    sample_time_ms: 87.265\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.11764705882353\n",
      "    ram_util_percent: 67.91764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 92.38917636847586\n",
      "    mean_inference_ms: 0.6872075669802871\n",
      "    mean_processing_ms: 6.55510324362934\n",
      "  time_since_restore: 247.57489919662476\n",
      "  time_this_iter_s: 10.292799949645996\n",
      "  time_total_s: 247.57489919662476\n",
      "  timestamp: 1582120324\n",
      "  timesteps_since_restore: 2400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2400\n",
      "  training_iteration: 24\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         247.575</td><td style=\"text-align: right;\">       2400</td><td style=\"text-align: right;\">   1.625</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-52-16\n",
      "  done: false\n",
      "  episode_len_mean: 17.45\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.6450000245124101\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 151\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.4982395538917\n",
      "      mean_inference_ms: 0.9195960485018216\n",
      "      mean_processing_ms: 5.116544778530415\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.465\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -7.629502773284912\n",
      "        critic_loss: 0.023343274369835854\n",
      "        max_q: 7.78577995300293\n",
      "        mean_q: 6.609404563903809\n",
      "        min_q: 5.610840797424316\n",
      "        model: {}\n",
      "        td_error: 0.04668654501438141\n",
      "    max_exploration: 0.7648\n",
      "    min_exploration: 0.7648\n",
      "    num_steps_sampled: 2500\n",
      "    num_steps_trained: 256000\n",
      "    num_target_updates: 2500\n",
      "    opt_peak_throughput: 57331.665\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.498\n",
      "    sample_time_ms: 86.192\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.423529411764704\n",
      "    ram_util_percent: 68.22941176470587\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 91.9221517724602\n",
      "    mean_inference_ms: 0.6878912130756913\n",
      "    mean_processing_ms: 6.53728818359549\n",
      "  time_since_restore: 257.76973509788513\n",
      "  time_this_iter_s: 10.194835901260376\n",
      "  time_total_s: 257.76973509788513\n",
      "  timestamp: 1582120336\n",
      "  timesteps_since_restore: 2500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2500\n",
      "  training_iteration: 25\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          257.77</td><td style=\"text-align: right;\">       2500</td><td style=\"text-align: right;\">   1.645</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 17.62\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.6620000247657298\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 156\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.53606630561532\n",
      "      mean_inference_ms: 0.9079217470066825\n",
      "      mean_processing_ms: 5.107665458580483\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.238\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -8.027950286865234\n",
      "        critic_loss: 0.02395259216427803\n",
      "        max_q: 8.062521934509277\n",
      "        mean_q: 7.047101974487305\n",
      "        min_q: 6.129852294921875\n",
      "        model: {}\n",
      "        td_error: 0.04790518432855606\n",
      "    max_exploration: 0.755\n",
      "    min_exploration: 0.755\n",
      "    num_steps_sampled: 2600\n",
      "    num_steps_trained: 281600\n",
      "    num_target_updates: 2600\n",
      "    opt_peak_throughput: 60404.698\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.428\n",
      "    sample_time_ms: 87.591\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.3\n",
      "    ram_util_percent: 68.27777777777777\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 91.43013378205127\n",
      "    mean_inference_ms: 0.6887056479950836\n",
      "    mean_processing_ms: 6.516093013840771\n",
      "  time_since_restore: 267.96386218070984\n",
      "  time_this_iter_s: 10.194127082824707\n",
      "  time_total_s: 267.96386218070984\n",
      "  timestamp: 1582120348\n",
      "  timesteps_since_restore: 2600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2600\n",
      "  training_iteration: 26\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         267.964</td><td style=\"text-align: right;\">       2600</td><td style=\"text-align: right;\">   1.662</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-52-41\n",
      "  done: false\n",
      "  episode_len_mean: 17.89\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.6890000251680612\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 162\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.53865998484022\n",
      "      mean_inference_ms: 0.9020951554949483\n",
      "      mean_processing_ms: 5.107531148057145\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.284\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -8.400341987609863\n",
      "        critic_loss: 0.022084936499595642\n",
      "        max_q: 8.785279273986816\n",
      "        mean_q: 7.448986053466797\n",
      "        min_q: 6.4665303230285645\n",
      "        model: {}\n",
      "        td_error: 0.044169872999191284\n",
      "    max_exploration: 0.7452\n",
      "    min_exploration: 0.7452\n",
      "    num_steps_sampled: 2700\n",
      "    num_steps_trained: 307200\n",
      "    num_target_updates: 2700\n",
      "    opt_peak_throughput: 59750.913\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.271\n",
      "    sample_time_ms: 86.034\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.647058823529417\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 90.81586326890537\n",
      "    mean_inference_ms: 0.6894338444594489\n",
      "    mean_processing_ms: 6.487526378366601\n",
      "  time_since_restore: 278.2566931247711\n",
      "  time_this_iter_s: 10.29283094406128\n",
      "  time_total_s: 278.2566931247711\n",
      "  timestamp: 1582120361\n",
      "  timesteps_since_restore: 2700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2700\n",
      "  training_iteration: 27\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         278.257</td><td style=\"text-align: right;\">       2700</td><td style=\"text-align: right;\">   1.689</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-52-53\n",
      "  done: false\n",
      "  episode_len_mean: 18.09\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.7090000254660844\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 166\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.66076960115352\n",
      "      mean_inference_ms: 0.891876220703125\n",
      "      mean_processing_ms: 5.074854386158479\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.449\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -8.822345733642578\n",
      "        critic_loss: 0.01791880838572979\n",
      "        max_q: 9.20348834991455\n",
      "        mean_q: 7.91651725769043\n",
      "        min_q: 7.031758785247803\n",
      "        model: {}\n",
      "        td_error: 0.03583761304616928\n",
      "    max_exploration: 0.7354\n",
      "    min_exploration: 0.7354\n",
      "    num_steps_sampled: 2800\n",
      "    num_steps_trained: 332800\n",
      "    num_target_updates: 2800\n",
      "    opt_peak_throughput: 57535.343\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.103\n",
      "    sample_time_ms: 76.839\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.235294117647058\n",
      "    ram_util_percent: 68.72941176470589\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 90.39244896672938\n",
      "    mean_inference_ms: 0.6889737666570919\n",
      "    mean_processing_ms: 6.466684898765443\n",
      "  time_since_restore: 288.35102224349976\n",
      "  time_this_iter_s: 10.094329118728638\n",
      "  time_total_s: 288.35102224349976\n",
      "  timestamp: 1582120373\n",
      "  timesteps_since_restore: 2800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2800\n",
      "  training_iteration: 28\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         288.351</td><td style=\"text-align: right;\">       2800</td><td style=\"text-align: right;\">   1.709</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-53-05\n",
      "  done: false\n",
      "  episode_len_mean: 18.37\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.737000025883317\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 171\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 32.0\n",
      "    episode_reward_max: 3.1000000461935997\n",
      "    episode_reward_mean: 3.1000000461935997\n",
      "    episode_reward_min: 3.1000000461935997\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.98476279458119\n",
      "      mean_inference_ms: 0.8801799539229085\n",
      "      mean_processing_ms: 4.9794025513882385\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.776\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -9.167701721191406\n",
      "        critic_loss: 0.020638570189476013\n",
      "        max_q: 9.7076416015625\n",
      "        mean_q: 8.29520320892334\n",
      "        min_q: 7.1643500328063965\n",
      "        model: {}\n",
      "        td_error: 0.041277140378952026\n",
      "    max_exploration: 0.7256\n",
      "    min_exploration: 0.7256\n",
      "    num_steps_sampled: 2900\n",
      "    num_steps_trained: 358400\n",
      "    num_target_updates: 2900\n",
      "    opt_peak_throughput: 53606.146\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.581\n",
      "    sample_time_ms: 75.957\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.366666666666667\n",
      "    ram_util_percent: 68.62777777777778\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 89.8460357075277\n",
      "    mean_inference_ms: 0.6875808428769977\n",
      "    mean_processing_ms: 6.4360376419972685\n",
      "  time_since_restore: 298.5467073917389\n",
      "  time_this_iter_s: 10.195685148239136\n",
      "  time_total_s: 298.5467073917389\n",
      "  timestamp: 1582120385\n",
      "  timesteps_since_restore: 2900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 2900\n",
      "  training_iteration: 29\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         298.547</td><td style=\"text-align: right;\">       2900</td><td style=\"text-align: right;\">   1.737</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-53-18\n",
      "  done: false\n",
      "  episode_len_mean: 18.55\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.755000026151538\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 176\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 82.97742292682642\n",
      "      mean_inference_ms: 0.8727194188715337\n",
      "      mean_processing_ms: 4.98554096491408\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.622\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -9.603005409240723\n",
      "        critic_loss: 0.022130202502012253\n",
      "        max_q: 10.264901161193848\n",
      "        mean_q: 8.73186206817627\n",
      "        min_q: 7.483299255371094\n",
      "        model: {}\n",
      "        td_error: 0.044260405004024506\n",
      "    max_exploration: 0.7158\n",
      "    min_exploration: 0.7158\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 384000\n",
      "    num_target_updates: 3000\n",
      "    opt_peak_throughput: 55383.487\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.093\n",
      "    sample_time_ms: 76.586\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.922222222222224\n",
      "    ram_util_percent: 69.09999999999998\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 89.28192238986128\n",
      "    mean_inference_ms: 0.685658960092085\n",
      "    mean_processing_ms: 6.40365666743577\n",
      "  time_since_restore: 308.74112367630005\n",
      "  time_this_iter_s: 10.194416284561157\n",
      "  time_total_s: 308.74112367630005\n",
      "  timestamp: 1582120398\n",
      "  timesteps_since_restore: 3000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 30\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         308.741</td><td style=\"text-align: right;\">       3000</td><td style=\"text-align: right;\">   1.755</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 18.72\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.7720000264048577\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 181\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.04568059516676\n",
      "      mean_inference_ms: 0.8636265090017607\n",
      "      mean_processing_ms: 4.965872114354914\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.429\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -9.940629005432129\n",
      "        critic_loss: 0.017845017835497856\n",
      "        max_q: 10.474294662475586\n",
      "        mean_q: 9.098596572875977\n",
      "        min_q: 7.997012138366699\n",
      "        model: {}\n",
      "        td_error: 0.03569003939628601\n",
      "    max_exploration: 0.706\n",
      "    min_exploration: 0.706\n",
      "    num_steps_sampled: 3100\n",
      "    num_steps_trained: 409600\n",
      "    num_target_updates: 3100\n",
      "    opt_peak_throughput: 57797.972\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.981\n",
      "    sample_time_ms: 77.158\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.388235294117646\n",
      "    ram_util_percent: 69.12352941176471\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 88.70459020655174\n",
      "    mean_inference_ms: 0.6831201003672556\n",
      "    mean_processing_ms: 6.3696070690517015\n",
      "  time_since_restore: 318.93557357788086\n",
      "  time_this_iter_s: 10.19444990158081\n",
      "  time_total_s: 318.93557357788086\n",
      "  timestamp: 1582120410\n",
      "  timesteps_since_restore: 3100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3100\n",
      "  training_iteration: 31\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         318.936</td><td style=\"text-align: right;\">       3100</td><td style=\"text-align: right;\">   1.772</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-53-42\n",
      "  done: false\n",
      "  episode_len_mean: 19.06\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.806000026911497\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 186\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.08883566311027\n",
      "      mean_inference_ms: 0.8556353381651937\n",
      "      mean_processing_ms: 4.955401518477722\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.153\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -10.203975677490234\n",
      "        critic_loss: 0.01999945566058159\n",
      "        max_q: 10.529949188232422\n",
      "        mean_q: 9.401723861694336\n",
      "        min_q: 8.239794731140137\n",
      "        model: {}\n",
      "        td_error: 0.03999890759587288\n",
      "    max_exploration: 0.6961999999999999\n",
      "    min_exploration: 0.6961999999999999\n",
      "    num_steps_sampled: 3200\n",
      "    num_steps_trained: 435200\n",
      "    num_target_updates: 3200\n",
      "    opt_peak_throughput: 61634.913\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.342\n",
      "    sample_time_ms: 87.957\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.511111111111113\n",
      "    ram_util_percent: 69.15\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 88.11437257686032\n",
      "    mean_inference_ms: 0.6801137468217638\n",
      "    mean_processing_ms: 6.335311749293392\n",
      "  time_since_restore: 329.1290135383606\n",
      "  time_this_iter_s: 10.193439960479736\n",
      "  time_total_s: 329.1290135383606\n",
      "  timestamp: 1582120422\n",
      "  timesteps_since_restore: 3200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3200\n",
      "  training_iteration: 32\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         329.129</td><td style=\"text-align: right;\">       3200</td><td style=\"text-align: right;\">   1.806</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 19.36\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.836000027358532\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 190\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.12423798170957\n",
      "      mean_inference_ms: 0.8486302738839929\n",
      "      mean_processing_ms: 4.946076057173989\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.312\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -10.62000560760498\n",
      "        critic_loss: 0.016572795808315277\n",
      "        max_q: 11.189291954040527\n",
      "        mean_q: 9.819623947143555\n",
      "        min_q: 8.712859153747559\n",
      "        model: {}\n",
      "        td_error: 0.033145587891340256\n",
      "    max_exploration: 0.6864\n",
      "    min_exploration: 0.6864\n",
      "    num_steps_sampled: 3300\n",
      "    num_steps_trained: 460800\n",
      "    num_target_updates: 3300\n",
      "    opt_peak_throughput: 59365.391\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.5\n",
      "    sample_time_ms: 77.473\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.876470588235293\n",
      "    ram_util_percent: 69.05882352941175\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 87.63142943865881\n",
      "    mean_inference_ms: 0.6774890568032587\n",
      "    mean_processing_ms: 6.304288431793139\n",
      "  time_since_restore: 339.2227976322174\n",
      "  time_this_iter_s: 10.093784093856812\n",
      "  time_total_s: 339.2227976322174\n",
      "  timestamp: 1582120434\n",
      "  timesteps_since_restore: 3300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3300\n",
      "  training_iteration: 33\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         339.223</td><td style=\"text-align: right;\">       3300</td><td style=\"text-align: right;\">   1.836</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-54-07\n",
      "  done: false\n",
      "  episode_len_mean: 19.68\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.868000027835369\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 195\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.15891405110845\n",
      "      mean_inference_ms: 0.8425485989278998\n",
      "      mean_processing_ms: 4.936087558420565\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.676\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -10.916051864624023\n",
      "        critic_loss: 0.020126918330788612\n",
      "        max_q: 11.350526809692383\n",
      "        mean_q: 10.136876106262207\n",
      "        min_q: 9.069600105285645\n",
      "        model: {}\n",
      "        td_error: 0.04025384038686752\n",
      "    max_exploration: 0.6766\n",
      "    min_exploration: 0.6766\n",
      "    num_steps_sampled: 3400\n",
      "    num_steps_trained: 486400\n",
      "    num_target_updates: 3400\n",
      "    opt_peak_throughput: 54743.644\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.633\n",
      "    sample_time_ms: 86.111\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.67058823529412\n",
      "    ram_util_percent: 69.07058823529411\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 87.01279096539537\n",
      "    mean_inference_ms: 0.6740968792307056\n",
      "    mean_processing_ms: 6.263749371119149\n",
      "  time_since_restore: 349.4188766479492\n",
      "  time_this_iter_s: 10.196079015731812\n",
      "  time_total_s: 349.4188766479492\n",
      "  timestamp: 1582120447\n",
      "  timesteps_since_restore: 3400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3400\n",
      "  training_iteration: 34\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         349.419</td><td style=\"text-align: right;\">       3400</td><td style=\"text-align: right;\">   1.868</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 19.99\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.8990000282973052\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 200\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.14839668631873\n",
      "      mean_inference_ms: 0.8384542235100877\n",
      "      mean_processing_ms: 4.940855918557011\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.761\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.18241024017334\n",
      "        critic_loss: 0.020359301939606667\n",
      "        max_q: 11.797761917114258\n",
      "        mean_q: 10.417459487915039\n",
      "        min_q: 9.218125343322754\n",
      "        model: {}\n",
      "        td_error: 0.040718600153923035\n",
      "    max_exploration: 0.6668000000000001\n",
      "    min_exploration: 0.6668000000000001\n",
      "    num_steps_sampled: 3500\n",
      "    num_steps_trained: 512000\n",
      "    num_target_updates: 3500\n",
      "    opt_peak_throughput: 53772.051\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.802\n",
      "    sample_time_ms: 85.604\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.81111111111111\n",
      "    ram_util_percent: 69.11666666666666\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 86.37929024861133\n",
      "    mean_inference_ms: 0.6705942310904208\n",
      "    mean_processing_ms: 6.221087354712308\n",
      "  time_since_restore: 359.6138586997986\n",
      "  time_this_iter_s: 10.194982051849365\n",
      "  time_total_s: 359.6138586997986\n",
      "  timestamp: 1582120459\n",
      "  timesteps_since_restore: 3500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3500\n",
      "  training_iteration: 35\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         359.614</td><td style=\"text-align: right;\">       3500</td><td style=\"text-align: right;\">   1.899</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-54-31\n",
      "  done: false\n",
      "  episode_len_mean: 20.13\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.9130000285059214\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 205\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.15336253528022\n",
      "      mean_inference_ms: 0.8379095070054354\n",
      "      mean_processing_ms: 4.956951974423643\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.006\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.461017608642578\n",
      "        critic_loss: 0.01780617982149124\n",
      "        max_q: 12.058745384216309\n",
      "        mean_q: 10.73430061340332\n",
      "        min_q: 9.590615272521973\n",
      "        model: {}\n",
      "        td_error: 0.03561235964298248\n",
      "    max_exploration: 0.657\n",
      "    min_exploration: 0.657\n",
      "    num_steps_sampled: 3600\n",
      "    num_steps_trained: 537600\n",
      "    num_target_updates: 3600\n",
      "    opt_peak_throughput: 51139.573\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.082\n",
      "    sample_time_ms: 84.293\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.511764705882356\n",
      "    ram_util_percent: 69.17058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 85.82374535104886\n",
      "    mean_inference_ms: 0.6671084351213998\n",
      "    mean_processing_ms: 6.1793284744956285\n",
      "  time_since_restore: 369.81232166290283\n",
      "  time_this_iter_s: 10.198462963104248\n",
      "  time_total_s: 369.81232166290283\n",
      "  timestamp: 1582120471\n",
      "  timesteps_since_restore: 3600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3600\n",
      "  training_iteration: 36\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         369.812</td><td style=\"text-align: right;\">       3600</td><td style=\"text-align: right;\">   1.913</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-54-43\n",
      "  done: false\n",
      "  episode_len_mean: 20.37\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.9370000288635492\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 209\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.18812484982648\n",
      "      mean_inference_ms: 0.8335140687000903\n",
      "      mean_processing_ms: 4.943349995190585\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.702\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.694435119628906\n",
      "        critic_loss: 0.023376833647489548\n",
      "        max_q: 12.32382869720459\n",
      "        mean_q: 11.03309440612793\n",
      "        min_q: 9.818268775939941\n",
      "        model: {}\n",
      "        td_error: 0.0467536635696888\n",
      "    max_exploration: 0.6472\n",
      "    min_exploration: 0.6472\n",
      "    num_steps_sampled: 3700\n",
      "    num_steps_trained: 563200\n",
      "    num_target_updates: 3700\n",
      "    opt_peak_throughput: 54448.83\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.972\n",
      "    sample_time_ms: 75.558\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.31176470588235\n",
      "    ram_util_percent: 69.47058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 85.41309061978815\n",
      "    mean_inference_ms: 0.6643346924763206\n",
      "    mean_processing_ms: 6.146290140189373\n",
      "  time_since_restore: 379.9087088108063\n",
      "  time_this_iter_s: 10.096387147903442\n",
      "  time_total_s: 379.9087088108063\n",
      "  timestamp: 1582120483\n",
      "  timesteps_since_restore: 3700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3700\n",
      "  training_iteration: 37\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         379.909</td><td style=\"text-align: right;\">       3700</td><td style=\"text-align: right;\">   1.937</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-54-55\n",
      "  done: false\n",
      "  episode_len_mean: 20.38\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.9380000288784505\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 214\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.15783761193047\n",
      "      mean_inference_ms: 0.8284674291999732\n",
      "      mean_processing_ms: 4.953936387051463\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.017925262451172\n",
      "        critic_loss: 0.019698789343237877\n",
      "        max_q: 12.640327453613281\n",
      "        mean_q: 11.33387279510498\n",
      "        min_q: 10.46507453918457\n",
      "        model: {}\n",
      "        td_error: 0.039397574961185455\n",
      "    max_exploration: 0.6374\n",
      "    min_exploration: 0.6374\n",
      "    num_steps_sampled: 3800\n",
      "    num_steps_trained: 588800\n",
      "    num_target_updates: 3800\n",
      "    opt_peak_throughput: 57180.232\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.3\n",
      "    sample_time_ms: 76.576\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.735294117647054\n",
      "    ram_util_percent: 69.50588235294117\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 84.93894685402033\n",
      "    mean_inference_ms: 0.6609988455781317\n",
      "    mean_processing_ms: 6.105388473915964\n",
      "  time_since_restore: 390.1017847061157\n",
      "  time_this_iter_s: 10.193075895309448\n",
      "  time_total_s: 390.1017847061157\n",
      "  timestamp: 1582120495\n",
      "  timesteps_since_restore: 3800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3800\n",
      "  training_iteration: 38\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         390.102</td><td style=\"text-align: right;\">       3800</td><td style=\"text-align: right;\">   1.938</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 20.57\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 1.9570000291615726\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 219\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.28287966513747\n",
      "      mean_inference_ms: 0.8221878959801778\n",
      "      mean_processing_ms: 4.9167914253672915\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.5\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.35447883605957\n",
      "        critic_loss: 0.0200694277882576\n",
      "        max_q: 13.224713325500488\n",
      "        mean_q: 11.681577682495117\n",
      "        min_q: 10.568188667297363\n",
      "        model: {}\n",
      "        td_error: 0.0401388555765152\n",
      "    max_exploration: 0.6275999999999999\n",
      "    min_exploration: 0.6275999999999999\n",
      "    num_steps_sampled: 3900\n",
      "    num_steps_trained: 614400\n",
      "    num_target_updates: 3900\n",
      "    opt_peak_throughput: 56890.601\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.205\n",
      "    sample_time_ms: 76.298\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.070588235294117\n",
      "    ram_util_percent: 69.51764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 84.4923028158196\n",
      "    mean_inference_ms: 0.657918743973759\n",
      "    mean_processing_ms: 6.064268856055285\n",
      "  time_since_restore: 400.2971396446228\n",
      "  time_this_iter_s: 10.19535493850708\n",
      "  time_total_s: 400.2971396446228\n",
      "  timestamp: 1582120507\n",
      "  timesteps_since_restore: 3900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 3900\n",
      "  training_iteration: 39\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         400.297</td><td style=\"text-align: right;\">       3900</td><td style=\"text-align: right;\">   1.957</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-55-20\n",
      "  done: false\n",
      "  episode_len_mean: 20.26\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9260000286996364\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 224\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.364697975841\n",
      "      mean_inference_ms: 0.8165216058250365\n",
      "      mean_processing_ms: 4.8927629450887755\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.357\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.594715118408203\n",
      "        critic_loss: 0.01745663955807686\n",
      "        max_q: 13.446419715881348\n",
      "        mean_q: 11.950752258300781\n",
      "        min_q: 10.85835075378418\n",
      "        model: {}\n",
      "        td_error: 0.03491327911615372\n",
      "    max_exploration: 0.6178\n",
      "    min_exploration: 0.6178\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 640000\n",
      "    num_target_updates: 4000\n",
      "    opt_peak_throughput: 58760.149\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.257\n",
      "    sample_time_ms: 77.819\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.933333333333337\n",
      "    ram_util_percent: 69.58888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 84.07499038896937\n",
      "    mean_inference_ms: 0.6552093563736553\n",
      "    mean_processing_ms: 6.026764189987869\n",
      "  time_since_restore: 410.4912836551666\n",
      "  time_this_iter_s: 10.194144010543823\n",
      "  time_total_s: 410.4912836551666\n",
      "  timestamp: 1582120520\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 40\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         410.491</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\">   1.926</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 20.27\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9270000287145377\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 230\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.38712403890365\n",
      "      mean_inference_ms: 0.8118873544349541\n",
      "      mean_processing_ms: 4.887148046007513\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.774282455444336\n",
      "        critic_loss: 0.022583000361919403\n",
      "        max_q: 13.380671501159668\n",
      "        mean_q: 12.153886795043945\n",
      "        min_q: 11.057034492492676\n",
      "        model: {}\n",
      "        td_error: 0.045166000723838806\n",
      "    max_exploration: 0.608\n",
      "    min_exploration: 0.608\n",
      "    num_steps_sampled: 4100\n",
      "    num_steps_trained: 665600\n",
      "    num_target_updates: 4100\n",
      "    opt_peak_throughput: 54936.625\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.108\n",
      "    sample_time_ms: 86.402\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.005555555555553\n",
      "    ram_util_percent: 69.55555555555556\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 83.61930354916791\n",
      "    mean_inference_ms: 0.6524011551409394\n",
      "    mean_processing_ms: 5.988446134306769\n",
      "  time_since_restore: 420.7842617034912\n",
      "  time_this_iter_s: 10.292978048324585\n",
      "  time_total_s: 420.7842617034912\n",
      "  timestamp: 1582120532\n",
      "  timesteps_since_restore: 4100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4100\n",
      "  training_iteration: 41\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         420.784</td><td style=\"text-align: right;\">       4100</td><td style=\"text-align: right;\">   1.927</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 20.2\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9200000286102294\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 235\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.39134623519087\n",
      "      mean_inference_ms: 0.8073478145936949\n",
      "      mean_processing_ms: 4.886862714733698\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.26\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.079463005065918\n",
      "        critic_loss: 0.018819786608219147\n",
      "        max_q: 14.069272994995117\n",
      "        mean_q: 12.469489097595215\n",
      "        min_q: 11.218268394470215\n",
      "        model: {}\n",
      "        td_error: 0.03763957694172859\n",
      "    max_exploration: 0.5982000000000001\n",
      "    min_exploration: 0.5982000000000001\n",
      "    num_steps_sampled: 4200\n",
      "    num_steps_trained: 691200\n",
      "    num_target_updates: 4200\n",
      "    opt_peak_throughput: 60087.961\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.428\n",
      "    sample_time_ms: 89.096\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.876470588235296\n",
      "    ram_util_percent: 69.51764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 83.26796959994739\n",
      "    mean_inference_ms: 0.6503344270663789\n",
      "    mean_processing_ms: 5.958730096888084\n",
      "  time_since_restore: 430.9788339138031\n",
      "  time_this_iter_s: 10.19457221031189\n",
      "  time_total_s: 430.9788339138031\n",
      "  timestamp: 1582120544\n",
      "  timesteps_since_restore: 4200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4200\n",
      "  training_iteration: 42\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         430.979</td><td style=\"text-align: right;\">       4200</td><td style=\"text-align: right;\">    1.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-55-57\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9000000283122063\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 240\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.51447679454165\n",
      "      mean_inference_ms: 0.800966449049921\n",
      "      mean_processing_ms: 4.850862108075056\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.011\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.292941093444824\n",
      "        critic_loss: 0.02316216565668583\n",
      "        max_q: 13.960855484008789\n",
      "        mean_q: 12.701349258422852\n",
      "        min_q: 11.745985984802246\n",
      "        model: {}\n",
      "        td_error: 0.04632433503866196\n",
      "    max_exploration: 0.5884\n",
      "    min_exploration: 0.5884\n",
      "    num_steps_sampled: 4300\n",
      "    num_steps_trained: 716800\n",
      "    num_target_updates: 4300\n",
      "    opt_peak_throughput: 63819.374\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.817\n",
      "    sample_time_ms: 79.669\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.52352941176471\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 82.93621898747602\n",
      "    mean_inference_ms: 0.6484540314843242\n",
      "    mean_processing_ms: 5.932477056357907\n",
      "  time_since_restore: 441.1707420349121\n",
      "  time_this_iter_s: 10.191908121109009\n",
      "  time_total_s: 441.1707420349121\n",
      "  timestamp: 1582120557\n",
      "  timesteps_since_restore: 4300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4300\n",
      "  training_iteration: 43\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         441.171</td><td style=\"text-align: right;\">       4300</td><td style=\"text-align: right;\">     1.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-56-09\n",
      "  done: false\n",
      "  episode_len_mean: 19.88\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8880000281333924\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 245\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.6604563461272\n",
      "      mean_inference_ms: 0.7970699897179236\n",
      "      mean_processing_ms: 4.807804329727395\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.536\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.525003433227539\n",
      "        critic_loss: 0.016595669090747833\n",
      "        max_q: 14.155104637145996\n",
      "        mean_q: 12.960136413574219\n",
      "        min_q: 11.982422828674316\n",
      "        model: {}\n",
      "        td_error: 0.03319133445620537\n",
      "    max_exploration: 0.5786\n",
      "    min_exploration: 0.5786\n",
      "    num_steps_sampled: 4400\n",
      "    num_steps_trained: 742400\n",
      "    num_target_updates: 4400\n",
      "    opt_peak_throughput: 56431.941\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.088\n",
      "    sample_time_ms: 76.734\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.877777777777776\n",
      "    ram_util_percent: 69.87222222222222\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 82.62660347159752\n",
      "    mean_inference_ms: 0.6468484492882424\n",
      "    mean_processing_ms: 5.905449657216089\n",
      "  time_since_restore: 451.3651089668274\n",
      "  time_this_iter_s: 10.194366931915283\n",
      "  time_total_s: 451.3651089668274\n",
      "  timestamp: 1582120569\n",
      "  timesteps_since_restore: 4400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4400\n",
      "  training_iteration: 44\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         451.365</td><td style=\"text-align: right;\">       4400</td><td style=\"text-align: right;\">   1.888</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-56-22\n",
      "  done: false\n",
      "  episode_len_mean: 20.17\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.917000028565526\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 250\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.62511392762536\n",
      "      mean_inference_ms: 0.7960519294855427\n",
      "      mean_processing_ms: 4.818658216283955\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.69\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.680095672607422\n",
      "        critic_loss: 0.022194888442754745\n",
      "        max_q: 14.424777030944824\n",
      "        mean_q: 13.135027885437012\n",
      "        min_q: 12.20718765258789\n",
      "        model: {}\n",
      "        td_error: 0.04438977688550949\n",
      "    max_exploration: 0.5688\n",
      "    min_exploration: 0.5688\n",
      "    num_steps_sampled: 4500\n",
      "    num_steps_trained: 768000\n",
      "    num_target_updates: 4500\n",
      "    opt_peak_throughput: 54582.795\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.482\n",
      "    sample_time_ms: 85.331\n",
      "    update_time_ms: 0.006\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.994736842105258\n",
      "    ram_util_percent: 69.95263157894736\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 82.32911150837737\n",
      "    mean_inference_ms: 0.6453285699924939\n",
      "    mean_processing_ms: 5.879582261651246\n",
      "  time_since_restore: 461.56158423423767\n",
      "  time_this_iter_s: 10.196475267410278\n",
      "  time_total_s: 461.56158423423767\n",
      "  timestamp: 1582120582\n",
      "  timesteps_since_restore: 4500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4500\n",
      "  training_iteration: 45\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         461.562</td><td style=\"text-align: right;\">       4500</td><td style=\"text-align: right;\">   1.917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-56-34\n",
      "  done: false\n",
      "  episode_len_mean: 20.04\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9040000283718108\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 255\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.62118284145515\n",
      "      mean_inference_ms: 0.7944868472283948\n",
      "      mean_processing_ms: 4.8204838872669695\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.929070472717285\n",
      "        critic_loss: 0.015496758744120598\n",
      "        max_q: 14.697822570800781\n",
      "        mean_q: 13.385271072387695\n",
      "        min_q: 12.486170768737793\n",
      "        model: {}\n",
      "        td_error: 0.030993517488241196\n",
      "    max_exploration: 0.5589999999999999\n",
      "    min_exploration: 0.5589999999999999\n",
      "    num_steps_sampled: 4600\n",
      "    num_steps_trained: 793600\n",
      "    num_target_updates: 4600\n",
      "    opt_peak_throughput: 52624.858\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.549\n",
      "    sample_time_ms: 73.621\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.688235294117646\n",
      "    ram_util_percent: 69.85882352941177\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 82.04437561658843\n",
      "    mean_inference_ms: 0.6439893164200207\n",
      "    mean_processing_ms: 5.855363918507355\n",
      "  time_since_restore: 471.75957226753235\n",
      "  time_this_iter_s: 10.197988033294678\n",
      "  time_total_s: 471.75957226753235\n",
      "  timestamp: 1582120594\n",
      "  timesteps_since_restore: 4600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4600\n",
      "  training_iteration: 46\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">          471.76</td><td style=\"text-align: right;\">       4600</td><td style=\"text-align: right;\">   1.904</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-56-46\n",
      "  done: false\n",
      "  episode_len_mean: 20.25\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9250000286847353\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 261\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.6047577997943\n",
      "      mean_inference_ms: 0.7920678114471137\n",
      "      mean_processing_ms: 4.8262041842167624\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.621\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.131996154785156\n",
      "        critic_loss: 0.01716144010424614\n",
      "        max_q: 15.15101146697998\n",
      "        mean_q: 13.613479614257812\n",
      "        min_q: 12.56198787689209\n",
      "        model: {}\n",
      "        td_error: 0.03432288020849228\n",
      "    max_exploration: 0.5491999999999999\n",
      "    min_exploration: 0.5491999999999999\n",
      "    num_steps_sampled: 4700\n",
      "    num_steps_trained: 819200\n",
      "    num_target_updates: 4700\n",
      "    opt_peak_throughput: 55394.345\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.154\n",
      "    sample_time_ms: 86.565\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.58823529411765\n",
      "    ram_util_percent: 69.88823529411765\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 81.72085477372744\n",
      "    mean_inference_ms: 0.6425416205021278\n",
      "    mean_processing_ms: 5.827856575773072\n",
      "  time_since_restore: 482.0532262325287\n",
      "  time_this_iter_s: 10.293653964996338\n",
      "  time_total_s: 482.0532262325287\n",
      "  timestamp: 1582120606\n",
      "  timesteps_since_restore: 4700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4700\n",
      "  training_iteration: 47\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         482.053</td><td style=\"text-align: right;\">       4700</td><td style=\"text-align: right;\">   1.925</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 20.01\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9010000283271073\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 266\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.64721785084012\n",
      "      mean_inference_ms: 0.7902262999496314\n",
      "      mean_processing_ms: 4.820212351432494\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.395\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.374452590942383\n",
      "        critic_loss: 0.015908511355519295\n",
      "        max_q: 15.339348793029785\n",
      "        mean_q: 13.874748229980469\n",
      "        min_q: 12.861776351928711\n",
      "        model: {}\n",
      "        td_error: 0.03181702271103859\n",
      "    max_exploration: 0.5394000000000001\n",
      "    min_exploration: 0.5394000000000001\n",
      "    num_steps_sampled: 4800\n",
      "    num_steps_trained: 844800\n",
      "    num_target_updates: 4800\n",
      "    opt_peak_throughput: 58246.006\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.071\n",
      "    sample_time_ms: 75.051\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.141176470588235\n",
      "    ram_util_percent: 69.93529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 81.46461821217669\n",
      "    mean_inference_ms: 0.6414213975112414\n",
      "    mean_processing_ms: 5.807299087249207\n",
      "  time_since_restore: 492.24745321273804\n",
      "  time_this_iter_s: 10.19422698020935\n",
      "  time_total_s: 492.24745321273804\n",
      "  timestamp: 1582120618\n",
      "  timesteps_since_restore: 4800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4800\n",
      "  training_iteration: 48\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         492.247</td><td style=\"text-align: right;\">       4800</td><td style=\"text-align: right;\">   1.901</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-57-11\n",
      "  done: false\n",
      "  episode_len_mean: 19.97\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8970000282675028\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 271\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.61059108250578\n",
      "      mean_inference_ms: 0.7869104824155709\n",
      "      mean_processing_ms: 4.829417931641771\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.882\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.446897506713867\n",
      "        critic_loss: 0.019584383815526962\n",
      "        max_q: 15.48901081085205\n",
      "        mean_q: 13.933553695678711\n",
      "        min_q: 13.045156478881836\n",
      "        model: {}\n",
      "        td_error: 0.039168763905763626\n",
      "    max_exploration: 0.5296000000000001\n",
      "    min_exploration: 0.5296000000000001\n",
      "    num_steps_sampled: 4900\n",
      "    num_steps_trained: 870400\n",
      "    num_target_updates: 4900\n",
      "    opt_peak_throughput: 65949.796\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.896\n",
      "    sample_time_ms: 79.707\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.927777777777777\n",
      "    ram_util_percent: 69.96666666666667\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 81.22865660311156\n",
      "    mean_inference_ms: 0.6403212879871765\n",
      "    mean_processing_ms: 5.788707510673244\n",
      "  time_since_restore: 502.44019412994385\n",
      "  time_this_iter_s: 10.19274091720581\n",
      "  time_total_s: 502.44019412994385\n",
      "  timestamp: 1582120631\n",
      "  timesteps_since_restore: 4900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4900\n",
      "  training_iteration: 49\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">          502.44</td><td style=\"text-align: right;\">       4900</td><td style=\"text-align: right;\">   1.897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-57-23\n",
      "  done: false\n",
      "  episode_len_mean: 20.01\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9010000283271073\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 276\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.5958854394025\n",
      "      mean_inference_ms: 0.783580569078296\n",
      "      mean_processing_ms: 4.834900034188125\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.192\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.581899642944336\n",
      "        critic_loss: 0.02115088887512684\n",
      "        max_q: 15.493768692016602\n",
      "        mean_q: 14.10855484008789\n",
      "        min_q: 13.139521598815918\n",
      "        model: {}\n",
      "        td_error: 0.042301781475543976\n",
      "    max_exploration: 0.5198\n",
      "    min_exploration: 0.5198\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 896000\n",
      "    num_target_updates: 5000\n",
      "    opt_peak_throughput: 61069.822\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.067\n",
      "    sample_time_ms: 79.117\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.68235294117647\n",
      "    ram_util_percent: 70.0\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 81.00927666601874\n",
      "    mean_inference_ms: 0.6391910025774986\n",
      "    mean_processing_ms: 5.770342760542855\n",
      "  time_since_restore: 512.6316571235657\n",
      "  time_this_iter_s: 10.191462993621826\n",
      "  time_total_s: 512.6316571235657\n",
      "  timestamp: 1582120643\n",
      "  timesteps_since_restore: 5000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 50\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         512.632</td><td style=\"text-align: right;\">       5000</td><td style=\"text-align: right;\">   1.901</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 20.03\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9030000283569097\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 281\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.55223581343483\n",
      "      mean_inference_ms: 0.7812589920334457\n",
      "      mean_processing_ms: 4.848476549980322\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.189\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.818588256835938\n",
      "        critic_loss: 0.012046461924910545\n",
      "        max_q: 15.760720252990723\n",
      "        mean_q: 14.34298324584961\n",
      "        min_q: 13.482767105102539\n",
      "        model: {}\n",
      "        td_error: 0.02409292198717594\n",
      "    max_exploration: 0.51\n",
      "    min_exploration: 0.51\n",
      "    num_steps_sampled: 5100\n",
      "    num_steps_trained: 921600\n",
      "    num_target_updates: 5100\n",
      "    opt_peak_throughput: 61106.662\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.933\n",
      "    sample_time_ms: 78.214\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.658823529411762\n",
      "    ram_util_percent: 69.9764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 80.80087217040294\n",
      "    mean_inference_ms: 0.6381034359845783\n",
      "    mean_processing_ms: 5.754334956211262\n",
      "  time_since_restore: 522.8235774040222\n",
      "  time_this_iter_s: 10.191920280456543\n",
      "  time_total_s: 522.8235774040222\n",
      "  timestamp: 1582120655\n",
      "  timesteps_since_restore: 5100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5100\n",
      "  training_iteration: 51\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         522.824</td><td style=\"text-align: right;\">       5100</td><td style=\"text-align: right;\">   1.903</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 19.87\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.887000028118491\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 286\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.52292965227694\n",
      "      mean_inference_ms: 0.7805422665601108\n",
      "      mean_processing_ms: 4.856931122016567\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.651\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.9014253616333\n",
      "        critic_loss: 0.015398520976305008\n",
      "        max_q: 15.811500549316406\n",
      "        mean_q: 14.461811065673828\n",
      "        min_q: 13.614574432373047\n",
      "        model: {}\n",
      "        td_error: 0.030797043815255165\n",
      "    max_exploration: 0.5002\n",
      "    min_exploration: 0.5002\n",
      "    num_steps_sampled: 5200\n",
      "    num_steps_trained: 947200\n",
      "    num_target_updates: 5200\n",
      "    opt_peak_throughput: 55042.231\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.215\n",
      "    sample_time_ms: 75.415\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.68125\n",
      "    ram_util_percent: 70.53125\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 80.6003141650587\n",
      "    mean_inference_ms: 0.6371471302323161\n",
      "    mean_processing_ms: 5.739050742771232\n",
      "  time_since_restore: 533.0186984539032\n",
      "  time_this_iter_s: 10.195121049880981\n",
      "  time_total_s: 533.0186984539032\n",
      "  timestamp: 1582120666\n",
      "  timesteps_since_restore: 5200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5200\n",
      "  training_iteration: 52\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         533.019</td><td style=\"text-align: right;\">       5200</td><td style=\"text-align: right;\">   1.887</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-57-58\n",
      "  done: false\n",
      "  episode_len_mean: 19.89\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8890000281482935\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 291\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.50520104743971\n",
      "      mean_inference_ms: 0.7826254789549498\n",
      "      mean_processing_ms: 4.862149208523137\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 7.382\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.109365463256836\n",
      "        critic_loss: 0.01464342325925827\n",
      "        max_q: 16.311988830566406\n",
      "        mean_q: 14.690410614013672\n",
      "        min_q: 13.931314468383789\n",
      "        model: {}\n",
      "        td_error: 0.02928684838116169\n",
      "    max_exploration: 0.49039999999999995\n",
      "    min_exploration: 0.49039999999999995\n",
      "    num_steps_sampled: 5300\n",
      "    num_steps_trained: 972800\n",
      "    num_target_updates: 5300\n",
      "    opt_peak_throughput: 34679.231\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 25.959\n",
      "    sample_time_ms: 74.305\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.22352941176471\n",
      "    ram_util_percent: 70.54117647058825\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 80.39946500153349\n",
      "    mean_inference_ms: 0.6365568498659313\n",
      "    mean_processing_ms: 5.726275593185301\n",
      "  time_since_restore: 543.2224593162537\n",
      "  time_this_iter_s: 10.203760862350464\n",
      "  time_total_s: 543.2224593162537\n",
      "  timestamp: 1582120678\n",
      "  timesteps_since_restore: 5300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5300\n",
      "  training_iteration: 53\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         543.222</td><td style=\"text-align: right;\">       5300</td><td style=\"text-align: right;\">   1.889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 19.82\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8820000280439855\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 296\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.53823801665527\n",
      "      mean_inference_ms: 0.7880451748751449\n",
      "      mean_processing_ms: 4.853121616828094\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 7.982\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.266992568969727\n",
      "        critic_loss: 0.014368785545229912\n",
      "        max_q: 16.517377853393555\n",
      "        mean_q: 14.84475326538086\n",
      "        min_q: 13.807197570800781\n",
      "        model: {}\n",
      "        td_error: 0.028737571090459824\n",
      "    max_exploration: 0.4806\n",
      "    min_exploration: 0.4806\n",
      "    num_steps_sampled: 5400\n",
      "    num_steps_trained: 998400\n",
      "    num_target_updates: 5400\n",
      "    opt_peak_throughput: 32072.866\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 28.997\n",
      "    sample_time_ms: 72.15\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.9\n",
      "    ram_util_percent: 67.9470588235294\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 80.19707336293077\n",
      "    mean_inference_ms: 0.6362546361199465\n",
      "    mean_processing_ms: 5.714182061423318\n",
      "  time_since_restore: 553.4360342025757\n",
      "  time_this_iter_s: 10.213574886322021\n",
      "  time_total_s: 553.4360342025757\n",
      "  timestamp: 1582120690\n",
      "  timesteps_since_restore: 5400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5400\n",
      "  training_iteration: 54\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         553.436</td><td style=\"text-align: right;\">       5400</td><td style=\"text-align: right;\">   1.882</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 19.62\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.862000027745962\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 302\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.53352506520272\n",
      "      mean_inference_ms: 0.7898926031920924\n",
      "      mean_processing_ms: 4.8538927481570155\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 6.498\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.39073657989502\n",
      "        critic_loss: 0.014057529158890247\n",
      "        max_q: 16.26972007751465\n",
      "        mean_q: 14.983879089355469\n",
      "        min_q: 14.063982009887695\n",
      "        model: {}\n",
      "        td_error: 0.028115058317780495\n",
      "    max_exploration: 0.4708\n",
      "    min_exploration: 0.4708\n",
      "    num_steps_sampled: 5500\n",
      "    num_steps_trained: 1024000\n",
      "    num_target_updates: 5500\n",
      "    opt_peak_throughput: 39395.274\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 26.344\n",
      "    sample_time_ms: 76.175\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.277777777777786\n",
      "    ram_util_percent: 69.58888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 79.95775544587156\n",
      "    mean_inference_ms: 0.6361592128331638\n",
      "    mean_processing_ms: 5.701550644038972\n",
      "  time_since_restore: 563.7411231994629\n",
      "  time_this_iter_s: 10.305088996887207\n",
      "  time_total_s: 563.7411231994629\n",
      "  timestamp: 1582120703\n",
      "  timesteps_since_restore: 5500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5500\n",
      "  training_iteration: 55\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         563.741</td><td style=\"text-align: right;\">       5500</td><td style=\"text-align: right;\">   1.862</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-58-35\n",
      "  done: false\n",
      "  episode_len_mean: 19.32\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8320000272989274\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 308\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.52845690108293\n",
      "      mean_inference_ms: 0.7931424292507551\n",
      "      mean_processing_ms: 4.8553373245213995\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 6.559\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.51571273803711\n",
      "        critic_loss: 0.01439308375120163\n",
      "        max_q: 16.163496017456055\n",
      "        mean_q: 15.097982406616211\n",
      "        min_q: 14.398468971252441\n",
      "        model: {}\n",
      "        td_error: 0.02878616750240326\n",
      "    max_exploration: 0.46099999999999997\n",
      "    min_exploration: 0.46099999999999997\n",
      "    num_steps_sampled: 5600\n",
      "    num_steps_trained: 1049600\n",
      "    num_target_updates: 5600\n",
      "    opt_peak_throughput: 39027.559\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 26.964\n",
      "    sample_time_ms: 75.621\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.21111111111111\n",
      "    ram_util_percent: 69.8722222222222\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 79.72169694197122\n",
      "    mean_inference_ms: 0.6362923701273508\n",
      "    mean_processing_ms: 5.691639193092003\n",
      "  time_since_restore: 574.0495173931122\n",
      "  time_this_iter_s: 10.308394193649292\n",
      "  time_total_s: 574.0495173931122\n",
      "  timestamp: 1582120715\n",
      "  timesteps_since_restore: 5600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5600\n",
      "  training_iteration: 56\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          574.05</td><td style=\"text-align: right;\">       5600</td><td style=\"text-align: right;\">   1.832</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-58-47\n",
      "  done: false\n",
      "  episode_len_mean: 19.31\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.831000027284026\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 313\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.56431242707488\n",
      "      mean_inference_ms: 0.7925480604171753\n",
      "      mean_processing_ms: 4.844796154406164\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.428\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.572412490844727\n",
      "        critic_loss: 0.017567846924066544\n",
      "        max_q: 16.276416778564453\n",
      "        mean_q: 15.17966079711914\n",
      "        min_q: 14.396183967590332\n",
      "        model: {}\n",
      "        td_error: 0.03513569384813309\n",
      "    max_exploration: 0.45119999999999993\n",
      "    min_exploration: 0.45119999999999993\n",
      "    num_steps_sampled: 5700\n",
      "    num_steps_trained: 1075200\n",
      "    num_target_updates: 5700\n",
      "    opt_peak_throughput: 47166.966\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 22.441\n",
      "    sample_time_ms: 81.562\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.729411764705883\n",
      "    ram_util_percent: 70.37058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 79.52977793210293\n",
      "    mean_inference_ms: 0.6365811633171031\n",
      "    mean_processing_ms: 5.684990217432448\n",
      "  time_since_restore: 584.2522025108337\n",
      "  time_this_iter_s: 10.202685117721558\n",
      "  time_total_s: 584.2522025108337\n",
      "  timestamp: 1582120727\n",
      "  timesteps_since_restore: 5700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5700\n",
      "  training_iteration: 57\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         584.252</td><td style=\"text-align: right;\">       5700</td><td style=\"text-align: right;\">   1.831</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 19.25\n",
      "  episode_reward_max: 3.6000000536441803\n",
      "  episode_reward_mean: 1.8250000271946192\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 318\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.5891279091398\n",
      "      mean_inference_ms: 0.7898212903999238\n",
      "      mean_processing_ms: 4.837247384971832\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.544\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.777932167053223\n",
      "        critic_loss: 0.01552021224051714\n",
      "        max_q: 16.75365447998047\n",
      "        mean_q: 15.393366813659668\n",
      "        min_q: 14.535212516784668\n",
      "        model: {}\n",
      "        td_error: 0.03104042448103428\n",
      "    max_exploration: 0.4414\n",
      "    min_exploration: 0.4414\n",
      "    num_steps_sampled: 5800\n",
      "    num_steps_trained: 1100800\n",
      "    num_target_updates: 5800\n",
      "    opt_peak_throughput: 56341.331\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.954\n",
      "    sample_time_ms: 85.848\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.947058823529414\n",
      "    ram_util_percent: 70.4529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 79.34155056689606\n",
      "    mean_inference_ms: 0.6369319960275215\n",
      "    mean_processing_ms: 5.679836778624798\n",
      "  time_since_restore: 594.4480283260345\n",
      "  time_this_iter_s: 10.195825815200806\n",
      "  time_total_s: 594.4480283260345\n",
      "  timestamp: 1582120740\n",
      "  timesteps_since_restore: 5800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5800\n",
      "  training_iteration: 58\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         594.448</td><td style=\"text-align: right;\">       5800</td><td style=\"text-align: right;\">   1.825</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 19.27\n",
      "  episode_reward_max: 3.6000000536441803\n",
      "  episode_reward_mean: 1.8270000272244216\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 323\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.63626562058926\n",
      "      mean_inference_ms: 0.7876230403780937\n",
      "      mean_processing_ms: 4.823367670178413\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.656\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.83840560913086\n",
      "        critic_loss: 0.012763949111104012\n",
      "        max_q: 16.889110565185547\n",
      "        mean_q: 15.478558540344238\n",
      "        min_q: 14.679019927978516\n",
      "        model: {}\n",
      "        td_error: 0.025527896359562874\n",
      "    max_exploration: 0.4316000000000001\n",
      "    min_exploration: 0.4316000000000001\n",
      "    num_steps_sampled: 5900\n",
      "    num_steps_trained: 1126400\n",
      "    num_target_updates: 5900\n",
      "    opt_peak_throughput: 54981.071\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.099\n",
      "    sample_time_ms: 85.965\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.16111111111111\n",
      "    ram_util_percent: 70.6277777777778\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 79.15990483580396\n",
      "    mean_inference_ms: 0.6372874515953811\n",
      "    mean_processing_ms: 5.675205786233164\n",
      "  time_since_restore: 604.645256280899\n",
      "  time_this_iter_s: 10.197227954864502\n",
      "  time_total_s: 604.645256280899\n",
      "  timestamp: 1582120752\n",
      "  timesteps_since_restore: 5900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 5900\n",
      "  training_iteration: 59\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         604.645</td><td style=\"text-align: right;\">       5900</td><td style=\"text-align: right;\">   1.827</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-59-25\n",
      "  done: false\n",
      "  episode_len_mean: 19.09\n",
      "  episode_reward_max: 3.6000000536441803\n",
      "  episode_reward_mean: 1.8090000269562005\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 329\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.63322603253563\n",
      "      mean_inference_ms: 0.7864978110029732\n",
      "      mean_processing_ms: 4.8243697105601235\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.179\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.961847305297852\n",
      "        critic_loss: 0.010985195636749268\n",
      "        max_q: 16.831310272216797\n",
      "        mean_q: 15.60919189453125\n",
      "        min_q: 14.880831718444824\n",
      "        model: {}\n",
      "        td_error: 0.021970389410853386\n",
      "    max_exploration: 0.42180000000000006\n",
      "    min_exploration: 0.42180000000000006\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 1152000\n",
      "    num_target_updates: 6000\n",
      "    opt_peak_throughput: 61252.372\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.281\n",
      "    sample_time_ms: 88.087\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.555555555555554\n",
      "    ram_util_percent: 70.10000000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 78.95021172052728\n",
      "    mean_inference_ms: 0.6377195631476724\n",
      "    mean_processing_ms: 5.669774881945024\n",
      "  time_since_restore: 614.9396471977234\n",
      "  time_this_iter_s: 10.29439091682434\n",
      "  time_total_s: 614.9396471977234\n",
      "  timestamp: 1582120765\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 60\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">          614.94</td><td style=\"text-align: right;\">       6000</td><td style=\"text-align: right;\">   1.809</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-59-37\n",
      "  done: false\n",
      "  episode_len_mean: 19.06\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.806000026911497\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 334\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.63218224535551\n",
      "      mean_inference_ms: 0.783571316867661\n",
      "      mean_processing_ms: 4.826009544049377\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.158\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.041770935058594\n",
      "        critic_loss: 0.015682250261306763\n",
      "        max_q: 17.074817657470703\n",
      "        mean_q: 15.690847396850586\n",
      "        min_q: 14.859125137329102\n",
      "        model: {}\n",
      "        td_error: 0.031364500522613525\n",
      "    max_exploration: 0.41200000000000003\n",
      "    min_exploration: 0.41200000000000003\n",
      "    num_steps_sampled: 6100\n",
      "    num_steps_trained: 1177600\n",
      "    num_target_updates: 6100\n",
      "    opt_peak_throughput: 61568.118\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.843\n",
      "    sample_time_ms: 88.223\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.61176470588235\n",
      "    ram_util_percent: 69.2764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 78.7800996913959\n",
      "    mean_inference_ms: 0.6381178123401656\n",
      "    mean_processing_ms: 5.665292547672502\n",
      "  time_since_restore: 625.1310379505157\n",
      "  time_this_iter_s: 10.191390752792358\n",
      "  time_total_s: 625.1310379505157\n",
      "  timestamp: 1582120777\n",
      "  timesteps_since_restore: 6100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6100\n",
      "  training_iteration: 61\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         625.131</td><td style=\"text-align: right;\">       6100</td><td style=\"text-align: right;\">   1.806</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_08-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 19.16\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.8160000270605088\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 340\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.58381059830361\n",
      "      mean_inference_ms: 0.7815122426312926\n",
      "      mean_processing_ms: 4.840749337479105\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.328\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.167652130126953\n",
      "        critic_loss: 0.011211192235350609\n",
      "        max_q: 16.584562301635742\n",
      "        mean_q: 15.83088493347168\n",
      "        min_q: 15.117058753967285\n",
      "        model: {}\n",
      "        td_error: 0.022422384470701218\n",
      "    max_exploration: 0.4022\n",
      "    min_exploration: 0.4022\n",
      "    num_steps_sampled: 6200\n",
      "    num_steps_trained: 1203200\n",
      "    num_target_updates: 6200\n",
      "    opt_peak_throughput: 59144.339\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.851\n",
      "    sample_time_ms: 88.223\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.923529411764704\n",
      "    ram_util_percent: 69.23529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 78.58068904311824\n",
      "    mean_inference_ms: 0.6386707988320416\n",
      "    mean_processing_ms: 5.661267572391125\n",
      "  time_since_restore: 635.4235379695892\n",
      "  time_this_iter_s: 10.292500019073486\n",
      "  time_total_s: 635.4235379695892\n",
      "  timestamp: 1582120789\n",
      "  timesteps_since_restore: 6200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6200\n",
      "  training_iteration: 62\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         635.424</td><td style=\"text-align: right;\">       6200</td><td style=\"text-align: right;\">   1.816</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-00-01\n",
      "  done: false\n",
      "  episode_len_mean: 18.85\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.7850000265985728\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 346\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.63028058558265\n",
      "      mean_inference_ms: 0.7782595947690724\n",
      "      mean_processing_ms: 4.826588015402517\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.378\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.275630950927734\n",
      "        critic_loss: 0.012274688109755516\n",
      "        max_q: 17.528512954711914\n",
      "        mean_q: 15.95482063293457\n",
      "        min_q: 15.249104499816895\n",
      "        model: {}\n",
      "        td_error: 0.024549376219511032\n",
      "    max_exploration: 0.39239999999999997\n",
      "    min_exploration: 0.39239999999999997\n",
      "    num_steps_sampled: 6300\n",
      "    num_steps_trained: 1228800\n",
      "    num_target_updates: 6300\n",
      "    opt_peak_throughput: 58475.663\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.945\n",
      "    sample_time_ms: 88.14\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.258823529411767\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 78.38996386845889\n",
      "    mean_inference_ms: 0.6392494682406185\n",
      "    mean_processing_ms: 5.659897007271486\n",
      "  time_since_restore: 645.7158720493317\n",
      "  time_this_iter_s: 10.292334079742432\n",
      "  time_total_s: 645.7158720493317\n",
      "  timestamp: 1582120801\n",
      "  timesteps_since_restore: 6300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6300\n",
      "  training_iteration: 63\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         645.716</td><td style=\"text-align: right;\">       6300</td><td style=\"text-align: right;\">   1.785</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-00-13\n",
      "  done: false\n",
      "  episode_len_mean: 18.82\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.7820000265538694\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 351\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.60678203193847\n",
      "      mean_inference_ms: 0.7758128496502417\n",
      "      mean_processing_ms: 4.834268546500933\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.95\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.399934768676758\n",
      "        critic_loss: 0.012489235028624535\n",
      "        max_q: 16.96861457824707\n",
      "        mean_q: 16.087514877319336\n",
      "        min_q: 15.416369438171387\n",
      "        model: {}\n",
      "        td_error: 0.02497847191989422\n",
      "    max_exploration: 0.38260000000000005\n",
      "    min_exploration: 0.38260000000000005\n",
      "    num_steps_sampled: 6400\n",
      "    num_steps_trained: 1254400\n",
      "    num_target_updates: 6400\n",
      "    opt_peak_throughput: 64814.824\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.779\n",
      "    sample_time_ms: 89.758\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.33888888888889\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 78.24021584376291\n",
      "    mean_inference_ms: 0.6396500119413505\n",
      "    mean_processing_ms: 5.658782255217716\n",
      "  time_since_restore: 655.9084811210632\n",
      "  time_this_iter_s: 10.192609071731567\n",
      "  time_total_s: 655.9084811210632\n",
      "  timestamp: 1582120813\n",
      "  timesteps_since_restore: 6400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6400\n",
      "  training_iteration: 64\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         655.908</td><td style=\"text-align: right;\">       6400</td><td style=\"text-align: right;\">   1.782</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 18.94\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.7940000267326832\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.61705240405752\n",
      "      mean_inference_ms: 0.773448027749927\n",
      "      mean_processing_ms: 4.831282524027434\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.005\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.45046615600586\n",
      "        critic_loss: 0.013627365231513977\n",
      "        max_q: 16.93523406982422\n",
      "        mean_q: 16.125259399414062\n",
      "        min_q: 15.377484321594238\n",
      "        model: {}\n",
      "        td_error: 0.027254730463027954\n",
      "    max_exploration: 0.3728\n",
      "    min_exploration: 0.3728\n",
      "    num_steps_sampled: 6500\n",
      "    num_steps_trained: 1280000\n",
      "    num_target_updates: 6500\n",
      "    opt_peak_throughput: 63926.902\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.817\n",
      "    sample_time_ms: 89.574\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.694117647058828\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 78.10104921128355\n",
      "    mean_inference_ms: 0.6399261120650043\n",
      "    mean_processing_ms: 5.65750074358723\n",
      "  time_since_restore: 666.0998048782349\n",
      "  time_this_iter_s: 10.19132375717163\n",
      "  time_total_s: 666.0998048782349\n",
      "  timestamp: 1582120825\n",
      "  timesteps_since_restore: 6500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6500\n",
      "  training_iteration: 65\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">           666.1</td><td style=\"text-align: right;\">       6500</td><td style=\"text-align: right;\">   1.794</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-00-37\n",
      "  done: false\n",
      "  episode_len_mean: 18.8\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.780000026524067\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 362\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.66074128584428\n",
      "      mean_inference_ms: 0.7703686093950605\n",
      "      mean_processing_ms: 4.818944664268227\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.239\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.538196563720703\n",
      "        critic_loss: 0.011572795920073986\n",
      "        max_q: 16.918851852416992\n",
      "        mean_q: 16.228107452392578\n",
      "        min_q: 15.535493850708008\n",
      "        model: {}\n",
      "        td_error: 0.023145589977502823\n",
      "    max_exploration: 0.363\n",
      "    min_exploration: 0.363\n",
      "    num_steps_sampled: 6600\n",
      "    num_steps_trained: 1305600\n",
      "    num_target_updates: 6600\n",
      "    opt_peak_throughput: 60396.203\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.581\n",
      "    sample_time_ms: 88.241\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.13529411764706\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.94178778845469\n",
      "    mean_inference_ms: 0.6401954702313667\n",
      "    mean_processing_ms: 5.655951233811573\n",
      "  time_since_restore: 676.3916578292847\n",
      "  time_this_iter_s: 10.291852951049805\n",
      "  time_total_s: 676.3916578292847\n",
      "  timestamp: 1582120837\n",
      "  timesteps_since_restore: 6600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6600\n",
      "  training_iteration: 66\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         676.392</td><td style=\"text-align: right;\">       6600</td><td style=\"text-align: right;\">    1.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 18.8\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.780000026524067\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 367\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.62696793197927\n",
      "      mean_inference_ms: 0.7681836739429453\n",
      "      mean_processing_ms: 4.82981557345522\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.263\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.648372650146484\n",
      "        critic_loss: 0.011921558529138565\n",
      "        max_q: 17.208059310913086\n",
      "        mean_q: 16.362567901611328\n",
      "        min_q: 15.680883407592773\n",
      "        model: {}\n",
      "        td_error: 0.02384311705827713\n",
      "    max_exploration: 0.35319999999999996\n",
      "    min_exploration: 0.35319999999999996\n",
      "    num_steps_sampled: 6700\n",
      "    num_steps_trained: 1331200\n",
      "    num_target_updates: 6700\n",
      "    opt_peak_throughput: 60047.301\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.698\n",
      "    sample_time_ms: 78.364\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.26111111111111\n",
      "    ram_util_percent: 69.20555555555558\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.81493156930536\n",
      "    mean_inference_ms: 0.6403973216848755\n",
      "    mean_processing_ms: 5.654749750861072\n",
      "  time_since_restore: 686.5843069553375\n",
      "  time_this_iter_s: 10.192649126052856\n",
      "  time_total_s: 686.5843069553375\n",
      "  timestamp: 1582120850\n",
      "  timesteps_since_restore: 6700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6700\n",
      "  training_iteration: 67\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         686.584</td><td style=\"text-align: right;\">       6700</td><td style=\"text-align: right;\">    1.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 18.94\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.7940000267326832\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 372\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.58283628782722\n",
      "      mean_inference_ms: 0.7667193233763399\n",
      "      mean_processing_ms: 4.843008640276287\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.103\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.644271850585938\n",
      "        critic_loss: 0.010612363927066326\n",
      "        max_q: 16.99202537536621\n",
      "        mean_q: 16.364425659179688\n",
      "        min_q: 15.531322479248047\n",
      "        model: {}\n",
      "        td_error: 0.0212247297167778\n",
      "    max_exploration: 0.3433999999999999\n",
      "    min_exploration: 0.3433999999999999\n",
      "    num_steps_sampled: 6800\n",
      "    num_steps_trained: 1356800\n",
      "    num_target_updates: 6800\n",
      "    opt_peak_throughput: 62392.752\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.668\n",
      "    sample_time_ms: 88.855\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.55625\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.69115917848636\n",
      "    mean_inference_ms: 0.6406206810530617\n",
      "    mean_processing_ms: 5.653867226324003\n",
      "  time_since_restore: 696.7779369354248\n",
      "  time_this_iter_s: 10.19362998008728\n",
      "  time_total_s: 696.7779369354248\n",
      "  timestamp: 1582120861\n",
      "  timesteps_since_restore: 6800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6800\n",
      "  training_iteration: 68\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         696.778</td><td style=\"text-align: right;\">       6800</td><td style=\"text-align: right;\">   1.794</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 18.6\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.7600000262260438\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 378\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.5612755258771\n",
      "      mean_inference_ms: 0.7647513700624039\n",
      "      mean_processing_ms: 4.850281216706227\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.127\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.72408676147461\n",
      "        critic_loss: 0.011039840057492256\n",
      "        max_q: 17.51818084716797\n",
      "        mean_q: 16.452054977416992\n",
      "        min_q: 15.592458724975586\n",
      "        model: {}\n",
      "        td_error: 0.022079680114984512\n",
      "    max_exploration: 0.3336\n",
      "    min_exploration: 0.3336\n",
      "    num_steps_sampled: 6900\n",
      "    num_steps_trained: 1382400\n",
      "    num_target_updates: 6900\n",
      "    opt_peak_throughput: 62025.846\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.494\n",
      "    sample_time_ms: 88.687\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.876470588235293\n",
      "    ram_util_percent: 69.29411764705881\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.54584771864579\n",
      "    mean_inference_ms: 0.6409258101901106\n",
      "    mean_processing_ms: 5.654812672892397\n",
      "  time_since_restore: 707.0705738067627\n",
      "  time_this_iter_s: 10.29263687133789\n",
      "  time_total_s: 707.0705738067627\n",
      "  timestamp: 1582120873\n",
      "  timesteps_since_restore: 6900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 6900\n",
      "  training_iteration: 69\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         707.071</td><td style=\"text-align: right;\">       6900</td><td style=\"text-align: right;\">    1.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 18.71\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.7710000263899566\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 382\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.58224844726887\n",
      "      mean_inference_ms: 0.7623865181986829\n",
      "      mean_processing_ms: 4.843679104091794\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.139\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.820119857788086\n",
      "        critic_loss: 0.007657681591808796\n",
      "        max_q: 17.391414642333984\n",
      "        mean_q: 16.566974639892578\n",
      "        min_q: 15.96218490600586\n",
      "        model: {}\n",
      "        td_error: 0.015315364114940166\n",
      "    max_exploration: 0.3238000000000001\n",
      "    min_exploration: 0.3238000000000001\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 1408000\n",
      "    num_target_updates: 7000\n",
      "    opt_peak_throughput: 61854.338\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.81\n",
      "    sample_time_ms: 78.583\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.658823529411762\n",
      "    ram_util_percent: 69.3\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.45165835955146\n",
      "    mean_inference_ms: 0.6411204415518346\n",
      "    mean_processing_ms: 5.65493403883482\n",
      "  time_since_restore: 717.1643998622894\n",
      "  time_this_iter_s: 10.093826055526733\n",
      "  time_total_s: 717.1643998622894\n",
      "  timestamp: 1582120885\n",
      "  timesteps_since_restore: 7000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 70\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         717.164</td><td style=\"text-align: right;\">       7000</td><td style=\"text-align: right;\">   1.771</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 18.46\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7460000260174275\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 388\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.58253312360554\n",
      "      mean_inference_ms: 0.7600208539613255\n",
      "      mean_processing_ms: 4.844053253453439\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.077\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.82343292236328\n",
      "        critic_loss: 0.009341234341263771\n",
      "        max_q: 17.49578857421875\n",
      "        mean_q: 16.559062957763672\n",
      "        min_q: 15.673433303833008\n",
      "        model: {}\n",
      "        td_error: 0.01868247054517269\n",
      "    max_exploration: 0.31400000000000006\n",
      "    min_exploration: 0.31400000000000006\n",
      "    num_steps_sampled: 7100\n",
      "    num_steps_trained: 1433600\n",
      "    num_target_updates: 7100\n",
      "    opt_peak_throughput: 62793.021\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.03\n",
      "    sample_time_ms: 79.144\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.600000000000001\n",
      "    ram_util_percent: 69.3\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.32254718079474\n",
      "    mean_inference_ms: 0.6412657424003926\n",
      "    mean_processing_ms: 5.655202657058038\n",
      "  time_since_restore: 727.4565608501434\n",
      "  time_this_iter_s: 10.292160987854004\n",
      "  time_total_s: 727.4565608501434\n",
      "  timestamp: 1582120897\n",
      "  timesteps_since_restore: 7100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7100\n",
      "  training_iteration: 71\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         727.457</td><td style=\"text-align: right;\">       7100</td><td style=\"text-align: right;\">   1.746</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 18.44\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7440000259876252\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 393\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.53199304076674\n",
      "      mean_inference_ms: 0.7580968073612667\n",
      "      mean_processing_ms: 4.859825181219862\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.057\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.901527404785156\n",
      "        critic_loss: 0.009384632110595703\n",
      "        max_q: 17.560213088989258\n",
      "        mean_q: 16.651893615722656\n",
      "        min_q: 15.697147369384766\n",
      "        model: {}\n",
      "        td_error: 0.018769264221191406\n",
      "    max_exploration: 0.3042\n",
      "    min_exploration: 0.3042\n",
      "    num_steps_sampled: 7200\n",
      "    num_steps_trained: 1459200\n",
      "    num_target_updates: 7200\n",
      "    opt_peak_throughput: 63098.186\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.54\n",
      "    sample_time_ms: 79.93\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.01764705882353\n",
      "    ram_util_percent: 69.3\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.23285344377909\n",
      "    mean_inference_ms: 0.6410507706548679\n",
      "    mean_processing_ms: 5.655346221260151\n",
      "  time_since_restore: 737.6483688354492\n",
      "  time_this_iter_s: 10.191807985305786\n",
      "  time_total_s: 737.6483688354492\n",
      "  timestamp: 1582120910\n",
      "  timesteps_since_restore: 7200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7200\n",
      "  training_iteration: 72\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         737.648</td><td style=\"text-align: right;\">       7200</td><td style=\"text-align: right;\">   1.744</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 18.57\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7570000261813403\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 399\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 13.0\n",
      "    episode_reward_max: 1.2000000178813934\n",
      "    episode_reward_mean: 1.2000000178813934\n",
      "    episode_reward_min: 1.2000000178813934\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.45214801486976\n",
      "      mean_inference_ms: 0.756497710809236\n",
      "      mean_processing_ms: 4.8840761031565485\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.971\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.908069610595703\n",
      "        critic_loss: 0.008226856589317322\n",
      "        max_q: 17.255046844482422\n",
      "        mean_q: 16.662153244018555\n",
      "        min_q: 16.005338668823242\n",
      "        model: {}\n",
      "        td_error: 0.016453713178634644\n",
      "    max_exploration: 0.2944\n",
      "    min_exploration: 0.2944\n",
      "    num_steps_sampled: 7300\n",
      "    num_steps_trained: 1484800\n",
      "    num_target_updates: 7300\n",
      "    opt_peak_throughput: 64462.284\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.214\n",
      "    sample_time_ms: 89.848\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.7375\n",
      "    ram_util_percent: 69.31875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.14597484952559\n",
      "    mean_inference_ms: 0.6403931059739595\n",
      "    mean_processing_ms: 5.655675360972018\n",
      "  time_since_restore: 747.9386789798737\n",
      "  time_this_iter_s: 10.290310144424438\n",
      "  time_total_s: 747.9386789798737\n",
      "  timestamp: 1582120921\n",
      "  timesteps_since_restore: 7300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7300\n",
      "  training_iteration: 73\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         747.939</td><td style=\"text-align: right;\">       7300</td><td style=\"text-align: right;\">   1.757</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-02-12\n",
      "  done: false\n",
      "  episode_len_mean: 18.66\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7660000263154507\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 404\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.52412350170243\n",
      "      mean_inference_ms: 0.7531811386252804\n",
      "      mean_processing_ms: 4.862907782714074\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.933\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.98830795288086\n",
      "        critic_loss: 0.007142842747271061\n",
      "        max_q: 17.4792537689209\n",
      "        mean_q: 16.755413055419922\n",
      "        min_q: 15.914578437805176\n",
      "        model: {}\n",
      "        td_error: 0.014285685494542122\n",
      "    max_exploration: 0.2846000000000001\n",
      "    min_exploration: 0.2846000000000001\n",
      "    num_steps_sampled: 7400\n",
      "    num_steps_trained: 1510400\n",
      "    num_target_updates: 7400\n",
      "    opt_peak_throughput: 65083.151\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.858\n",
      "    sample_time_ms: 90.018\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.3\n",
      "    ram_util_percent: 69.55\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.08697437940633\n",
      "    mean_inference_ms: 0.6395713384666856\n",
      "    mean_processing_ms: 5.655055836002383\n",
      "  time_since_restore: 758.1335508823395\n",
      "  time_this_iter_s: 10.19487190246582\n",
      "  time_total_s: 758.1335508823395\n",
      "  timestamp: 1582120932\n",
      "  timesteps_since_restore: 7400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7400\n",
      "  training_iteration: 74\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         758.134</td><td style=\"text-align: right;\">       7400</td><td style=\"text-align: right;\">   1.766</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 18.59\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7590000262111425\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 410\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.50473880173263\n",
      "      mean_inference_ms: 0.7519576317651611\n",
      "      mean_processing_ms: 4.868604893101718\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.271\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.050743103027344\n",
      "        critic_loss: 0.009675299748778343\n",
      "        max_q: 17.409196853637695\n",
      "        mean_q: 16.822860717773438\n",
      "        min_q: 16.149839401245117\n",
      "        model: {}\n",
      "        td_error: 0.019350599497556686\n",
      "    max_exploration: 0.27480000000000004\n",
      "    min_exploration: 0.27480000000000004\n",
      "    num_steps_sampled: 7500\n",
      "    num_steps_trained: 1536000\n",
      "    num_target_updates: 7500\n",
      "    opt_peak_throughput: 59938.363\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.473\n",
      "    sample_time_ms: 86.129\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.731578947368423\n",
      "    ram_util_percent: 69.85789473684211\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 77.03074473086207\n",
      "    mean_inference_ms: 0.6383019477960099\n",
      "    mean_processing_ms: 5.65428395778816\n",
      "  time_since_restore: 768.4271590709686\n",
      "  time_this_iter_s: 10.29360818862915\n",
      "  time_total_s: 768.4271590709686\n",
      "  timestamp: 1582120945\n",
      "  timesteps_since_restore: 7500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7500\n",
      "  training_iteration: 75\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         768.427</td><td style=\"text-align: right;\">       7500</td><td style=\"text-align: right;\">   1.759</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-02-37\n",
      "  done: false\n",
      "  episode_len_mean: 18.5\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.750000026077032\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 415\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.48624969219178\n",
      "      mean_inference_ms: 0.7505496196076963\n",
      "      mean_processing_ms: 4.874767819379335\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.54\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.091854095458984\n",
      "        critic_loss: 0.0075586289167404175\n",
      "        max_q: 17.591190338134766\n",
      "        mean_q: 16.86160659790039\n",
      "        min_q: 16.14426040649414\n",
      "        model: {}\n",
      "        td_error: 0.015117255970835686\n",
      "    max_exploration: 0.265\n",
      "    min_exploration: 0.265\n",
      "    num_steps_sampled: 7600\n",
      "    num_steps_trained: 1561600\n",
      "    num_target_updates: 7600\n",
      "    opt_peak_throughput: 56393.411\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.113\n",
      "    sample_time_ms: 86.806\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.264705882352942\n",
      "    ram_util_percent: 71.16470588235293\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.9922878137421\n",
      "    mean_inference_ms: 0.6371164665458271\n",
      "    mean_processing_ms: 5.65364734457447\n",
      "  time_since_restore: 778.6227562427521\n",
      "  time_this_iter_s: 10.195597171783447\n",
      "  time_total_s: 778.6227562427521\n",
      "  timestamp: 1582120957\n",
      "  timesteps_since_restore: 7600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7600\n",
      "  training_iteration: 76\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         778.623</td><td style=\"text-align: right;\">       7600</td><td style=\"text-align: right;\">    1.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 18.39\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7390000259131193\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 420\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.4775912783455\n",
      "      mean_inference_ms: 0.7493864524966685\n",
      "      mean_processing_ms: 4.8773195855296825\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.903\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.07494354248047\n",
      "        critic_loss: 0.0077851396054029465\n",
      "        max_q: 17.602514266967773\n",
      "        mean_q: 16.860713958740234\n",
      "        min_q: 16.077016830444336\n",
      "        model: {}\n",
      "        td_error: 0.015570278279483318\n",
      "    max_exploration: 0.2552\n",
      "    min_exploration: 0.2552\n",
      "    num_steps_sampled: 7700\n",
      "    num_steps_trained: 1587200\n",
      "    num_target_updates: 7700\n",
      "    opt_peak_throughput: 52214.638\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.583\n",
      "    sample_time_ms: 73.673\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.91875\n",
      "    ram_util_percent: 71.58125\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.95504549639057\n",
      "    mean_inference_ms: 0.6359619460146786\n",
      "    mean_processing_ms: 5.652988358330026\n",
      "  time_since_restore: 788.8187851905823\n",
      "  time_this_iter_s: 10.1960289478302\n",
      "  time_total_s: 788.8187851905823\n",
      "  timestamp: 1582120969\n",
      "  timesteps_since_restore: 7700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7700\n",
      "  training_iteration: 77\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         788.819</td><td style=\"text-align: right;\">       7700</td><td style=\"text-align: right;\">   1.739</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-03-01\n",
      "  done: false\n",
      "  episode_len_mean: 18.62\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.762000026255846\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 425\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.43072581391797\n",
      "      mean_inference_ms: 0.7485519912747319\n",
      "      mean_processing_ms: 4.891301006204468\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.627\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.177804946899414\n",
      "        critic_loss: 0.007115702144801617\n",
      "        max_q: 17.50815200805664\n",
      "        mean_q: 16.96880531311035\n",
      "        min_q: 16.434959411621094\n",
      "        model: {}\n",
      "        td_error: 0.014231404289603233\n",
      "    max_exploration: 0.24539999999999995\n",
      "    min_exploration: 0.24539999999999995\n",
      "    num_steps_sampled: 7800\n",
      "    num_steps_trained: 1612800\n",
      "    num_target_updates: 7800\n",
      "    opt_peak_throughput: 55330.689\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.487\n",
      "    sample_time_ms: 76.245\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.08235294117647\n",
      "    ram_util_percent: 71.69999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.91777988878151\n",
      "    mean_inference_ms: 0.6348695857167707\n",
      "    mean_processing_ms: 5.652056744212513\n",
      "  time_since_restore: 799.0148901939392\n",
      "  time_this_iter_s: 10.196105003356934\n",
      "  time_total_s: 799.0148901939392\n",
      "  timestamp: 1582120981\n",
      "  timesteps_since_restore: 7800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7800\n",
      "  training_iteration: 78\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         799.015</td><td style=\"text-align: right;\">       7800</td><td style=\"text-align: right;\">   1.762</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-03-13\n",
      "  done: false\n",
      "  episode_len_mean: 18.55\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.755000026151538\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 431\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.41374542204501\n",
      "      mean_inference_ms: 0.7467701835768725\n",
      "      mean_processing_ms: 4.897129123628637\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.098\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.212268829345703\n",
      "        critic_loss: 0.007892819121479988\n",
      "        max_q: 17.60776710510254\n",
      "        mean_q: 17.016460418701172\n",
      "        min_q: 16.525745391845703\n",
      "        model: {}\n",
      "        td_error: 0.015785640105605125\n",
      "    max_exploration: 0.23560000000000003\n",
      "    min_exploration: 0.23560000000000003\n",
      "    num_steps_sampled: 7900\n",
      "    num_steps_trained: 1638400\n",
      "    num_target_updates: 7900\n",
      "    opt_peak_throughput: 62471.161\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.301\n",
      "    sample_time_ms: 79.142\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.776470588235295\n",
      "    ram_util_percent: 71.71176470588236\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.87439712782174\n",
      "    mean_inference_ms: 0.6336040820139036\n",
      "    mean_processing_ms: 5.650453812849385\n",
      "  time_since_restore: 809.3078484535217\n",
      "  time_this_iter_s: 10.29295825958252\n",
      "  time_total_s: 809.3078484535217\n",
      "  timestamp: 1582120993\n",
      "  timesteps_since_restore: 7900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 7900\n",
      "  training_iteration: 79\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         809.308</td><td style=\"text-align: right;\">       7900</td><td style=\"text-align: right;\">   1.755</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 18.66\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7660000263154507\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 436\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.37822143307126\n",
      "      mean_inference_ms: 0.7450629827898864\n",
      "      mean_processing_ms: 4.908463807232612\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.188\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.265287399291992\n",
      "        critic_loss: 0.007676049135625362\n",
      "        max_q: 17.624486923217773\n",
      "        mean_q: 17.073326110839844\n",
      "        min_q: 16.513608932495117\n",
      "        model: {}\n",
      "        td_error: 0.015352096408605576\n",
      "    max_exploration: 0.2258\n",
      "    min_exploration: 0.2258\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 1664000\n",
      "    num_target_updates: 8000\n",
      "    opt_peak_throughput: 61129.275\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.392\n",
      "    sample_time_ms: 87.71\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.605882352941176\n",
      "    ram_util_percent: 71.96470588235294\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.84002823038358\n",
      "    mean_inference_ms: 0.6325643877219848\n",
      "    mean_processing_ms: 5.648896899574117\n",
      "  time_since_restore: 819.498836517334\n",
      "  time_this_iter_s: 10.190988063812256\n",
      "  time_total_s: 819.498836517334\n",
      "  timestamp: 1582121005\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 80\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         819.499</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\">   1.766</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-03-36\n",
      "  done: false\n",
      "  episode_len_mean: 18.78\n",
      "  episode_reward_max: 2.9000000432133675\n",
      "  episode_reward_mean: 1.7780000264942646\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 441\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.40848497764672\n",
      "      mean_inference_ms: 0.7427860790383615\n",
      "      mean_processing_ms: 4.8997332843117\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.014\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.25641632080078\n",
      "        critic_loss: 0.007740487344563007\n",
      "        max_q: 17.78165054321289\n",
      "        mean_q: 17.072586059570312\n",
      "        min_q: 16.383392333984375\n",
      "        model: {}\n",
      "        td_error: 0.015480974689126015\n",
      "    max_exploration: 0.21599999999999997\n",
      "    min_exploration: 0.21599999999999997\n",
      "    num_steps_sampled: 8100\n",
      "    num_steps_trained: 1689600\n",
      "    num_target_updates: 8100\n",
      "    opt_peak_throughput: 63773.888\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.309\n",
      "    sample_time_ms: 88.353\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.841176470588234\n",
      "    ram_util_percent: 72.09411764705884\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.80680280936943\n",
      "    mean_inference_ms: 0.6315179650606546\n",
      "    mean_processing_ms: 5.6466957343162\n",
      "  time_since_restore: 829.6908645629883\n",
      "  time_this_iter_s: 10.192028045654297\n",
      "  time_total_s: 829.6908645629883\n",
      "  timestamp: 1582121016\n",
      "  timesteps_since_restore: 8100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8100\n",
      "  training_iteration: 81\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         829.691</td><td style=\"text-align: right;\">       8100</td><td style=\"text-align: right;\">   1.778</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 19.07\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.8070000269263984\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 446\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.46389044601074\n",
      "      mean_inference_ms: 0.7415940261788384\n",
      "      mean_processing_ms: 4.883120832847429\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.049\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.260154724121094\n",
      "        critic_loss: 0.00635792501270771\n",
      "        max_q: 17.765460968017578\n",
      "        mean_q: 17.068817138671875\n",
      "        min_q: 16.39816665649414\n",
      "        model: {}\n",
      "        td_error: 0.01271585002541542\n",
      "    max_exploration: 0.20619999999999994\n",
      "    min_exploration: 0.20619999999999994\n",
      "    num_steps_sampled: 8200\n",
      "    num_steps_trained: 1715200\n",
      "    num_target_updates: 8200\n",
      "    opt_peak_throughput: 63220.041\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.883\n",
      "    sample_time_ms: 89.491\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.67058823529412\n",
      "    ram_util_percent: 72.17058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.7747287306546\n",
      "    mean_inference_ms: 0.6304650268203283\n",
      "    mean_processing_ms: 5.643897802841366\n",
      "  time_since_restore: 839.8825716972351\n",
      "  time_this_iter_s: 10.191707134246826\n",
      "  time_total_s: 839.8825716972351\n",
      "  timestamp: 1582121029\n",
      "  timesteps_since_restore: 8200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8200\n",
      "  training_iteration: 82\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         839.883</td><td style=\"text-align: right;\">       8200</td><td style=\"text-align: right;\">   1.807</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-04-01\n",
      "  done: false\n",
      "  episode_len_mean: 18.99\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.799000026807189\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 451\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.47403477219974\n",
      "      mean_inference_ms: 0.7397172407866603\n",
      "      mean_processing_ms: 4.880343896770909\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.036\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.309406280517578\n",
      "        critic_loss: 0.006742890924215317\n",
      "        max_q: 17.79950523376465\n",
      "        mean_q: 17.12679100036621\n",
      "        min_q: 16.446529388427734\n",
      "        model: {}\n",
      "        td_error: 0.013485781848430634\n",
      "    max_exploration: 0.19640000000000002\n",
      "    min_exploration: 0.19640000000000002\n",
      "    num_steps_sampled: 8300\n",
      "    num_steps_trained: 1740800\n",
      "    num_target_updates: 8300\n",
      "    opt_peak_throughput: 63435.174\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.823\n",
      "    sample_time_ms: 89.567\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.85\n",
      "    ram_util_percent: 72.31666666666666\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.7432414827726\n",
      "    mean_inference_ms: 0.6294421572768293\n",
      "    mean_processing_ms: 5.641210045547458\n",
      "  time_since_restore: 850.0741076469421\n",
      "  time_this_iter_s: 10.191535949707031\n",
      "  time_total_s: 850.0741076469421\n",
      "  timestamp: 1582121041\n",
      "  timesteps_since_restore: 8300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8300\n",
      "  training_iteration: 83\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         850.074</td><td style=\"text-align: right;\">       8300</td><td style=\"text-align: right;\">   1.799</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 18.89\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.7890000266581774\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 456\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.48429442783974\n",
      "      mean_inference_ms: 0.7380561455668018\n",
      "      mean_processing_ms: 4.877550375528176\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.298\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.344669342041016\n",
      "        critic_loss: 0.007067517377436161\n",
      "        max_q: 17.923011779785156\n",
      "        mean_q: 17.16671371459961\n",
      "        min_q: 16.531042098999023\n",
      "        model: {}\n",
      "        td_error: 0.014135034754872322\n",
      "    max_exploration: 0.1866000000000001\n",
      "    min_exploration: 0.1866000000000001\n",
      "    num_steps_sampled: 8400\n",
      "    num_steps_trained: 1766400\n",
      "    num_target_updates: 8400\n",
      "    opt_peak_throughput: 59567.936\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.625\n",
      "    sample_time_ms: 78.534\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.323529411764707\n",
      "    ram_util_percent: 73.37058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.71227428232761\n",
      "    mean_inference_ms: 0.6284524423341032\n",
      "    mean_processing_ms: 5.638626243383587\n",
      "  time_since_restore: 860.2667405605316\n",
      "  time_this_iter_s: 10.192632913589478\n",
      "  time_total_s: 860.2667405605316\n",
      "  timestamp: 1582121054\n",
      "  timesteps_since_restore: 8400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8400\n",
      "  training_iteration: 84\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         860.267</td><td style=\"text-align: right;\">       8400</td><td style=\"text-align: right;\">   1.789</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 19.1\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.8100000269711018\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 461\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.52755975093085\n",
      "      mean_inference_ms: 0.737521748185683\n",
      "      mean_processing_ms: 4.86398032058178\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.655\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.375198364257812\n",
      "        critic_loss: 0.0063208467327058315\n",
      "        max_q: 17.737722396850586\n",
      "        mean_q: 17.1934814453125\n",
      "        min_q: 16.69619369506836\n",
      "        model: {}\n",
      "        td_error: 0.012641694396734238\n",
      "    max_exploration: 0.17680000000000007\n",
      "    min_exploration: 0.17680000000000007\n",
      "    num_steps_sampled: 8500\n",
      "    num_steps_trained: 1792000\n",
      "    num_target_updates: 8500\n",
      "    opt_peak_throughput: 54993.461\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.112\n",
      "    sample_time_ms: 86.742\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.094444444444445\n",
      "    ram_util_percent: 73.28888888888888\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.68123041057653\n",
      "    mean_inference_ms: 0.6275149596736358\n",
      "    mean_processing_ms: 5.635335329788108\n",
      "  time_since_restore: 870.4617385864258\n",
      "  time_this_iter_s: 10.194998025894165\n",
      "  time_total_s: 870.4617385864258\n",
      "  timestamp: 1582121066\n",
      "  timesteps_since_restore: 8500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8500\n",
      "  training_iteration: 85\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         870.462</td><td style=\"text-align: right;\">       8500</td><td style=\"text-align: right;\">    1.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 19.33\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.8330000273138285\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 466\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.51040107352857\n",
      "      mean_inference_ms: 0.7368664650566247\n",
      "      mean_processing_ms: 4.869679469178743\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.647\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.380159378051758\n",
      "        critic_loss: 0.006423284765332937\n",
      "        max_q: 17.938051223754883\n",
      "        mean_q: 17.214340209960938\n",
      "        min_q: 16.686872482299805\n",
      "        model: {}\n",
      "        td_error: 0.012846570461988449\n",
      "    max_exploration: 0.16700000000000004\n",
      "    min_exploration: 0.16700000000000004\n",
      "    num_steps_sampled: 8600\n",
      "    num_steps_trained: 1817600\n",
      "    num_target_updates: 8600\n",
      "    opt_peak_throughput: 55093.632\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.515\n",
      "    sample_time_ms: 85.267\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.805555555555557\n",
      "    ram_util_percent: 68.6611111111111\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.64817540824413\n",
      "    mean_inference_ms: 0.6266370033144334\n",
      "    mean_processing_ms: 5.6321148478794365\n",
      "  time_since_restore: 880.6581554412842\n",
      "  time_this_iter_s: 10.196416854858398\n",
      "  time_total_s: 880.6581554412842\n",
      "  timestamp: 1582121078\n",
      "  timesteps_since_restore: 8600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8600\n",
      "  training_iteration: 86\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         880.658</td><td style=\"text-align: right;\">       8600</td><td style=\"text-align: right;\">   1.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-04-50\n",
      "  done: false\n",
      "  episode_len_mean: 19.34\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.8340000273287296\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 470\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.5529187236788\n",
      "      mean_inference_ms: 0.7361371564583263\n",
      "      mean_processing_ms: 4.8569504257685905\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.046\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.384780883789062\n",
      "        critic_loss: 0.0075098732486367226\n",
      "        max_q: 17.95191192626953\n",
      "        mean_q: 17.21957778930664\n",
      "        min_q: 16.731731414794922\n",
      "        model: {}\n",
      "        td_error: 0.01501974742859602\n",
      "    max_exploration: 0.1572\n",
      "    min_exploration: 0.1572\n",
      "    num_steps_sampled: 8700\n",
      "    num_steps_trained: 1843200\n",
      "    num_target_updates: 8700\n",
      "    opt_peak_throughput: 50735.071\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.971\n",
      "    sample_time_ms: 72.847\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.125\n",
      "    ram_util_percent: 67.975\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.62059567015943\n",
      "    mean_inference_ms: 0.625986845845706\n",
      "    mean_processing_ms: 5.629192119276588\n",
      "  time_since_restore: 890.7532982826233\n",
      "  time_this_iter_s: 10.095142841339111\n",
      "  time_total_s: 890.7532982826233\n",
      "  timestamp: 1582121090\n",
      "  timesteps_since_restore: 8700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8700\n",
      "  training_iteration: 87\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         890.753</td><td style=\"text-align: right;\">       8700</td><td style=\"text-align: right;\">   1.834</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-05-03\n",
      "  done: false\n",
      "  episode_len_mean: 19.52\n",
      "  episode_reward_max: 3.0000000447034836\n",
      "  episode_reward_mean: 1.8520000275969506\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 475\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.55268509811093\n",
      "      mean_inference_ms: 0.7354888855207485\n",
      "      mean_processing_ms: 4.856617493786543\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.509\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.434349060058594\n",
      "        critic_loss: 0.006882883608341217\n",
      "        max_q: 17.87297248840332\n",
      "        mean_q: 17.27886962890625\n",
      "        min_q: 16.62311553955078\n",
      "        model: {}\n",
      "        td_error: 0.013765767216682434\n",
      "    max_exploration: 0.14739999999999998\n",
      "    min_exploration: 0.14739999999999998\n",
      "    num_steps_sampled: 8800\n",
      "    num_steps_trained: 1868800\n",
      "    num_target_updates: 8800\n",
      "    opt_peak_throughput: 56770.285\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.453\n",
      "    sample_time_ms: 86.437\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.877777777777776\n",
      "    ram_util_percent: 68.6388888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.58496586213715\n",
      "    mean_inference_ms: 0.6252234937641479\n",
      "    mean_processing_ms: 5.625183079729959\n",
      "  time_since_restore: 900.9484074115753\n",
      "  time_this_iter_s: 10.195109128952026\n",
      "  time_total_s: 900.9484074115753\n",
      "  timestamp: 1582121103\n",
      "  timesteps_since_restore: 8800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8800\n",
      "  training_iteration: 88\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         900.948</td><td style=\"text-align: right;\">       8800</td><td style=\"text-align: right;\">   1.852</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-05-15\n",
      "  done: false\n",
      "  episode_len_mean: 19.61\n",
      "  episode_reward_max: 3.300000049173832\n",
      "  episode_reward_mean: 1.861000027731061\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 480\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.54465517711941\n",
      "      mean_inference_ms: 0.7349791712314674\n",
      "      mean_processing_ms: 4.858808186528059\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.646\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.42451286315918\n",
      "        critic_loss: 0.006775095593184233\n",
      "        max_q: 17.903350830078125\n",
      "        mean_q: 17.281810760498047\n",
      "        min_q: 16.868907928466797\n",
      "        model: {}\n",
      "        td_error: 0.01355019025504589\n",
      "    max_exploration: 0.13760000000000006\n",
      "    min_exploration: 0.13760000000000006\n",
      "    num_steps_sampled: 8900\n",
      "    num_steps_trained: 1894400\n",
      "    num_target_updates: 8900\n",
      "    opt_peak_throughput: 55096.459\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.236\n",
      "    sample_time_ms: 86.322\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.91176470588235\n",
      "    ram_util_percent: 67.95294117647059\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.54832273149485\n",
      "    mean_inference_ms: 0.6245075498663998\n",
      "    mean_processing_ms: 5.621222059057882\n",
      "  time_since_restore: 911.143087387085\n",
      "  time_this_iter_s: 10.194679975509644\n",
      "  time_total_s: 911.143087387085\n",
      "  timestamp: 1582121115\n",
      "  timesteps_since_restore: 8900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8900\n",
      "  training_iteration: 89\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         911.143</td><td style=\"text-align: right;\">       8900</td><td style=\"text-align: right;\">   1.861</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 19.43\n",
      "  episode_reward_max: 3.300000049173832\n",
      "  episode_reward_mean: 1.84300002746284\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 486\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.57752872738247\n",
      "      mean_inference_ms: 0.7337529113642837\n",
      "      mean_processing_ms: 4.848507320441665\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.391433715820312\n",
      "        critic_loss: 0.00531306117773056\n",
      "        max_q: 17.94683265686035\n",
      "        mean_q: 17.247241973876953\n",
      "        min_q: 16.70708465576172\n",
      "        model: {}\n",
      "        td_error: 0.01062612235546112\n",
      "    max_exploration: 0.12780000000000002\n",
      "    min_exploration: 0.12780000000000002\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 1920000\n",
      "    num_target_updates: 9000\n",
      "    opt_peak_throughput: 54939.155\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.075\n",
      "    sample_time_ms: 86.644\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.694444444444443\n",
      "    ram_util_percent: 68.02777777777777\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.50378707866754\n",
      "    mean_inference_ms: 0.6237140840043138\n",
      "    mean_processing_ms: 5.6178555951763745\n",
      "  time_since_restore: 921.4372682571411\n",
      "  time_this_iter_s: 10.294180870056152\n",
      "  time_total_s: 921.4372682571411\n",
      "  timestamp: 1582121127\n",
      "  timesteps_since_restore: 9000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 90\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         921.437</td><td style=\"text-align: right;\">       9000</td><td style=\"text-align: right;\">   1.843</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-05-39\n",
      "  done: false\n",
      "  episode_len_mean: 19.52\n",
      "  episode_reward_max: 3.300000049173832\n",
      "  episode_reward_mean: 1.8520000275969506\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 491\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.55381740089554\n",
      "      mean_inference_ms: 0.7324905199379418\n",
      "      mean_processing_ms: 4.855546975809995\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.147\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.3927001953125\n",
      "        critic_loss: 0.00567401060834527\n",
      "        max_q: 17.7828369140625\n",
      "        mean_q: 17.239110946655273\n",
      "        min_q: 16.62677764892578\n",
      "        model: {}\n",
      "        td_error: 0.011348020285367966\n",
      "    max_exploration: 0.118\n",
      "    min_exploration: 0.118\n",
      "    num_steps_sampled: 9100\n",
      "    num_steps_trained: 1945600\n",
      "    num_target_updates: 9100\n",
      "    opt_peak_throughput: 61735.556\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.977\n",
      "    sample_time_ms: 89.51\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.505882352941175\n",
      "    ram_util_percent: 68.08235294117648\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.46684787814952\n",
      "    mean_inference_ms: 0.6230682016449272\n",
      "    mean_processing_ms: 5.615112755988331\n",
      "  time_since_restore: 931.6303291320801\n",
      "  time_this_iter_s: 10.193060874938965\n",
      "  time_total_s: 931.6303291320801\n",
      "  timestamp: 1582121139\n",
      "  timesteps_since_restore: 9100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9100\n",
      "  training_iteration: 91\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">          931.63</td><td style=\"text-align: right;\">       9100</td><td style=\"text-align: right;\">   1.852</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 19.54\n",
      "  episode_reward_max: 3.300000049173832\n",
      "  episode_reward_mean: 1.8540000276267528\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 496\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 38.0\n",
      "    episode_reward_max: 3.7000000551342964\n",
      "    episode_reward_mean: 3.7000000551342964\n",
      "    episode_reward_min: 3.7000000551342964\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.68911901869319\n",
      "      mean_inference_ms: 0.729981781675307\n",
      "      mean_processing_ms: 4.81492620613379\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.827\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.42816925048828\n",
      "        critic_loss: 0.005497996229678392\n",
      "        max_q: 17.860633850097656\n",
      "        mean_q: 17.28122329711914\n",
      "        min_q: 16.751216888427734\n",
      "        model: {}\n",
      "        td_error: 0.010995992459356785\n",
      "    max_exploration: 0.10819999999999996\n",
      "    min_exploration: 0.10819999999999996\n",
      "    num_steps_sampled: 9200\n",
      "    num_steps_trained: 1971200\n",
      "    num_target_updates: 9200\n",
      "    opt_peak_throughput: 66886.05\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.194\n",
      "    sample_time_ms: 90.691\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.77058823529412\n",
      "    ram_util_percent: 68.00588235294117\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.43064196210952\n",
      "    mean_inference_ms: 0.6224449260306715\n",
      "    mean_processing_ms: 5.612086795293919\n",
      "  time_since_restore: 941.823380947113\n",
      "  time_this_iter_s: 10.193051815032959\n",
      "  time_total_s: 941.823380947113\n",
      "  timestamp: 1582121151\n",
      "  timesteps_since_restore: 9200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9200\n",
      "  training_iteration: 92\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         941.823</td><td style=\"text-align: right;\">       9200</td><td style=\"text-align: right;\">   1.854</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 19.51\n",
      "  episode_reward_max: 3.300000049173832\n",
      "  episode_reward_mean: 1.8510000275820493\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 501\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.73452478380345\n",
      "      mean_inference_ms: 0.728239348871791\n",
      "      mean_processing_ms: 4.801596812347867\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.863\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.438674926757812\n",
      "        critic_loss: 0.005264148116111755\n",
      "        max_q: 17.80647850036621\n",
      "        mean_q: 17.301210403442383\n",
      "        min_q: 16.767728805541992\n",
      "        model: {}\n",
      "        td_error: 0.010528297163546085\n",
      "    max_exploration: 0.09839999999999993\n",
      "    min_exploration: 0.09839999999999993\n",
      "    num_steps_sampled: 9300\n",
      "    num_steps_trained: 1996800\n",
      "    num_target_updates: 9300\n",
      "    opt_peak_throughput: 66273.814\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.473\n",
      "    sample_time_ms: 90.19\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.27\n",
      "    ram_util_percent: 68.02499999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.39530130864115\n",
      "    mean_inference_ms: 0.6218163437363984\n",
      "    mean_processing_ms: 5.608849644491438\n",
      "  time_since_restore: 952.014487028122\n",
      "  time_this_iter_s: 10.191106081008911\n",
      "  time_total_s: 952.014487028122\n",
      "  timestamp: 1582121165\n",
      "  timesteps_since_restore: 9300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9300\n",
      "  training_iteration: 93\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         952.014</td><td style=\"text-align: right;\">       9300</td><td style=\"text-align: right;\">   1.851</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 19.74\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8740000279247762\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 505\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.79403980062428\n",
      "      mean_inference_ms: 0.7263972164077347\n",
      "      mean_processing_ms: 4.783521393106637\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.047\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.419326782226562\n",
      "        critic_loss: 0.0057388050481677055\n",
      "        max_q: 17.705467224121094\n",
      "        mean_q: 17.27869415283203\n",
      "        min_q: 16.63829231262207\n",
      "        model: {}\n",
      "        td_error: 0.011477609165012836\n",
      "    max_exploration: 0.08860000000000001\n",
      "    min_exploration: 0.08860000000000001\n",
      "    num_steps_sampled: 9400\n",
      "    num_steps_trained: 2022400\n",
      "    num_target_updates: 9400\n",
      "    opt_peak_throughput: 63264.367\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.535\n",
      "    sample_time_ms: 79.355\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.866666666666667\n",
      "    ram_util_percent: 68.09999999999998\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.36727912767647\n",
      "    mean_inference_ms: 0.6213233835167692\n",
      "    mean_processing_ms: 5.605776711622331\n",
      "  time_since_restore: 962.1046371459961\n",
      "  time_this_iter_s: 10.090150117874146\n",
      "  time_total_s: 962.1046371459961\n",
      "  timestamp: 1582121178\n",
      "  timesteps_since_restore: 9400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9400\n",
      "  training_iteration: 94\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         962.105</td><td style=\"text-align: right;\">       9400</td><td style=\"text-align: right;\">   1.874</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 19.72\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8720000278949738\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 511\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.77681633473833\n",
      "      mean_inference_ms: 0.7253423029062699\n",
      "      mean_processing_ms: 4.788878360225578\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.863\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.41387367248535\n",
      "        critic_loss: 0.004871970508247614\n",
      "        max_q: 17.885942459106445\n",
      "        mean_q: 17.286376953125\n",
      "        min_q: 16.714672088623047\n",
      "        model: {}\n",
      "        td_error: 0.009743941016495228\n",
      "    max_exploration: 0.07880000000000009\n",
      "    min_exploration: 0.07880000000000009\n",
      "    num_steps_sampled: 9500\n",
      "    num_steps_trained: 2048000\n",
      "    num_target_updates: 9500\n",
      "    opt_peak_throughput: 66261.544\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.385\n",
      "    sample_time_ms: 90.17\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.399999999999995\n",
      "    ram_util_percent: 68.73333333333333\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.32650095643464\n",
      "    mean_inference_ms: 0.6205988247301436\n",
      "    mean_processing_ms: 5.601084611807611\n",
      "  time_since_restore: 972.3959589004517\n",
      "  time_this_iter_s: 10.291321754455566\n",
      "  time_total_s: 972.3959589004517\n",
      "  timestamp: 1582121191\n",
      "  timesteps_since_restore: 9500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9500\n",
      "  training_iteration: 95\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         972.396</td><td style=\"text-align: right;\">       9500</td><td style=\"text-align: right;\">   1.872</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-06-42\n",
      "  done: false\n",
      "  episode_len_mean: 19.85\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.885000028088689\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 516\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.78076392870683\n",
      "      mean_inference_ms: 0.7255108310626104\n",
      "      mean_processing_ms: 4.79354617687372\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.171\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.43480682373047\n",
      "        critic_loss: 0.004742090590298176\n",
      "        max_q: 17.89259147644043\n",
      "        mean_q: 17.308788299560547\n",
      "        min_q: 16.683717727661133\n",
      "        model: {}\n",
      "        td_error: 0.009484181180596352\n",
      "    max_exploration: 0.06900000000000006\n",
      "    min_exploration: 0.06900000000000006\n",
      "    num_steps_sampled: 9600\n",
      "    num_steps_trained: 2073600\n",
      "    num_target_updates: 9600\n",
      "    opt_peak_throughput: 61382.281\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.416\n",
      "    sample_time_ms: 87.699\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.599999999999998\n",
      "    ram_util_percent: 69.69411764705882\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.29426272944218\n",
      "    mean_inference_ms: 0.6199805133900815\n",
      "    mean_processing_ms: 5.597262711896683\n",
      "  time_since_restore: 982.5877139568329\n",
      "  time_this_iter_s: 10.191755056381226\n",
      "  time_total_s: 982.5877139568329\n",
      "  timestamp: 1582121202\n",
      "  timesteps_since_restore: 9600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9600\n",
      "  training_iteration: 96\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         982.588</td><td style=\"text-align: right;\">       9600</td><td style=\"text-align: right;\">   1.885</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 20.11\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.911000028476119\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 520\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.78531681028124\n",
      "      mean_inference_ms: 0.7246799632325612\n",
      "      mean_processing_ms: 4.78674478467366\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.322\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.49913787841797\n",
      "        critic_loss: 0.00689539173617959\n",
      "        max_q: 17.917015075683594\n",
      "        mean_q: 17.378032684326172\n",
      "        min_q: 16.97744369506836\n",
      "        model: {}\n",
      "        td_error: 0.013790784403681755\n",
      "    max_exploration: 0.05920000000000003\n",
      "    min_exploration: 0.05920000000000003\n",
      "    num_steps_sampled: 9700\n",
      "    num_steps_trained: 2099200\n",
      "    num_target_updates: 9700\n",
      "    opt_peak_throughput: 59230.471\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.384\n",
      "    sample_time_ms: 79.593\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.917647058823523\n",
      "    ram_util_percent: 69.75882352941176\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.2705919796284\n",
      "    mean_inference_ms: 0.6194467047996945\n",
      "    mean_processing_ms: 5.593850610048483\n",
      "  time_since_restore: 992.6783249378204\n",
      "  time_this_iter_s: 10.090610980987549\n",
      "  time_total_s: 992.6783249378204\n",
      "  timestamp: 1582121215\n",
      "  timesteps_since_restore: 9700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9700\n",
      "  training_iteration: 97\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         992.678</td><td style=\"text-align: right;\">       9700</td><td style=\"text-align: right;\">   1.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-07-07\n",
      "  done: false\n",
      "  episode_len_mean: 20.01\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9010000283271073\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 525\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.76081529653297\n",
      "      mean_inference_ms: 0.7240127842381315\n",
      "      mean_processing_ms: 4.794407790561892\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.094\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.42361068725586\n",
      "        critic_loss: 0.004593910649418831\n",
      "        max_q: 17.793231964111328\n",
      "        mean_q: 17.308643341064453\n",
      "        min_q: 16.765026092529297\n",
      "        model: {}\n",
      "        td_error: 0.009187821298837662\n",
      "    max_exploration: 0.0494\n",
      "    min_exploration: 0.0494\n",
      "    num_steps_sampled: 9800\n",
      "    num_steps_trained: 2124800\n",
      "    num_target_updates: 9800\n",
      "    opt_peak_throughput: 62531.918\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.626\n",
      "    sample_time_ms: 77.756\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.67647058823529\n",
      "    ram_util_percent: 69.78235294117647\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.24292437049067\n",
      "    mean_inference_ms: 0.6187484528627724\n",
      "    mean_processing_ms: 5.589753687008314\n",
      "  time_since_restore: 1002.8724358081818\n",
      "  time_this_iter_s: 10.194110870361328\n",
      "  time_total_s: 1002.8724358081818\n",
      "  timestamp: 1582121227\n",
      "  timesteps_since_restore: 9800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9800\n",
      "  training_iteration: 98\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1002.87</td><td style=\"text-align: right;\">       9800</td><td style=\"text-align: right;\">   1.901</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-07-19\n",
      "  done: false\n",
      "  episode_len_mean: 20.05\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9050000283867121\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 531\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.76578745944381\n",
      "      mean_inference_ms: 0.7228381604818848\n",
      "      mean_processing_ms: 4.793572603932727\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.087\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.483722686767578\n",
      "        critic_loss: 0.006630136165767908\n",
      "        max_q: 17.86777114868164\n",
      "        mean_q: 17.36347198486328\n",
      "        min_q: 16.861709594726562\n",
      "        model: {}\n",
      "        td_error: 0.01326027326285839\n",
      "    max_exploration: 0.03960000000000008\n",
      "    min_exploration: 0.03960000000000008\n",
      "    num_steps_sampled: 9900\n",
      "    num_steps_trained: 2150400\n",
      "    num_target_updates: 9900\n",
      "    opt_peak_throughput: 62639.533\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.939\n",
      "    sample_time_ms: 90.545\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.24705882352941\n",
      "    ram_util_percent: 69.73529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.21218355503953\n",
      "    mean_inference_ms: 0.6178777724596864\n",
      "    mean_processing_ms: 5.585419647560803\n",
      "  time_since_restore: 1013.163125038147\n",
      "  time_this_iter_s: 10.29068922996521\n",
      "  time_total_s: 1013.163125038147\n",
      "  timestamp: 1582121239\n",
      "  timesteps_since_restore: 9900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 9900\n",
      "  training_iteration: 99\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1013.16</td><td style=\"text-align: right;\">       9900</td><td style=\"text-align: right;\">   1.905</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-07-31\n",
      "  done: false\n",
      "  episode_len_mean: 19.93\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.893000028207898\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 536\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.75611534612693\n",
      "      mean_inference_ms: 0.722083970421889\n",
      "      mean_processing_ms: 4.796967144700579\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.159\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.36437225341797\n",
      "        critic_loss: 0.004558056127279997\n",
      "        max_q: 17.634103775024414\n",
      "        mean_q: 17.2584171295166\n",
      "        min_q: 16.798261642456055\n",
      "        model: {}\n",
      "        td_error: 0.009116111323237419\n",
      "    max_exploration: 0.02980000000000005\n",
      "    min_exploration: 0.02980000000000005\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 2176000\n",
      "    num_target_updates: 10000\n",
      "    opt_peak_throughput: 61557.176\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.027\n",
      "    sample_time_ms: 80.053\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.666666666666668\n",
      "    ram_util_percent: 69.86666666666666\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.18794929031849\n",
      "    mean_inference_ms: 0.6171505756621665\n",
      "    mean_processing_ms: 5.581868778565241\n",
      "  time_since_restore: 1023.3718748092651\n",
      "  time_this_iter_s: 10.208749771118164\n",
      "  time_total_s: 1023.3718748092651\n",
      "  timestamp: 1582121251\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 100\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1023.37</td><td style=\"text-align: right;\">      10000</td><td style=\"text-align: right;\">   1.893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-07-43\n",
      "  done: false\n",
      "  episode_len_mean: 19.75\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8750000279396772\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 542\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.74630104610398\n",
      "      mean_inference_ms: 0.7215287027612506\n",
      "      mean_processing_ms: 4.799702755362478\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.989\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.349687576293945\n",
      "        critic_loss: 0.004061858169734478\n",
      "        max_q: 17.63067626953125\n",
      "        mean_q: 17.246374130249023\n",
      "        min_q: 16.663419723510742\n",
      "        model: {}\n",
      "        td_error: 0.008123716339468956\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10100\n",
      "    num_steps_trained: 2201600\n",
      "    num_target_updates: 10100\n",
      "    opt_peak_throughput: 64182.541\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.222\n",
      "    sample_time_ms: 90.328\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.788235294117644\n",
      "    ram_util_percent: 68.84705882352942\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.15888965693924\n",
      "    mean_inference_ms: 0.6163347057844067\n",
      "    mean_processing_ms: 5.578393454900274\n",
      "  time_since_restore: 1033.6506378650665\n",
      "  time_this_iter_s: 10.278763055801392\n",
      "  time_total_s: 1033.6506378650665\n",
      "  timestamp: 1582121263\n",
      "  timesteps_since_restore: 10100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10100\n",
      "  training_iteration: 101\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1033.65</td><td style=\"text-align: right;\">      10100</td><td style=\"text-align: right;\">   1.875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 19.94\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8940000282227993\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 546\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.73783503089787\n",
      "      mean_inference_ms: 0.7206576180176557\n",
      "      mean_processing_ms: 4.8019922829886985\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.799\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.36722183227539\n",
      "        critic_loss: 0.00452605914324522\n",
      "        max_q: 17.803462982177734\n",
      "        mean_q: 17.27171516418457\n",
      "        min_q: 16.80793571472168\n",
      "        model: {}\n",
      "        td_error: 0.00905211828649044\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10200\n",
      "    num_steps_trained: 2227200\n",
      "    num_target_updates: 10200\n",
      "    opt_peak_throughput: 53347.534\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.591\n",
      "    sample_time_ms: 86.876\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.58823529411765\n",
      "    ram_util_percent: 68.66470588235293\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.13881195470343\n",
      "    mean_inference_ms: 0.6158353357729257\n",
      "    mean_processing_ms: 5.57572473005732\n",
      "  time_since_restore: 1043.7450368404388\n",
      "  time_this_iter_s: 10.094398975372314\n",
      "  time_total_s: 1043.7450368404388\n",
      "  timestamp: 1582121275\n",
      "  timesteps_since_restore: 10200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10200\n",
      "  training_iteration: 102\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1043.75</td><td style=\"text-align: right;\">      10200</td><td style=\"text-align: right;\">   1.894</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-08-07\n",
      "  done: false\n",
      "  episode_len_mean: 20.17\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.917000028565526\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 550\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.71497811497869\n",
      "      mean_inference_ms: 0.7204437041067863\n",
      "      mean_processing_ms: 4.809163604770695\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.181\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.417984008789062\n",
      "        critic_loss: 0.005760528147220612\n",
      "        max_q: 17.72699546813965\n",
      "        mean_q: 17.306922912597656\n",
      "        min_q: 16.86806869506836\n",
      "        model: {}\n",
      "        td_error: 0.011521058157086372\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10300\n",
      "    num_steps_trained: 2252800\n",
      "    num_target_updates: 10300\n",
      "    opt_peak_throughput: 49413.56\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.983\n",
      "    sample_time_ms: 84.017\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.06875\n",
      "    ram_util_percent: 68.775\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.11765438135549\n",
      "    mean_inference_ms: 0.6153877761962923\n",
      "    mean_processing_ms: 5.572746342563062\n",
      "  time_since_restore: 1053.842082977295\n",
      "  time_this_iter_s: 10.097046136856079\n",
      "  time_total_s: 1053.842082977295\n",
      "  timestamp: 1582121287\n",
      "  timesteps_since_restore: 10300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10300\n",
      "  training_iteration: 103\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1053.84</td><td style=\"text-align: right;\">      10300</td><td style=\"text-align: right;\">   1.917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-08-18\n",
      "  done: false\n",
      "  episode_len_mean: 20.17\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.917000028565526\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 555\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 33.0\n",
      "    episode_reward_max: 3.200000047683716\n",
      "    episode_reward_mean: 3.200000047683716\n",
      "    episode_reward_min: 3.200000047683716\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.795788761144\n",
      "      mean_inference_ms: 0.7197261121185208\n",
      "      mean_processing_ms: 4.784547959864008\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.683\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.348114013671875\n",
      "        critic_loss: 0.003645814023911953\n",
      "        max_q: 17.612823486328125\n",
      "        mean_q: 17.24856948852539\n",
      "        min_q: 16.827880859375\n",
      "        model: {}\n",
      "        td_error: 0.007291628047823906\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10400\n",
      "    num_steps_trained: 2278400\n",
      "    num_target_updates: 10400\n",
      "    opt_peak_throughput: 54667.275\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.249\n",
      "    sample_time_ms: 74.795\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.970588235294116\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.08884346930428\n",
      "    mean_inference_ms: 0.6148906839329983\n",
      "    mean_processing_ms: 5.569104856324694\n",
      "  time_since_restore: 1064.0385999679565\n",
      "  time_this_iter_s: 10.196516990661621\n",
      "  time_total_s: 1064.0385999679565\n",
      "  timestamp: 1582121298\n",
      "  timesteps_since_restore: 10400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10400\n",
      "  training_iteration: 104\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1064.04</td><td style=\"text-align: right;\">      10400</td><td style=\"text-align: right;\">   1.917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-08-32\n",
      "  done: false\n",
      "  episode_len_mean: 20.15\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9150000285357236\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 560\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.79105665691088\n",
      "      mean_inference_ms: 0.7188378899201285\n",
      "      mean_processing_ms: 4.783437055897409\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.116\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.29339027404785\n",
      "        critic_loss: 0.005483489017933607\n",
      "        max_q: 17.649944305419922\n",
      "        mean_q: 17.201448440551758\n",
      "        min_q: 16.83124542236328\n",
      "        model: {}\n",
      "        td_error: 0.01096697710454464\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10500\n",
      "    num_steps_trained: 2304000\n",
      "    num_target_updates: 10500\n",
      "    opt_peak_throughput: 62195.065\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.019\n",
      "    sample_time_ms: 79.193\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.242105263157896\n",
      "    ram_util_percent: 69.07894736842104\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.060716021389\n",
      "    mean_inference_ms: 0.6143903133400141\n",
      "    mean_processing_ms: 5.565576087510257\n",
      "  time_since_restore: 1074.231612920761\n",
      "  time_this_iter_s: 10.193012952804565\n",
      "  time_total_s: 1074.231612920761\n",
      "  timestamp: 1582121312\n",
      "  timesteps_since_restore: 10500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10500\n",
      "  training_iteration: 105\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1074.23</td><td style=\"text-align: right;\">      10500</td><td style=\"text-align: right;\">   1.915</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-08-44\n",
      "  done: false\n",
      "  episode_len_mean: 19.8\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.880000028014183\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 567\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.80280751799873\n",
      "      mean_inference_ms: 0.7177517596852904\n",
      "      mean_processing_ms: 4.779664896921829\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.985\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.311527252197266\n",
      "        critic_loss: 0.004614084959030151\n",
      "        max_q: 17.66705894470215\n",
      "        mean_q: 17.225244522094727\n",
      "        min_q: 16.79557991027832\n",
      "        model: {}\n",
      "        td_error: 0.009228168986737728\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10600\n",
      "    num_steps_trained: 2329600\n",
      "    num_target_updates: 10600\n",
      "    opt_peak_throughput: 64242.446\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.651\n",
      "    sample_time_ms: 88.796\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.711764705882356\n",
      "    ram_util_percent: 69.05294117647058\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.02643380676498\n",
      "    mean_inference_ms: 0.6136339193239422\n",
      "    mean_processing_ms: 5.561469588488419\n",
      "  time_since_restore: 1084.6234591007233\n",
      "  time_this_iter_s: 10.391846179962158\n",
      "  time_total_s: 1084.6234591007233\n",
      "  timestamp: 1582121324\n",
      "  timesteps_since_restore: 10600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10600\n",
      "  training_iteration: 106\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1084.62</td><td style=\"text-align: right;\">      10600</td><td style=\"text-align: right;\">    1.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 19.87\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.887000028118491\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 571\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.80749525953924\n",
      "      mean_inference_ms: 0.7165477026938569\n",
      "      mean_processing_ms: 4.7786178210359695\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.01\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.279865264892578\n",
      "        critic_loss: 0.0038617567624896765\n",
      "        max_q: 17.55697250366211\n",
      "        mean_q: 17.183578491210938\n",
      "        min_q: 16.516908645629883\n",
      "        model: {}\n",
      "        td_error: 0.007723513059318066\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10700\n",
      "    num_steps_trained: 2355200\n",
      "    num_target_updates: 10700\n",
      "    opt_peak_throughput: 63843.28\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.32\n",
      "    sample_time_ms: 89.118\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.511111111111113\n",
      "    ram_util_percent: 68.96111111111112\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 76.00956087764997\n",
      "    mean_inference_ms: 0.6131714073043479\n",
      "    mean_processing_ms: 5.559183504660632\n",
      "  time_since_restore: 1094.7145080566406\n",
      "  time_this_iter_s: 10.091048955917358\n",
      "  time_total_s: 1094.7145080566406\n",
      "  timestamp: 1582121336\n",
      "  timesteps_since_restore: 10700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10700\n",
      "  training_iteration: 107\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1094.71</td><td style=\"text-align: right;\">      10700</td><td style=\"text-align: right;\">   1.887</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-09-08\n",
      "  done: false\n",
      "  episode_len_mean: 19.94\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8940000282227993\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 575\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.81704783042441\n",
      "      mean_inference_ms: 0.7167930839307183\n",
      "      mean_processing_ms: 4.7758347659170095\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.345\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.30164337158203\n",
      "        critic_loss: 0.004107836168259382\n",
      "        max_q: 17.7001953125\n",
      "        mean_q: 17.213720321655273\n",
      "        min_q: 16.74608612060547\n",
      "        model: {}\n",
      "        td_error: 0.008215673267841339\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10800\n",
      "    num_steps_trained: 2380800\n",
      "    num_target_updates: 10800\n",
      "    opt_peak_throughput: 58913.612\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.644\n",
      "    sample_time_ms: 77.348\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.758823529411764\n",
      "    ram_util_percent: 68.83529411764705\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.99336209146233\n",
      "    mean_inference_ms: 0.6126990731058704\n",
      "    mean_processing_ms: 5.556500329036903\n",
      "  time_since_restore: 1104.8077619075775\n",
      "  time_this_iter_s: 10.09325385093689\n",
      "  time_total_s: 1104.8077619075775\n",
      "  timestamp: 1582121348\n",
      "  timesteps_since_restore: 10800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10800\n",
      "  training_iteration: 108\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1104.81</td><td style=\"text-align: right;\">      10800</td><td style=\"text-align: right;\">   1.894</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-09-21\n",
      "  done: false\n",
      "  episode_len_mean: 19.9\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8900000281631946\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 581\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.81365296398553\n",
      "      mean_inference_ms: 0.717234147190543\n",
      "      mean_processing_ms: 4.776834936892956\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.501\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.26487159729004\n",
      "        critic_loss: 0.00521426647901535\n",
      "        max_q: 17.649877548217773\n",
      "        mean_q: 17.179283142089844\n",
      "        min_q: 16.789583206176758\n",
      "        model: {}\n",
      "        td_error: 0.0104285329580307\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 10900\n",
      "    num_steps_trained: 2406400\n",
      "    num_target_updates: 10900\n",
      "    opt_peak_throughput: 56877.643\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.33\n",
      "    sample_time_ms: 85.553\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.28235294117647\n",
      "    ram_util_percent: 69.18823529411765\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.97082449858578\n",
      "    mean_inference_ms: 0.6119896799937292\n",
      "    mean_processing_ms: 5.5525088878111095\n",
      "  time_since_restore: 1115.1035737991333\n",
      "  time_this_iter_s: 10.295811891555786\n",
      "  time_total_s: 1115.1035737991333\n",
      "  timestamp: 1582121361\n",
      "  timesteps_since_restore: 10900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 10900\n",
      "  training_iteration: 109\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">          1115.1</td><td style=\"text-align: right;\">      10900</td><td style=\"text-align: right;\">    1.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-09-33\n",
      "  done: false\n",
      "  episode_len_mean: 19.92\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.892000028192997\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 586\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.79231521061489\n",
      "      mean_inference_ms: 0.7174587049404112\n",
      "      mean_processing_ms: 4.781799957531841\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.381\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.26567268371582\n",
      "        critic_loss: 0.003564633196219802\n",
      "        max_q: 17.631261825561523\n",
      "        mean_q: 17.172401428222656\n",
      "        min_q: 16.70184326171875\n",
      "        model: {}\n",
      "        td_error: 0.0071292659267783165\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 2432000\n",
      "    num_target_updates: 11000\n",
      "    opt_peak_throughput: 58436.519\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.279\n",
      "    sample_time_ms: 86.623\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.41666666666667\n",
      "    ram_util_percent: 69.39444444444445\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.95260719678514\n",
      "    mean_inference_ms: 0.6113949656867784\n",
      "    mean_processing_ms: 5.548802526107308\n",
      "  time_since_restore: 1125.299263715744\n",
      "  time_this_iter_s: 10.195689916610718\n",
      "  time_total_s: 1125.299263715744\n",
      "  timestamp: 1582121373\n",
      "  timesteps_since_restore: 11000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 110\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">          1125.3</td><td style=\"text-align: right;\">      11000</td><td style=\"text-align: right;\">   1.892</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 19.98\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8980000282824039\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 591\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.79629380895534\n",
      "      mean_inference_ms: 0.7174523247965766\n",
      "      mean_processing_ms: 4.78099635201231\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.699\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.194900512695312\n",
      "        critic_loss: 0.004962636157870293\n",
      "        max_q: 17.581607818603516\n",
      "        mean_q: 17.118793487548828\n",
      "        min_q: 16.637733459472656\n",
      "        model: {}\n",
      "        td_error: 0.009925272315740585\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11100\n",
      "    num_steps_trained: 2457600\n",
      "    num_target_updates: 11100\n",
      "    opt_peak_throughput: 54480.601\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.916\n",
      "    sample_time_ms: 87.288\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.5125\n",
      "    ram_util_percent: 69.48124999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.93444790882494\n",
      "    mean_inference_ms: 0.6108229629367512\n",
      "    mean_processing_ms: 5.5451629661901904\n",
      "  time_since_restore: 1135.5084636211395\n",
      "  time_this_iter_s: 10.209199905395508\n",
      "  time_total_s: 1135.5084636211395\n",
      "  timestamp: 1582121385\n",
      "  timesteps_since_restore: 11100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11100\n",
      "  training_iteration: 111\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1135.51</td><td style=\"text-align: right;\">      11100</td><td style=\"text-align: right;\">   1.898</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 19.81\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8810000280290842\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 596\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.78139333949508\n",
      "      mean_inference_ms: 0.7162571741204181\n",
      "      mean_processing_ms: 4.785516842097992\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.959\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.207622528076172\n",
      "        critic_loss: 0.003976435400545597\n",
      "        max_q: 17.527782440185547\n",
      "        mean_q: 17.12735366821289\n",
      "        min_q: 16.705209732055664\n",
      "        model: {}\n",
      "        td_error: 0.00795286986976862\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11200\n",
      "    num_steps_trained: 2483200\n",
      "    num_target_updates: 11200\n",
      "    opt_peak_throughput: 64655.978\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.924\n",
      "    sample_time_ms: 79.255\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.783333333333335\n",
      "    ram_util_percent: 69.60555555555555\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.91583760659141\n",
      "    mean_inference_ms: 0.6102814878560743\n",
      "    mean_processing_ms: 5.54156859674265\n",
      "  time_since_restore: 1145.6952066421509\n",
      "  time_this_iter_s: 10.186743021011353\n",
      "  time_total_s: 1145.6952066421509\n",
      "  timestamp: 1582121397\n",
      "  timesteps_since_restore: 11200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11200\n",
      "  training_iteration: 112\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">          1145.7</td><td style=\"text-align: right;\">      11200</td><td style=\"text-align: right;\">   1.881</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 19.81\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8810000280290842\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 601\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.81703515457951\n",
      "      mean_inference_ms: 0.7155866794336855\n",
      "      mean_processing_ms: 4.7749112439311405\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.25\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.188335418701172\n",
      "        critic_loss: 0.004100531339645386\n",
      "        max_q: 17.46463394165039\n",
      "        mean_q: 17.11170768737793\n",
      "        min_q: 16.729732513427734\n",
      "        model: {}\n",
      "        td_error: 0.008201062679290771\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11300\n",
      "    num_steps_trained: 2508800\n",
      "    num_target_updates: 11300\n",
      "    opt_peak_throughput: 60236.957\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.263\n",
      "    sample_time_ms: 79.147\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.76470588235294\n",
      "    ram_util_percent: 69.83529411764708\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.8964971806669\n",
      "    mean_inference_ms: 0.6097779441553807\n",
      "    mean_processing_ms: 5.538039304654614\n",
      "  time_since_restore: 1155.8884489536285\n",
      "  time_this_iter_s: 10.193242311477661\n",
      "  time_total_s: 1155.8884489536285\n",
      "  timestamp: 1582121409\n",
      "  timesteps_since_restore: 11300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11300\n",
      "  training_iteration: 113\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1155.89</td><td style=\"text-align: right;\">      11300</td><td style=\"text-align: right;\">   1.881</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-10-21\n",
      "  done: false\n",
      "  episode_len_mean: 19.72\n",
      "  episode_reward_max: 3.300000049173832\n",
      "  episode_reward_mean: 1.8720000278949738\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 606\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.84620073637657\n",
      "      mean_inference_ms: 0.7147532658125956\n",
      "      mean_processing_ms: 4.766419806977675\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.551\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.13202476501465\n",
      "        critic_loss: 0.003944375552237034\n",
      "        max_q: 17.453712463378906\n",
      "        mean_q: 17.064453125\n",
      "        min_q: 16.648767471313477\n",
      "        model: {}\n",
      "        td_error: 0.007888751104474068\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11400\n",
      "    num_steps_trained: 2534400\n",
      "    num_target_updates: 11400\n",
      "    opt_peak_throughput: 56248.655\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.249\n",
      "    sample_time_ms: 78.268\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.972222222222225\n",
      "    ram_util_percent: 69.97777777777777\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.87722610945221\n",
      "    mean_inference_ms: 0.6093025334675707\n",
      "    mean_processing_ms: 5.535075131726953\n",
      "  time_since_restore: 1166.0881848335266\n",
      "  time_this_iter_s: 10.199735879898071\n",
      "  time_total_s: 1166.0881848335266\n",
      "  timestamp: 1582121421\n",
      "  timesteps_since_restore: 11400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11400\n",
      "  training_iteration: 114\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1166.09</td><td style=\"text-align: right;\">      11400</td><td style=\"text-align: right;\">   1.872</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9000000283122063\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 611\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 33.0\n",
      "    episode_reward_max: 3.200000047683716\n",
      "    episode_reward_mean: 3.200000047683716\n",
      "    episode_reward_min: 3.200000047683716\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.91616018254965\n",
      "      mean_inference_ms: 0.71480029790488\n",
      "      mean_processing_ms: 4.744928197932994\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.48\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.157596588134766\n",
      "        critic_loss: 0.003751730313524604\n",
      "        max_q: 17.48596954345703\n",
      "        mean_q: 17.077762603759766\n",
      "        min_q: 16.67793846130371\n",
      "        model: {}\n",
      "        td_error: 0.00750346016138792\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11500\n",
      "    num_steps_trained: 2560000\n",
      "    num_target_updates: 11500\n",
      "    opt_peak_throughput: 57144.931\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.242\n",
      "    sample_time_ms: 87.649\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.883333333333333\n",
      "    ram_util_percent: 69.28333333333335\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.8577297435703\n",
      "    mean_inference_ms: 0.6088391994663528\n",
      "    mean_processing_ms: 5.5314490833599645\n",
      "  time_since_restore: 1176.2788116931915\n",
      "  time_this_iter_s: 10.190626859664917\n",
      "  time_total_s: 1176.2788116931915\n",
      "  timestamp: 1582121434\n",
      "  timesteps_since_restore: 11500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11500\n",
      "  training_iteration: 115\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1176.28</td><td style=\"text-align: right;\">      11500</td><td style=\"text-align: right;\">     1.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-10-47\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.9000000283122063\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 616\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.92578631992868\n",
      "      mean_inference_ms: 0.7137962951961714\n",
      "      mean_processing_ms: 4.742032737128819\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.186\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.176223754882812\n",
      "        critic_loss: 0.003574816510081291\n",
      "        max_q: 17.58938217163086\n",
      "        mean_q: 17.10189437866211\n",
      "        min_q: 16.58235740661621\n",
      "        model: {}\n",
      "        td_error: 0.007149633020162582\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11600\n",
      "    num_steps_trained: 2585600\n",
      "    num_target_updates: 11600\n",
      "    opt_peak_throughput: 61162.006\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.088\n",
      "    sample_time_ms: 88.059\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.772222222222222\n",
      "    ram_util_percent: 69.17777777777776\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.83796799999973\n",
      "    mean_inference_ms: 0.6084183492371273\n",
      "    mean_processing_ms: 5.5279517093325445\n",
      "  time_since_restore: 1186.471913576126\n",
      "  time_this_iter_s: 10.19310188293457\n",
      "  time_total_s: 1186.471913576126\n",
      "  timestamp: 1582121447\n",
      "  timesteps_since_restore: 11600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11600\n",
      "  training_iteration: 116\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1186.47</td><td style=\"text-align: right;\">      11600</td><td style=\"text-align: right;\">     1.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 19.71\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8710000278800727\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 622\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.92927059932936\n",
      "      mean_inference_ms: 0.7137868284805441\n",
      "      mean_processing_ms: 4.740119167256131\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.324\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.114654541015625\n",
      "        critic_loss: 0.0041945879347622395\n",
      "        max_q: 17.380062103271484\n",
      "        mean_q: 17.043704986572266\n",
      "        min_q: 16.707454681396484\n",
      "        model: {}\n",
      "        td_error: 0.008389174938201904\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11700\n",
      "    num_steps_trained: 2611200\n",
      "    num_target_updates: 11700\n",
      "    opt_peak_throughput: 59204.67\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.298\n",
      "    sample_time_ms: 87.814\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.961111111111112\n",
      "    ram_util_percent: 69.32222222222224\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.81437441322417\n",
      "    mean_inference_ms: 0.6079564956828293\n",
      "    mean_processing_ms: 5.524944664318528\n",
      "  time_since_restore: 1196.7648756504059\n",
      "  time_this_iter_s: 10.292962074279785\n",
      "  time_total_s: 1196.7648756504059\n",
      "  timestamp: 1582121460\n",
      "  timesteps_since_restore: 11700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11700\n",
      "  training_iteration: 117\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1196.76</td><td style=\"text-align: right;\">      11700</td><td style=\"text-align: right;\">   1.871</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-11-12\n",
      "  done: false\n",
      "  episode_len_mean: 19.72\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8720000278949738\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 627\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.93803152065833\n",
      "      mean_inference_ms: 0.7134814401274746\n",
      "      mean_processing_ms: 4.737470034256722\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.397\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.040592193603516\n",
      "        critic_loss: 0.004235515836626291\n",
      "        max_q: 17.347169876098633\n",
      "        mean_q: 16.968042373657227\n",
      "        min_q: 16.534696578979492\n",
      "        model: {}\n",
      "        td_error: 0.008471031673252583\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11800\n",
      "    num_steps_trained: 2636800\n",
      "    num_target_updates: 11800\n",
      "    opt_peak_throughput: 58216.321\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.026\n",
      "    sample_time_ms: 87.03\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.823529411764707\n",
      "    ram_util_percent: 68.99411764705881\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.79429225353772\n",
      "    mean_inference_ms: 0.607608159482631\n",
      "    mean_processing_ms: 5.522259545130996\n",
      "  time_since_restore: 1206.958206653595\n",
      "  time_this_iter_s: 10.193331003189087\n",
      "  time_total_s: 1206.958206653595\n",
      "  timestamp: 1582121472\n",
      "  timesteps_since_restore: 11800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11800\n",
      "  training_iteration: 118\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1206.96</td><td style=\"text-align: right;\">      11800</td><td style=\"text-align: right;\">   1.872</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 19.9\n",
      "  episode_reward_max: 3.7000000551342964\n",
      "  episode_reward_mean: 1.8900000281631946\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 630\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.94508594912323\n",
      "      mean_inference_ms: 0.713875148735017\n",
      "      mean_processing_ms: 4.735188451154678\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.739\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.01980209350586\n",
      "        critic_loss: 0.00379126681946218\n",
      "        max_q: 17.331090927124023\n",
      "        mean_q: 16.95931625366211\n",
      "        min_q: 16.542903900146484\n",
      "        model: {}\n",
      "        td_error: 0.007582534104585648\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11900\n",
      "    num_steps_trained: 2662400\n",
      "    num_target_updates: 11900\n",
      "    opt_peak_throughput: 54024.474\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.253\n",
      "    sample_time_ms: 76.446\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.72941176470588\n",
      "    ram_util_percent: 69.51764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.78136401174052\n",
      "    mean_inference_ms: 0.6074120359262616\n",
      "    mean_processing_ms: 5.519983321203448\n",
      "  time_since_restore: 1216.9544777870178\n",
      "  time_this_iter_s: 9.996271133422852\n",
      "  time_total_s: 1216.9544777870178\n",
      "  timestamp: 1582121484\n",
      "  timesteps_since_restore: 11900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 11900\n",
      "  training_iteration: 119\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1216.95</td><td style=\"text-align: right;\">      11900</td><td style=\"text-align: right;\">    1.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-11-36\n",
      "  done: false\n",
      "  episode_len_mean: 20.06\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9060000284016132\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 635\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 83.9904028969116\n",
      "      mean_inference_ms: 0.7138264964277589\n",
      "      mean_processing_ms: 4.719539155150531\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.238\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.011281967163086\n",
      "        critic_loss: 0.004082844592630863\n",
      "        max_q: 17.2871036529541\n",
      "        mean_q: 16.950225830078125\n",
      "        min_q: 16.5878963470459\n",
      "        model: {}\n",
      "        td_error: 0.008165688253939152\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 2688000\n",
      "    num_target_updates: 12000\n",
      "    opt_peak_throughput: 60406.057\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.905\n",
      "    sample_time_ms: 77.309\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.33888888888889\n",
      "    ram_util_percent: 69.59999999999998\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.75905799571123\n",
      "    mean_inference_ms: 0.6071057409635753\n",
      "    mean_processing_ms: 5.516230345723195\n",
      "  time_since_restore: 1227.1614005565643\n",
      "  time_this_iter_s: 10.206922769546509\n",
      "  time_total_s: 1227.1614005565643\n",
      "  timestamp: 1582121496\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 120\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1227.16</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\">   1.906</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-11-49\n",
      "  done: false\n",
      "  episode_len_mean: 20.25\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9250000286847353\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 640\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.00435449491577\n",
      "      mean_inference_ms: 0.7131636614891234\n",
      "      mean_processing_ms: 4.71536962997684\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.154\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -17.036203384399414\n",
      "        critic_loss: 0.0038839681074023247\n",
      "        max_q: 17.28809928894043\n",
      "        mean_q: 16.965824127197266\n",
      "        min_q: 16.649553298950195\n",
      "        model: {}\n",
      "        td_error: 0.007767935749143362\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12100\n",
      "    num_steps_trained: 2713600\n",
      "    num_target_updates: 12100\n",
      "    opt_peak_throughput: 61622.533\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.094\n",
      "    sample_time_ms: 79.217\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.81111111111111\n",
      "    ram_util_percent: 70.02777777777777\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.73760708229504\n",
      "    mean_inference_ms: 0.606786253215266\n",
      "    mean_processing_ms: 5.512107681075021\n",
      "  time_since_restore: 1237.3432354927063\n",
      "  time_this_iter_s: 10.181834936141968\n",
      "  time_total_s: 1237.3432354927063\n",
      "  timestamp: 1582121509\n",
      "  timesteps_since_restore: 12100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12100\n",
      "  training_iteration: 121\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1237.34</td><td style=\"text-align: right;\">      12100</td><td style=\"text-align: right;\">   1.925</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 20.31\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9310000287741422\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 644\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.01203592410262\n",
      "      mean_inference_ms: 0.7127466358706945\n",
      "      mean_processing_ms: 4.7133483366014\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.591\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.940017700195312\n",
      "        critic_loss: 0.0036904383450746536\n",
      "        max_q: 17.2457218170166\n",
      "        mean_q: 16.88481903076172\n",
      "        min_q: 16.481464385986328\n",
      "        model: {}\n",
      "        td_error: 0.007380875758826733\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12200\n",
      "    num_steps_trained: 2739200\n",
      "    num_target_updates: 12200\n",
      "    opt_peak_throughput: 55761.995\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.234\n",
      "    sample_time_ms: 76.577\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.8\n",
      "    ram_util_percent: 69.92777777777778\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.7204977507457\n",
      "    mean_inference_ms: 0.6065262581360887\n",
      "    mean_processing_ms: 5.50829674645152\n",
      "  time_since_restore: 1247.4395327568054\n",
      "  time_this_iter_s: 10.096297264099121\n",
      "  time_total_s: 1247.4395327568054\n",
      "  timestamp: 1582121521\n",
      "  timesteps_since_restore: 12200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12200\n",
      "  training_iteration: 122\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1247.44</td><td style=\"text-align: right;\">      12200</td><td style=\"text-align: right;\">   1.931</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-12-14\n",
      "  done: false\n",
      "  episode_len_mean: 20.12\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9120000284910201\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 649\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 32.0\n",
      "    episode_reward_max: 3.1000000461935997\n",
      "    episode_reward_mean: 3.1000000461935997\n",
      "    episode_reward_min: 3.1000000461935997\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.07017132659709\n",
      "      mean_inference_ms: 0.7123326867576537\n",
      "      mean_processing_ms: 4.695673122814824\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.492\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.938919067382812\n",
      "        critic_loss: 0.0035846661776304245\n",
      "        max_q: 17.222740173339844\n",
      "        mean_q: 16.884185791015625\n",
      "        min_q: 16.518463134765625\n",
      "        model: {}\n",
      "        td_error: 0.007169331423938274\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12300\n",
      "    num_steps_trained: 2764800\n",
      "    num_target_updates: 12300\n",
      "    opt_peak_throughput: 56985.407\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.954\n",
      "    sample_time_ms: 76.972\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.517647058823535\n",
      "    ram_util_percent: 69.34117647058824\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.69997224189574\n",
      "    mean_inference_ms: 0.6061821546684877\n",
      "    mean_processing_ms: 5.504141419109133\n",
      "  time_since_restore: 1257.6348378658295\n",
      "  time_this_iter_s: 10.195305109024048\n",
      "  time_total_s: 1257.6348378658295\n",
      "  timestamp: 1582121534\n",
      "  timesteps_since_restore: 12300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12300\n",
      "  training_iteration: 123\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1257.63</td><td style=\"text-align: right;\">      12300</td><td style=\"text-align: right;\">   1.912</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-12-27\n",
      "  done: false\n",
      "  episode_len_mean: 20.11\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.911000028476119\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 655\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.0833992311806\n",
      "      mean_inference_ms: 0.7117108984307928\n",
      "      mean_processing_ms: 4.691299326690562\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.355\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.92625617980957\n",
      "        critic_loss: 0.003658684901893139\n",
      "        max_q: 17.155736923217773\n",
      "        mean_q: 16.864107131958008\n",
      "        min_q: 16.43561363220215\n",
      "        model: {}\n",
      "        td_error: 0.007317370269447565\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12400\n",
      "    num_steps_trained: 2790400\n",
      "    num_target_updates: 12400\n",
      "    opt_peak_throughput: 58784.276\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.594\n",
      "    sample_time_ms: 87.393\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.373684210526314\n",
      "    ram_util_percent: 69.34736842105264\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.67853396895562\n",
      "    mean_inference_ms: 0.6057385756212266\n",
      "    mean_processing_ms: 5.499459168191119\n",
      "  time_since_restore: 1267.9281158447266\n",
      "  time_this_iter_s: 10.293277978897095\n",
      "  time_total_s: 1267.9281158447266\n",
      "  timestamp: 1582121547\n",
      "  timesteps_since_restore: 12400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12400\n",
      "  training_iteration: 124\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1267.93</td><td style=\"text-align: right;\">      12400</td><td style=\"text-align: right;\">   1.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 20.14\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9140000285208225\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 660\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.06248649655471\n",
      "      mean_inference_ms: 0.7117237066077353\n",
      "      mean_processing_ms: 4.697736173738038\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.714\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.923105239868164\n",
      "        critic_loss: 0.0032022541854530573\n",
      "        max_q: 17.123292922973633\n",
      "        mean_q: 16.86219596862793\n",
      "        min_q: 16.544919967651367\n",
      "        model: {}\n",
      "        td_error: 0.006404508836567402\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12500\n",
      "    num_steps_trained: 2816000\n",
      "    num_target_updates: 12500\n",
      "    opt_peak_throughput: 54310.027\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.475\n",
      "    sample_time_ms: 76.042\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.32777777777778\n",
      "    ram_util_percent: 69.58888888888887\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.66124264094398\n",
      "    mean_inference_ms: 0.6053863401079612\n",
      "    mean_processing_ms: 5.495617094875277\n",
      "  time_since_restore: 1278.1237108707428\n",
      "  time_this_iter_s: 10.195595026016235\n",
      "  time_total_s: 1278.1237108707428\n",
      "  timestamp: 1582121559\n",
      "  timesteps_since_restore: 12500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12500\n",
      "  training_iteration: 125\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1278.12</td><td style=\"text-align: right;\">      12500</td><td style=\"text-align: right;\">   1.914</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 20.22\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9220000286400318\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 665\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 43.0\n",
      "    episode_reward_max: 4.200000062584877\n",
      "    episode_reward_mean: 4.200000062584877\n",
      "    episode_reward_min: 4.200000062584877\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.17755068523061\n",
      "      mean_inference_ms: 0.7111362469576457\n",
      "      mean_processing_ms: 4.663449211899673\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.664\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.907215118408203\n",
      "        critic_loss: 0.002853677375242114\n",
      "        max_q: 17.16567039489746\n",
      "        mean_q: 16.851655960083008\n",
      "        min_q: 16.546743392944336\n",
      "        model: {}\n",
      "        td_error: 0.005707354750484228\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12600\n",
      "    num_steps_trained: 2841600\n",
      "    num_target_updates: 12600\n",
      "    opt_peak_throughput: 54885.797\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.522\n",
      "    sample_time_ms: 75.177\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.65625\n",
      "    ram_util_percent: 69.75625\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.6430844699085\n",
      "    mean_inference_ms: 0.6050569940404072\n",
      "    mean_processing_ms: 5.491406212885368\n",
      "  time_since_restore: 1288.3165910243988\n",
      "  time_this_iter_s: 10.192880153656006\n",
      "  time_total_s: 1288.3165910243988\n",
      "  timestamp: 1582121571\n",
      "  timesteps_since_restore: 12600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12600\n",
      "  training_iteration: 126\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         1288.32</td><td style=\"text-align: right;\">      12600</td><td style=\"text-align: right;\">   1.922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 20.19\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9190000285953284\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 670\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.16710558076721\n",
      "      mean_inference_ms: 0.7107446983334689\n",
      "      mean_processing_ms: 4.666672190581722\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.199\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.868227005004883\n",
      "        critic_loss: 0.0026100543327629566\n",
      "        max_q: 17.158180236816406\n",
      "        mean_q: 16.819358825683594\n",
      "        min_q: 16.508407592773438\n",
      "        model: {}\n",
      "        td_error: 0.005220108665525913\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12700\n",
      "    num_steps_trained: 2867200\n",
      "    num_target_updates: 12700\n",
      "    opt_peak_throughput: 60974.107\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.258\n",
      "    sample_time_ms: 77.421\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.6\n",
      "    ram_util_percent: 70.30952380952382\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.62430162666759\n",
      "    mean_inference_ms: 0.6047370103104477\n",
      "    mean_processing_ms: 5.487508479021931\n",
      "  time_since_restore: 1298.5097870826721\n",
      "  time_this_iter_s: 10.193196058273315\n",
      "  time_total_s: 1298.5097870826721\n",
      "  timestamp: 1582121585\n",
      "  timesteps_since_restore: 12700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12700\n",
      "  training_iteration: 127\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         1298.51</td><td style=\"text-align: right;\">      12700</td><td style=\"text-align: right;\">   1.919</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-13-17\n",
      "  done: false\n",
      "  episode_len_mean: 20.49\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9490000290423632\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 673\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.16256098087224\n",
      "      mean_inference_ms: 0.7099675930152505\n",
      "      mean_processing_ms: 4.668160094379705\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.81827163696289\n",
      "        critic_loss: 0.0025657147634774446\n",
      "        max_q: 17.096311569213867\n",
      "        mean_q: 16.765548706054688\n",
      "        min_q: 16.454397201538086\n",
      "        model: {}\n",
      "        td_error: 0.005131429992616177\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12800\n",
      "    num_steps_trained: 2892800\n",
      "    num_target_updates: 12800\n",
      "    opt_peak_throughput: 62229.669\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.597\n",
      "    sample_time_ms: 88.533\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.987499999999997\n",
      "    ram_util_percent: 70.48750000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.61331649036549\n",
      "    mean_inference_ms: 0.6045467311363986\n",
      "    mean_processing_ms: 5.485044401862537\n",
      "  time_since_restore: 1308.5018961429596\n",
      "  time_this_iter_s: 9.992109060287476\n",
      "  time_total_s: 1308.5018961429596\n",
      "  timestamp: 1582121597\n",
      "  timesteps_since_restore: 12800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12800\n",
      "  training_iteration: 128\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">          1308.5</td><td style=\"text-align: right;\">      12800</td><td style=\"text-align: right;\">   1.949</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 20.63\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9630000292509795\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 677\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.12647305935108\n",
      "      mean_inference_ms: 0.7094943318234689\n",
      "      mean_processing_ms: 4.6794625554622415\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.142\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.763137817382812\n",
      "        critic_loss: 0.0029537260998040438\n",
      "        max_q: 16.988624572753906\n",
      "        mean_q: 16.718917846679688\n",
      "        min_q: 16.407499313354492\n",
      "        model: {}\n",
      "        td_error: 0.0059074517339468\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12900\n",
      "    num_steps_trained: 2918400\n",
      "    num_target_updates: 12900\n",
      "    opt_peak_throughput: 61799.158\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.677\n",
      "    sample_time_ms: 78.762\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.794117647058822\n",
      "    ram_util_percent: 70.4764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.59919437126061\n",
      "    mean_inference_ms: 0.604278891542815\n",
      "    mean_processing_ms: 5.481594273459893\n",
      "  time_since_restore: 1318.5955860614777\n",
      "  time_this_iter_s: 10.093689918518066\n",
      "  time_total_s: 1318.5955860614777\n",
      "  timestamp: 1582121609\n",
      "  timesteps_since_restore: 12900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12900\n",
      "  training_iteration: 129\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">          1318.6</td><td style=\"text-align: right;\">      12900</td><td style=\"text-align: right;\">   1.963</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 20.62\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.9620000292360782\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 683\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.12170775930917\n",
      "      mean_inference_ms: 0.7092099808250012\n",
      "      mean_processing_ms: 4.683537047610955\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.266\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.774812698364258\n",
      "        critic_loss: 0.002985741011798382\n",
      "        max_q: 16.9827938079834\n",
      "        mean_q: 16.715316772460938\n",
      "        min_q: 16.418907165527344\n",
      "        model: {}\n",
      "        td_error: 0.005971482023596764\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 2944000\n",
      "    num_target_updates: 13000\n",
      "    opt_peak_throughput: 60010.721\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.586\n",
      "    sample_time_ms: 88.571\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.517647058823528\n",
      "    ram_util_percent: 70.5764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.57924994740436\n",
      "    mean_inference_ms: 0.6038589803620449\n",
      "    mean_processing_ms: 5.4764357549247755\n",
      "  time_since_restore: 1328.8881719112396\n",
      "  time_this_iter_s: 10.292585849761963\n",
      "  time_total_s: 1328.8881719112396\n",
      "  timestamp: 1582121621\n",
      "  timesteps_since_restore: 13000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 130\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1328.89</td><td style=\"text-align: right;\">      13000</td><td style=\"text-align: right;\">   1.962</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 20.67\n",
      "  episode_reward_max: 3.9000000581145287\n",
      "  episode_reward_mean: 1.967000029310584\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 687\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.12073640262379\n",
      "      mean_inference_ms: 0.7082310514879062\n",
      "      mean_processing_ms: 4.683263491594255\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.196\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.748138427734375\n",
      "        critic_loss: 0.003056928515434265\n",
      "        max_q: 17.009065628051758\n",
      "        mean_q: 16.69613265991211\n",
      "        min_q: 16.42470932006836\n",
      "        model: {}\n",
      "        td_error: 0.00611385703086853\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13100\n",
      "    num_steps_trained: 2969600\n",
      "    num_target_updates: 13100\n",
      "    opt_peak_throughput: 61015.685\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.976\n",
      "    sample_time_ms: 76.174\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.688235294117646\n",
      "    ram_util_percent: 70.57058823529411\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.56667624554254\n",
      "    mean_inference_ms: 0.6035685994907309\n",
      "    mean_processing_ms: 5.472687481104489\n",
      "  time_since_restore: 1338.9809770584106\n",
      "  time_this_iter_s: 10.09280514717102\n",
      "  time_total_s: 1338.9809770584106\n",
      "  timestamp: 1582121633\n",
      "  timesteps_since_restore: 13100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13100\n",
      "  training_iteration: 131\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1338.98</td><td style=\"text-align: right;\">      13100</td><td style=\"text-align: right;\">   1.967</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-14-05\n",
      "  done: false\n",
      "  episode_len_mean: 21.0\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0000000298023224\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 691\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.11679682310648\n",
      "      mean_inference_ms: 0.7076884249730112\n",
      "      mean_processing_ms: 4.684424326617216\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.965\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.698810577392578\n",
      "        critic_loss: 0.0035288124345242977\n",
      "        max_q: 16.966161727905273\n",
      "        mean_q: 16.65030288696289\n",
      "        min_q: 16.361600875854492\n",
      "        model: {}\n",
      "        td_error: 0.007057624403387308\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13200\n",
      "    num_steps_trained: 2995200\n",
      "    num_target_updates: 13200\n",
      "    opt_peak_throughput: 64570.05\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.788\n",
      "    sample_time_ms: 89.636\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.958823529411767\n",
      "    ram_util_percent: 70.74117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.55428400656973\n",
      "    mean_inference_ms: 0.6032788806590395\n",
      "    mean_processing_ms: 5.46867820789304\n",
      "  time_since_restore: 1349.0718502998352\n",
      "  time_this_iter_s: 10.09087324142456\n",
      "  time_total_s: 1349.0718502998352\n",
      "  timestamp: 1582121645\n",
      "  timesteps_since_restore: 13200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13200\n",
      "  training_iteration: 132\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1349.07</td><td style=\"text-align: right;\">      13200</td><td style=\"text-align: right;\">       2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 21.31\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0310000302642583\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 695\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 35.0\n",
      "    episode_reward_max: 3.400000050663948\n",
      "    episode_reward_mean: 3.400000050663948\n",
      "    episode_reward_min: 3.400000050663948\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.18507197238277\n",
      "      mean_inference_ms: 0.7067024505470209\n",
      "      mean_processing_ms: 4.663602074068255\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.366\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.70116424560547\n",
      "        critic_loss: 0.0024878676049411297\n",
      "        max_q: 16.9862003326416\n",
      "        mean_q: 16.658416748046875\n",
      "        min_q: 16.398656845092773\n",
      "        model: {}\n",
      "        td_error: 0.004975735209882259\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13300\n",
      "    num_steps_trained: 3020800\n",
      "    num_target_updates: 13300\n",
      "    opt_peak_throughput: 58635.006\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.502\n",
      "    sample_time_ms: 87.457\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.77058823529412\n",
      "    ram_util_percent: 70.09411764705882\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.54171138220215\n",
      "    mean_inference_ms: 0.6030050898706137\n",
      "    mean_processing_ms: 5.464433829081142\n",
      "  time_since_restore: 1359.1662802696228\n",
      "  time_this_iter_s: 10.094429969787598\n",
      "  time_total_s: 1359.1662802696228\n",
      "  timestamp: 1582121657\n",
      "  timesteps_since_restore: 13300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13300\n",
      "  training_iteration: 133\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         1359.17</td><td style=\"text-align: right;\">      13300</td><td style=\"text-align: right;\">   2.031</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-14-30\n",
      "  done: false\n",
      "  episode_len_mean: 21.37\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0370000303536653\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 700\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.19110261859939\n",
      "      mean_inference_ms: 0.7062509189193279\n",
      "      mean_processing_ms: 4.662080821303576\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.452\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.65172576904297\n",
      "        critic_loss: 0.0026697597932070494\n",
      "        max_q: 16.968320846557617\n",
      "        mean_q: 16.601558685302734\n",
      "        min_q: 16.22849464416504\n",
      "        model: {}\n",
      "        td_error: 0.005339520052075386\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13400\n",
      "    num_steps_trained: 3046400\n",
      "    num_target_updates: 13400\n",
      "    opt_peak_throughput: 57498.679\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.068\n",
      "    sample_time_ms: 86.937\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.599999999999998\n",
      "    ram_util_percent: 69.8263157894737\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.52528248680085\n",
      "    mean_inference_ms: 0.6026801605278911\n",
      "    mean_processing_ms: 5.459188417234002\n",
      "  time_since_restore: 1369.3607003688812\n",
      "  time_this_iter_s: 10.194420099258423\n",
      "  time_total_s: 1369.3607003688812\n",
      "  timestamp: 1582121670\n",
      "  timesteps_since_restore: 13400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13400\n",
      "  training_iteration: 134\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1369.36</td><td style=\"text-align: right;\">      13400</td><td style=\"text-align: right;\">   2.037</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-14-42\n",
      "  done: false\n",
      "  episode_len_mean: 21.38\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0380000303685666\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 705\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.21619059406072\n",
      "      mean_inference_ms: 0.706078531904437\n",
      "      mean_processing_ms: 4.654565226729308\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.337\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.617084503173828\n",
      "        critic_loss: 0.0026055152993649244\n",
      "        max_q: 16.837574005126953\n",
      "        mean_q: 16.566205978393555\n",
      "        min_q: 16.113168716430664\n",
      "        model: {}\n",
      "        td_error: 0.0052110301330685616\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13500\n",
      "    num_steps_trained: 3072000\n",
      "    num_target_updates: 13500\n",
      "    opt_peak_throughput: 59033.456\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.721\n",
      "    sample_time_ms: 87.289\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.54444444444444\n",
      "    ram_util_percent: 69.86666666666667\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.50846501929854\n",
      "    mean_inference_ms: 0.6023773345499548\n",
      "    mean_processing_ms: 5.453997151304559\n",
      "  time_since_restore: 1379.554933309555\n",
      "  time_this_iter_s: 10.194232940673828\n",
      "  time_total_s: 1379.554933309555\n",
      "  timestamp: 1582121682\n",
      "  timesteps_since_restore: 13500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13500\n",
      "  training_iteration: 135\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         1379.55</td><td style=\"text-align: right;\">      13500</td><td style=\"text-align: right;\">   2.038</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-14-55\n",
      "  done: false\n",
      "  episode_len_mean: 21.17\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0170000300556423\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 709\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.23654359354238\n",
      "      mean_inference_ms: 0.7053089804557836\n",
      "      mean_processing_ms: 4.648542419953665\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.38\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.60011100769043\n",
      "        critic_loss: 0.002823208924382925\n",
      "        max_q: 16.839956283569336\n",
      "        mean_q: 16.56324577331543\n",
      "        min_q: 16.30451011657715\n",
      "        model: {}\n",
      "        td_error: 0.00564641784876585\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13600\n",
      "    num_steps_trained: 3097600\n",
      "    num_target_updates: 13600\n",
      "    opt_peak_throughput: 58451.471\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.408\n",
      "    sample_time_ms: 77.58\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.005555555555556\n",
      "    ram_util_percent: 69.92777777777779\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.49489115071077\n",
      "    mean_inference_ms: 0.6021414109336649\n",
      "    mean_processing_ms: 5.449876926788741\n",
      "  time_since_restore: 1389.6482782363892\n",
      "  time_this_iter_s: 10.093344926834106\n",
      "  time_total_s: 1389.6482782363892\n",
      "  timestamp: 1582121695\n",
      "  timesteps_since_restore: 13600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13600\n",
      "  training_iteration: 136\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1389.65</td><td style=\"text-align: right;\">      13600</td><td style=\"text-align: right;\">   2.017</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-15-08\n",
      "  done: false\n",
      "  episode_len_mean: 21.36\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.036000030338764\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 713\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.25617718634017\n",
      "      mean_inference_ms: 0.7048688066287303\n",
      "      mean_processing_ms: 4.642791478965539\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.372\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.534954071044922\n",
      "        critic_loss: 0.0025310423225164413\n",
      "        max_q: 16.80742645263672\n",
      "        mean_q: 16.496944427490234\n",
      "        min_q: 16.155681610107422\n",
      "        model: {}\n",
      "        td_error: 0.005062084645032883\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13700\n",
      "    num_steps_trained: 3123200\n",
      "    num_target_updates: 13700\n",
      "    opt_peak_throughput: 58558.899\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.46\n",
      "    sample_time_ms: 77.563\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.894117647058824\n",
      "    ram_util_percent: 69.84705882352941\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.48152013314888\n",
      "    mean_inference_ms: 0.6019027813209642\n",
      "    mean_processing_ms: 5.44527644294591\n",
      "  time_since_restore: 1399.7410843372345\n",
      "  time_this_iter_s: 10.092806100845337\n",
      "  time_total_s: 1399.7410843372345\n",
      "  timestamp: 1582121708\n",
      "  timesteps_since_restore: 13700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13700\n",
      "  training_iteration: 137\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1399.74</td><td style=\"text-align: right;\">      13700</td><td style=\"text-align: right;\">   2.036</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-15-20\n",
      "  done: false\n",
      "  episode_len_mean: 21.57\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0570000306516887\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 718\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.23156559370081\n",
      "      mean_inference_ms: 0.7044747836438128\n",
      "      mean_processing_ms: 4.6503116022704285\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.972\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.468395233154297\n",
      "        critic_loss: 0.0035073182079941034\n",
      "        max_q: 16.71940040588379\n",
      "        mean_q: 16.42273712158203\n",
      "        min_q: 16.148229598999023\n",
      "        model: {}\n",
      "        td_error: 0.007014636415988207\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13800\n",
      "    num_steps_trained: 3148800\n",
      "    num_target_updates: 13800\n",
      "    opt_peak_throughput: 51492.482\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.807\n",
      "    sample_time_ms: 74.576\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.56666666666667\n",
      "    ram_util_percent: 69.88888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.46517224115794\n",
      "    mean_inference_ms: 0.6015949016796182\n",
      "    mean_processing_ms: 5.439386583621922\n",
      "  time_since_restore: 1409.9349782466888\n",
      "  time_this_iter_s: 10.193893909454346\n",
      "  time_total_s: 1409.9349782466888\n",
      "  timestamp: 1582121720\n",
      "  timesteps_since_restore: 13800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13800\n",
      "  training_iteration: 138\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         1409.93</td><td style=\"text-align: right;\">      13800</td><td style=\"text-align: right;\">   2.057</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 21.78\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.078000030964613\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 722\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.2173632171522\n",
      "      mean_inference_ms: 0.7042795922196483\n",
      "      mean_processing_ms: 4.654414149728421\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.614\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.481136322021484\n",
      "        critic_loss: 0.0029862348455935717\n",
      "        max_q: 16.71780014038086\n",
      "        mean_q: 16.43436050415039\n",
      "        min_q: 16.115615844726562\n",
      "        model: {}\n",
      "        td_error: 0.005972469691187143\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13900\n",
      "    num_steps_trained: 3174400\n",
      "    num_target_updates: 13900\n",
      "    opt_peak_throughput: 55485.659\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.587\n",
      "    sample_time_ms: 76.407\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.33529411764706\n",
      "    ram_util_percent: 70.17058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.4517525212678\n",
      "    mean_inference_ms: 0.6013580802852946\n",
      "    mean_processing_ms: 5.434210216120737\n",
      "  time_since_restore: 1420.0322020053864\n",
      "  time_this_iter_s: 10.09722375869751\n",
      "  time_total_s: 1420.0322020053864\n",
      "  timestamp: 1582121732\n",
      "  timesteps_since_restore: 13900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 13900\n",
      "  training_iteration: 139\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1420.03</td><td style=\"text-align: right;\">      13900</td><td style=\"text-align: right;\">   2.078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-15-44\n",
      "  done: false\n",
      "  episode_len_mean: 21.89\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.089000031128526\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 727\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.24691767802888\n",
      "      mean_inference_ms: 0.7037967060402917\n",
      "      mean_processing_ms: 4.645648759557533\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.52\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.446704864501953\n",
      "        critic_loss: 0.002704370766878128\n",
      "        max_q: 16.65712547302246\n",
      "        mean_q: 16.40970230102539\n",
      "        min_q: 16.126558303833008\n",
      "        model: {}\n",
      "        td_error: 0.005408741999417543\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 3200000\n",
      "    num_target_updates: 14000\n",
      "    opt_peak_throughput: 56638.823\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.564\n",
      "    sample_time_ms: 77.597\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.31764705882353\n",
      "    ram_util_percent: 69.22941176470587\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.43559909427168\n",
      "    mean_inference_ms: 0.6010715856522626\n",
      "    mean_processing_ms: 5.427489601896577\n",
      "  time_since_restore: 1430.2289447784424\n",
      "  time_this_iter_s: 10.19674277305603\n",
      "  time_total_s: 1430.2289447784424\n",
      "  timestamp: 1582121744\n",
      "  timesteps_since_restore: 14000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 140\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1430.23</td><td style=\"text-align: right;\">      14000</td><td style=\"text-align: right;\">   2.089</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 21.61\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.061000030711293\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 732\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.26162001050731\n",
      "      mean_inference_ms: 0.7031907083732063\n",
      "      mean_processing_ms: 4.640884752063794\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.214\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.403554916381836\n",
      "        critic_loss: 0.0028276508674025536\n",
      "        max_q: 16.681991577148438\n",
      "        mean_q: 16.37152862548828\n",
      "        min_q: 16.08460807800293\n",
      "        model: {}\n",
      "        td_error: 0.005655301734805107\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14100\n",
      "    num_steps_trained: 3225600\n",
      "    num_target_updates: 14100\n",
      "    opt_peak_throughput: 60749.871\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.161\n",
      "    sample_time_ms: 77.44\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.488888888888887\n",
      "    ram_util_percent: 69.23888888888888\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.4203906047197\n",
      "    mean_inference_ms: 0.6007914822684504\n",
      "    mean_processing_ms: 5.421620600614367\n",
      "  time_since_restore: 1440.4209425449371\n",
      "  time_this_iter_s: 10.191997766494751\n",
      "  time_total_s: 1440.4209425449371\n",
      "  timestamp: 1582121756\n",
      "  timesteps_since_restore: 14100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14100\n",
      "  training_iteration: 141\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1440.42</td><td style=\"text-align: right;\">      14100</td><td style=\"text-align: right;\">   2.061</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 21.66\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.066000030785799\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 736\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 34.0\n",
      "    episode_reward_max: 3.300000049173832\n",
      "    episode_reward_mean: 3.300000049173832\n",
      "    episode_reward_min: 3.300000049173832\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.31910769264994\n",
      "      mean_inference_ms: 0.7018725678808717\n",
      "      mean_processing_ms: 4.623932779793933\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.218\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.41017723083496\n",
      "        critic_loss: 0.002265093382447958\n",
      "        max_q: 16.653276443481445\n",
      "        mean_q: 16.372941970825195\n",
      "        min_q: 16.101051330566406\n",
      "        model: {}\n",
      "        td_error: 0.004530186764895916\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14200\n",
      "    num_steps_trained: 3251200\n",
      "    num_target_updates: 14200\n",
      "    opt_peak_throughput: 60690.811\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.787\n",
      "    sample_time_ms: 79.465\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.900000000000002\n",
      "    ram_util_percent: 69.40588235294118\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.40836489532524\n",
      "    mean_inference_ms: 0.6005663228515242\n",
      "    mean_processing_ms: 5.416687191674632\n",
      "  time_since_restore: 1450.5127084255219\n",
      "  time_this_iter_s: 10.091765880584717\n",
      "  time_total_s: 1450.5127084255219\n",
      "  timestamp: 1582121769\n",
      "  timesteps_since_restore: 14200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14200\n",
      "  training_iteration: 142\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1450.51</td><td style=\"text-align: right;\">      14200</td><td style=\"text-align: right;\">   2.066</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-16-22\n",
      "  done: false\n",
      "  episode_len_mean: 21.96\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0960000312328337\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 741\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.34667084954164\n",
      "      mean_inference_ms: 0.7011802467043305\n",
      "      mean_processing_ms: 4.615864555475748\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.389963150024414\n",
      "        critic_loss: 0.0023614359088242054\n",
      "        max_q: 16.627342224121094\n",
      "        mean_q: 16.34874725341797\n",
      "        min_q: 15.985602378845215\n",
      "        model: {}\n",
      "        td_error: 0.004722871817648411\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14300\n",
      "    num_steps_trained: 3276800\n",
      "    num_target_updates: 14300\n",
      "    opt_peak_throughput: 57182.668\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.452\n",
      "    sample_time_ms: 87.523\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.7421052631579\n",
      "    ram_util_percent: 69.41052631578948\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.39321852294091\n",
      "    mean_inference_ms: 0.6002938030329371\n",
      "    mean_processing_ms: 5.410681537708053\n",
      "  time_since_restore: 1460.7074875831604\n",
      "  time_this_iter_s: 10.19477915763855\n",
      "  time_total_s: 1460.7074875831604\n",
      "  timestamp: 1582121782\n",
      "  timesteps_since_restore: 14300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14300\n",
      "  training_iteration: 143\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         1460.71</td><td style=\"text-align: right;\">      14300</td><td style=\"text-align: right;\">   2.096</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 21.85\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.085000031068921\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 745\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.34609580728365\n",
      "      mean_inference_ms: 0.7004921423726406\n",
      "      mean_processing_ms: 4.616156012254708\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.47\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.326679229736328\n",
      "        critic_loss: 0.0022358186542987823\n",
      "        max_q: 16.572959899902344\n",
      "        mean_q: 16.289634704589844\n",
      "        min_q: 16.015798568725586\n",
      "        model: {}\n",
      "        td_error: 0.00447163637727499\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14400\n",
      "    num_steps_trained: 3302400\n",
      "    num_target_updates: 14400\n",
      "    opt_peak_throughput: 57267.758\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.919\n",
      "    sample_time_ms: 87.624\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.916666666666668\n",
      "    ram_util_percent: 69.21666666666665\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.38146986959164\n",
      "    mean_inference_ms: 0.6000712685744207\n",
      "    mean_processing_ms: 5.406176328867193\n",
      "  time_since_restore: 1470.7993557453156\n",
      "  time_this_iter_s: 10.091868162155151\n",
      "  time_total_s: 1470.7993557453156\n",
      "  timestamp: 1582121795\n",
      "  timesteps_since_restore: 14400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14400\n",
      "  training_iteration: 144\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">          1470.8</td><td style=\"text-align: right;\">      14400</td><td style=\"text-align: right;\">   2.085</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 21.92\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0920000311732294\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 750\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.3365274068567\n",
      "      mean_inference_ms: 0.6998799826325697\n",
      "      mean_processing_ms: 4.6193576160679255\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.027\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.325366973876953\n",
      "        critic_loss: 0.0027672015130519867\n",
      "        max_q: 16.595651626586914\n",
      "        mean_q: 16.294795989990234\n",
      "        min_q: 15.976508140563965\n",
      "        model: {}\n",
      "        td_error: 0.005534402094781399\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14500\n",
      "    num_steps_trained: 3328000\n",
      "    num_target_updates: 14500\n",
      "    opt_peak_throughput: 63575.271\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.224\n",
      "    sample_time_ms: 89.095\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.577777777777776\n",
      "    ram_util_percent: 69.27222222222223\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.3678181964885\n",
      "    mean_inference_ms: 0.5997759762870133\n",
      "    mean_processing_ms: 5.400570179284389\n",
      "  time_since_restore: 1480.9906866550446\n",
      "  time_this_iter_s: 10.191330909729004\n",
      "  time_total_s: 1480.9906866550446\n",
      "  timestamp: 1582121807\n",
      "  timesteps_since_restore: 14500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14500\n",
      "  training_iteration: 145\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         1480.99</td><td style=\"text-align: right;\">      14500</td><td style=\"text-align: right;\">   2.092</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 22.0\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.1000000312924385\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 755\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.37725325959819\n",
      "      mean_inference_ms: 0.6991614630869374\n",
      "      mean_processing_ms: 4.606823539384129\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.219\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.232337951660156\n",
      "        critic_loss: 0.002883681794628501\n",
      "        max_q: 16.41646385192871\n",
      "        mean_q: 16.194252014160156\n",
      "        min_q: 15.855982780456543\n",
      "        model: {}\n",
      "        td_error: 0.005767364054918289\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14600\n",
      "    num_steps_trained: 3353600\n",
      "    num_target_updates: 14600\n",
      "    opt_peak_throughput: 60681.893\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.724\n",
      "    sample_time_ms: 88.373\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.094117647058823\n",
      "    ram_util_percent: 69.11764705882354\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.35431767367515\n",
      "    mean_inference_ms: 0.599470645462689\n",
      "    mean_processing_ms: 5.394959826461798\n",
      "  time_since_restore: 1491.1832015514374\n",
      "  time_this_iter_s: 10.192514896392822\n",
      "  time_total_s: 1491.1832015514374\n",
      "  timestamp: 1582121819\n",
      "  timesteps_since_restore: 14600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14600\n",
      "  training_iteration: 146\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         1491.18</td><td style=\"text-align: right;\">      14600</td><td style=\"text-align: right;\">     2.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-17-12\n",
      "  done: false\n",
      "  episode_len_mean: 21.97\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.097000031247735\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 760\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.36324059582887\n",
      "      mean_inference_ms: 0.6991664315023454\n",
      "      mean_processing_ms: 4.611402831516596\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.746\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.264171600341797\n",
      "        critic_loss: 0.002398027805611491\n",
      "        max_q: 16.5386962890625\n",
      "        mean_q: 16.231098175048828\n",
      "        min_q: 15.929990768432617\n",
      "        model: {}\n",
      "        td_error: 0.004796055611222982\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14700\n",
      "    num_steps_trained: 3379200\n",
      "    num_target_updates: 14700\n",
      "    opt_peak_throughput: 53944.406\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.611\n",
      "    sample_time_ms: 77.024\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.238888888888887\n",
      "    ram_util_percent: 69.14444444444445\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.34103700041233\n",
      "    mean_inference_ms: 0.599167639682691\n",
      "    mean_processing_ms: 5.389396584147419\n",
      "  time_since_restore: 1501.3768644332886\n",
      "  time_this_iter_s: 10.193662881851196\n",
      "  time_total_s: 1501.3768644332886\n",
      "  timestamp: 1582121832\n",
      "  timesteps_since_restore: 14700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14700\n",
      "  training_iteration: 147\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         1501.38</td><td style=\"text-align: right;\">      14700</td><td style=\"text-align: right;\">   2.097</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-17-24\n",
      "  done: false\n",
      "  episode_len_mean: 21.89\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.089000031128526\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 766\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.37981870039997\n",
      "      mean_inference_ms: 0.6988957088540612\n",
      "      mean_processing_ms: 4.606038358939639\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.524\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.196338653564453\n",
      "        critic_loss: 0.002520892536267638\n",
      "        max_q: 16.403261184692383\n",
      "        mean_q: 16.167865753173828\n",
      "        min_q: 15.894156455993652\n",
      "        model: {}\n",
      "        td_error: 0.005041785072535276\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14800\n",
      "    num_steps_trained: 3404800\n",
      "    num_target_updates: 14800\n",
      "    opt_peak_throughput: 56588.079\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.859\n",
      "    sample_time_ms: 86.977\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.647058823529413\n",
      "    ram_util_percent: 69.23529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.32572097464484\n",
      "    mean_inference_ms: 0.5988257792160917\n",
      "    mean_processing_ms: 5.382804887758599\n",
      "  time_since_restore: 1511.6717565059662\n",
      "  time_this_iter_s: 10.294892072677612\n",
      "  time_total_s: 1511.6717565059662\n",
      "  timestamp: 1582121844\n",
      "  timesteps_since_restore: 14800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14800\n",
      "  training_iteration: 148\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         1511.67</td><td style=\"text-align: right;\">      14800</td><td style=\"text-align: right;\">   2.089</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 21.72\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.072000030875206\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 771\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.37468957015324\n",
      "      mean_inference_ms: 0.6984773690719102\n",
      "      mean_processing_ms: 4.607633390135219\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.523\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.16065216064453\n",
      "        critic_loss: 0.001995398662984371\n",
      "        max_q: 16.345415115356445\n",
      "        mean_q: 16.123380661010742\n",
      "        min_q: 15.859956741333008\n",
      "        model: {}\n",
      "        td_error: 0.003990797325968742\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14900\n",
      "    num_steps_trained: 3430400\n",
      "    num_target_updates: 14900\n",
      "    opt_peak_throughput: 56594.939\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.459\n",
      "    sample_time_ms: 87.65\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.91111111111111\n",
      "    ram_util_percent: 69.36666666666666\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.31314681382587\n",
      "    mean_inference_ms: 0.5985436399337889\n",
      "    mean_processing_ms: 5.377576569022889\n",
      "  time_since_restore: 1521.866863489151\n",
      "  time_this_iter_s: 10.195106983184814\n",
      "  time_total_s: 1521.866863489151\n",
      "  timestamp: 1582121857\n",
      "  timesteps_since_restore: 14900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 14900\n",
      "  training_iteration: 149\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         1521.87</td><td style=\"text-align: right;\">      14900</td><td style=\"text-align: right;\">   2.072</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-17-49\n",
      "  done: false\n",
      "  episode_len_mean: 21.41\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.04100003041327\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 776\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.3919707677027\n",
      "      mean_inference_ms: 0.6978255248098113\n",
      "      mean_processing_ms: 4.602496391527718\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.21\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.143714904785156\n",
      "        critic_loss: 0.0019656263757497072\n",
      "        max_q: 16.35561752319336\n",
      "        mean_q: 16.114822387695312\n",
      "        min_q: 15.852514266967773\n",
      "        model: {}\n",
      "        td_error: 0.003931252285838127\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 3456000\n",
      "    num_target_updates: 15000\n",
      "    opt_peak_throughput: 60811.802\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.818\n",
      "    sample_time_ms: 88.399\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.78235294117647\n",
      "    ram_util_percent: 69.4470588235294\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.3009767910417\n",
      "    mean_inference_ms: 0.5982847713895667\n",
      "    mean_processing_ms: 5.37326860627065\n",
      "  time_since_restore: 1532.0589163303375\n",
      "  time_this_iter_s: 10.192052841186523\n",
      "  time_total_s: 1532.0589163303375\n",
      "  timestamp: 1582121869\n",
      "  timesteps_since_restore: 15000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 150\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         1532.06</td><td style=\"text-align: right;\">      15000</td><td style=\"text-align: right;\">   2.041</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-18-01\n",
      "  done: false\n",
      "  episode_len_mean: 21.49\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.049000030532479\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 781\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.43051831185028\n",
      "      mean_inference_ms: 0.6974104402342671\n",
      "      mean_processing_ms: 4.590696157868376\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.389\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.142658233642578\n",
      "        critic_loss: 0.0021426547318696976\n",
      "        max_q: 16.35417938232422\n",
      "        mean_q: 16.11444091796875\n",
      "        min_q: 15.844747543334961\n",
      "        model: {}\n",
      "        td_error: 0.004285309463739395\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15100\n",
      "    num_steps_trained: 3481600\n",
      "    num_target_updates: 15100\n",
      "    opt_peak_throughput: 58333.342\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.661\n",
      "    sample_time_ms: 88.144\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.327777777777776\n",
      "    ram_util_percent: 69.41666666666669\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.28886269937642\n",
      "    mean_inference_ms: 0.5980354129680937\n",
      "    mean_processing_ms: 5.369145901315405\n",
      "  time_since_restore: 1542.250975370407\n",
      "  time_this_iter_s: 10.19205904006958\n",
      "  time_total_s: 1542.250975370407\n",
      "  timestamp: 1582121881\n",
      "  timesteps_since_restore: 15100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15100\n",
      "  training_iteration: 151\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         1542.25</td><td style=\"text-align: right;\">      15100</td><td style=\"text-align: right;\">   2.049</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-18-14\n",
      "  done: false\n",
      "  episode_len_mean: 21.62\n",
      "  episode_reward_max: 4.100000061094761\n",
      "  episode_reward_mean: 2.0620000307261943\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 785\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 34.0\n",
      "    episode_reward_max: 3.300000049173832\n",
      "    episode_reward_mean: 3.300000049173832\n",
      "    episode_reward_min: 3.300000049173832\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.4811556741843\n",
      "      mean_inference_ms: 0.6972854844541374\n",
      "      mean_processing_ms: 4.575204418156786\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.139\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.114425659179688\n",
      "        critic_loss: 0.0020012157037854195\n",
      "        max_q: 16.3409366607666\n",
      "        mean_q: 16.08257293701172\n",
      "        min_q: 15.8651704788208\n",
      "        model: {}\n",
      "        td_error: 0.004002431407570839\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15200\n",
      "    num_steps_trained: 3507200\n",
      "    num_target_updates: 15200\n",
      "    opt_peak_throughput: 49815.898\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.595\n",
      "    sample_time_ms: 76.797\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.710526315789473\n",
      "    ram_util_percent: 69.45789473684209\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.27920200695478\n",
      "    mean_inference_ms: 0.5978412843131451\n",
      "    mean_processing_ms: 5.36574471659597\n",
      "  time_since_restore: 1552.346054315567\n",
      "  time_this_iter_s: 10.095078945159912\n",
      "  time_total_s: 1552.346054315567\n",
      "  timestamp: 1582121894\n",
      "  timesteps_since_restore: 15200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15200\n",
      "  training_iteration: 152\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         1552.35</td><td style=\"text-align: right;\">      15200</td><td style=\"text-align: right;\">   2.062</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 21.06\n",
      "  episode_reward_max: 3.500000052154064\n",
      "  episode_reward_mean: 2.0060000298917293\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 790\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.51847382363556\n",
      "      mean_inference_ms: 0.6967152474243516\n",
      "      mean_processing_ms: 4.56416021192694\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.594\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -16.09011459350586\n",
      "        critic_loss: 0.0022032989654690027\n",
      "        max_q: 16.406503677368164\n",
      "        mean_q: 16.06118392944336\n",
      "        min_q: 15.811867713928223\n",
      "        model: {}\n",
      "        td_error: 0.0044065979309380054\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15300\n",
      "    num_steps_trained: 3532800\n",
      "    num_target_updates: 15300\n",
      "    opt_peak_throughput: 55728.713\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.032\n",
      "    sample_time_ms: 76.624\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.21578947368421\n",
      "    ram_util_percent: 69.21578947368421\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.26638647870476\n",
      "    mean_inference_ms: 0.597623166638562\n",
      "    mean_processing_ms: 5.361944648239351\n",
      "  time_since_restore: 1562.5399851799011\n",
      "  time_this_iter_s: 10.193930864334106\n",
      "  time_total_s: 1562.5399851799011\n",
      "  timestamp: 1582121908\n",
      "  timesteps_since_restore: 15300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15300\n",
      "  training_iteration: 153\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         1562.54</td><td style=\"text-align: right;\">      15300</td><td style=\"text-align: right;\">   2.006</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 20.99\n",
      "  episode_reward_max: 3.400000050663948\n",
      "  episode_reward_mean: 1.9990000297874213\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 794\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.52476863882983\n",
      "      mean_inference_ms: 0.6967650215332872\n",
      "      mean_processing_ms: 4.561965692741315\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.261\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.985265731811523\n",
      "        critic_loss: 0.001836680807173252\n",
      "        max_q: 16.157236099243164\n",
      "        mean_q: 15.959388732910156\n",
      "        min_q: 15.686262130737305\n",
      "        model: {}\n",
      "        td_error: 0.003673361614346504\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15400\n",
      "    num_steps_trained: 3558400\n",
      "    num_target_updates: 15400\n",
      "    opt_peak_throughput: 60074.514\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.129\n",
      "    sample_time_ms: 77.975\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.47222222222222\n",
      "    ram_util_percent: 69.56111111111109\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.25656070213944\n",
      "    mean_inference_ms: 0.5974568776834982\n",
      "    mean_processing_ms: 5.358760732029925\n",
      "  time_since_restore: 1572.6337630748749\n",
      "  time_this_iter_s: 10.093777894973755\n",
      "  time_total_s: 1572.6337630748749\n",
      "  timestamp: 1582121921\n",
      "  timesteps_since_restore: 15400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15400\n",
      "  training_iteration: 154\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         1572.63</td><td style=\"text-align: right;\">      15400</td><td style=\"text-align: right;\">   1.999</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-18-53\n",
      "  done: false\n",
      "  episode_len_mean: 21.2\n",
      "  episode_reward_max: 3.8000000566244125\n",
      "  episode_reward_mean: 2.020000030100346\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 799\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.56115743004919\n",
      "      mean_inference_ms: 0.6959340650360412\n",
      "      mean_processing_ms: 4.551296383354437\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.021\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.990131378173828\n",
      "        critic_loss: 0.002128687221556902\n",
      "        max_q: 16.162303924560547\n",
      "        mean_q: 15.962236404418945\n",
      "        min_q: 15.467373847961426\n",
      "        model: {}\n",
      "        td_error: 0.004257374443113804\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15500\n",
      "    num_steps_trained: 3584000\n",
      "    num_target_updates: 15500\n",
      "    opt_peak_throughput: 63659.702\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.829\n",
      "    sample_time_ms: 89.262\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.305555555555557\n",
      "    ram_util_percent: 69.78888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.24506534653686\n",
      "    mean_inference_ms: 0.5972391868978039\n",
      "    mean_processing_ms: 5.35488934405332\n",
      "  time_since_restore: 1582.8244593143463\n",
      "  time_this_iter_s: 10.190696239471436\n",
      "  time_total_s: 1582.8244593143463\n",
      "  timestamp: 1582121933\n",
      "  timesteps_since_restore: 15500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15500\n",
      "  training_iteration: 155\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         1582.82</td><td style=\"text-align: right;\">      15500</td><td style=\"text-align: right;\">    2.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-19-06\n",
      "  done: false\n",
      "  episode_len_mean: 21.22\n",
      "  episode_reward_max: 3.8000000566244125\n",
      "  episode_reward_mean: 2.022000030130148\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 803\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.53889867862516\n",
      "      mean_inference_ms: 0.6955626461811001\n",
      "      mean_processing_ms: 4.55958542170368\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.169\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.953060150146484\n",
      "        critic_loss: 0.0018346670549362898\n",
      "        max_q: 16.139747619628906\n",
      "        mean_q: 15.926129341125488\n",
      "        min_q: 15.691071510314941\n",
      "        model: {}\n",
      "        td_error: 0.0036693343427032232\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15600\n",
      "    num_steps_trained: 3609600\n",
      "    num_target_updates: 15600\n",
      "    opt_peak_throughput: 61411.068\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.046\n",
      "    sample_time_ms: 79.977\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.88888888888889\n",
      "    ram_util_percent: 69.97222222222221\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.23643770094161\n",
      "    mean_inference_ms: 0.5970593857134676\n",
      "    mean_processing_ms: 5.351559496478587\n",
      "  time_since_restore: 1592.913550376892\n",
      "  time_this_iter_s: 10.089091062545776\n",
      "  time_total_s: 1592.913550376892\n",
      "  timestamp: 1582121946\n",
      "  timesteps_since_restore: 15600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15600\n",
      "  training_iteration: 156\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         1592.91</td><td style=\"text-align: right;\">      15600</td><td style=\"text-align: right;\">   2.022</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-19-18\n",
      "  done: false\n",
      "  episode_len_mean: 21.59\n",
      "  episode_reward_max: 3.8000000566244125\n",
      "  episode_reward_mean: 2.059000030681491\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 807\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.54833534751208\n",
      "      mean_inference_ms: 0.6948798970288697\n",
      "      mean_processing_ms: 4.556161485296104\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.108\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.9053955078125\n",
      "        critic_loss: 0.001697199884802103\n",
      "        max_q: 16.13373565673828\n",
      "        mean_q: 15.880468368530273\n",
      "        min_q: 15.612043380737305\n",
      "        model: {}\n",
      "        td_error: 0.0033944000024348497\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15700\n",
      "    num_steps_trained: 3635200\n",
      "    num_target_updates: 15700\n",
      "    opt_peak_throughput: 62317.432\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.036\n",
      "    sample_time_ms: 90.999\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.758823529411767\n",
      "    ram_util_percent: 70.19411764705882\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.22844565541295\n",
      "    mean_inference_ms: 0.5968645458849093\n",
      "    mean_processing_ms: 5.34813481419706\n",
      "  time_since_restore: 1603.011480331421\n",
      "  time_this_iter_s: 10.097929954528809\n",
      "  time_total_s: 1603.011480331421\n",
      "  timestamp: 1582121958\n",
      "  timesteps_since_restore: 15700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15700\n",
      "  training_iteration: 157\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         1603.01</td><td style=\"text-align: right;\">      15700</td><td style=\"text-align: right;\">   2.059</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 21.96\n",
      "  episode_reward_max: 4.400000065565109\n",
      "  episode_reward_mean: 2.0960000312328337\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 810\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.54280561945038\n",
      "      mean_inference_ms: 0.6942705710951078\n",
      "      mean_processing_ms: 4.5579119458952855\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.105\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.868339538574219\n",
      "        critic_loss: 0.0018159914761781693\n",
      "        max_q: 16.10125160217285\n",
      "        mean_q: 15.837717056274414\n",
      "        min_q: 15.587204933166504\n",
      "        model: {}\n",
      "        td_error: 0.0036319829523563385\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15800\n",
      "    num_steps_trained: 3660800\n",
      "    num_target_updates: 15800\n",
      "    opt_peak_throughput: 62355.794\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.809\n",
      "    sample_time_ms: 89.395\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.376470588235296\n",
      "    ram_util_percent: 70.28823529411765\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.22276166370723\n",
      "    mean_inference_ms: 0.596710947511014\n",
      "    mean_processing_ms: 5.345451717675667\n",
      "  time_since_restore: 1613.0025084018707\n",
      "  time_this_iter_s: 9.991028070449829\n",
      "  time_total_s: 1613.0025084018707\n",
      "  timestamp: 1582121970\n",
      "  timesteps_since_restore: 15800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15800\n",
      "  training_iteration: 158\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">            1613</td><td style=\"text-align: right;\">      15800</td><td style=\"text-align: right;\">   2.096</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-19-42\n",
      "  done: false\n",
      "  episode_len_mean: 22.2\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.120000031590462\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 813\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.52912473267637\n",
      "      mean_inference_ms: 0.6937513359394344\n",
      "      mean_processing_ms: 4.562101663733178\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.246\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.841859817504883\n",
      "        critic_loss: 0.0019534416496753693\n",
      "        max_q: 15.984774589538574\n",
      "        mean_q: 15.810304641723633\n",
      "        min_q: 15.570595741271973\n",
      "        model: {}\n",
      "        td_error: 0.0039068832993507385\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15900\n",
      "    num_steps_trained: 3686400\n",
      "    num_target_updates: 15900\n",
      "    opt_peak_throughput: 60290.735\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.315\n",
      "    sample_time_ms: 88.974\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.494117647058825\n",
      "    ram_util_percent: 70.20588235294117\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.2172268111693\n",
      "    mean_inference_ms: 0.596549321208294\n",
      "    mean_processing_ms: 5.342694432680775\n",
      "  time_since_restore: 1622.9951763153076\n",
      "  time_this_iter_s: 9.99266791343689\n",
      "  time_total_s: 1622.9951763153076\n",
      "  timestamp: 1582121982\n",
      "  timesteps_since_restore: 15900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 15900\n",
      "  training_iteration: 159\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">            1623</td><td style=\"text-align: right;\">      15900</td><td style=\"text-align: right;\">    2.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 22.25\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.1250000316649675\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 817\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 11.0\n",
      "    episode_reward_max: 1.0000000149011612\n",
      "    episode_reward_mean: 1.0000000149011612\n",
      "    episode_reward_min: 1.0000000149011612\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.48278031698088\n",
      "      mean_inference_ms: 0.6935204608477404\n",
      "      mean_processing_ms: 4.57630605496747\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.212\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.803910255432129\n",
      "        critic_loss: 0.0019166867714375257\n",
      "        max_q: 15.98856258392334\n",
      "        mean_q: 15.771122932434082\n",
      "        min_q: 15.547727584838867\n",
      "        model: {}\n",
      "        td_error: 0.0038333735428750515\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 3712000\n",
      "    num_target_updates: 16000\n",
      "    opt_peak_throughput: 60782.197\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.666\n",
      "    sample_time_ms: 77.76\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.323529411764703\n",
      "    ram_util_percent: 69.57058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.210006621037\n",
      "    mean_inference_ms: 0.5963403113145561\n",
      "    mean_processing_ms: 5.3388039612791225\n",
      "  time_since_restore: 1633.0873172283173\n",
      "  time_this_iter_s: 10.092140913009644\n",
      "  time_total_s: 1633.0873172283173\n",
      "  timestamp: 1582121994\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 160\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         1633.09</td><td style=\"text-align: right;\">      16000</td><td style=\"text-align: right;\">   2.125</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 22.1\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.11000003144145\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 822\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.46497201788051\n",
      "      mean_inference_ms: 0.6932765163246992\n",
      "      mean_processing_ms: 4.581832912043189\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.586\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.784069061279297\n",
      "        critic_loss: 0.0015130770625546575\n",
      "        max_q: 15.941132545471191\n",
      "        mean_q: 15.761296272277832\n",
      "        min_q: 15.472773551940918\n",
      "        model: {}\n",
      "        td_error: 0.0030261538922786713\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16100\n",
      "    num_steps_trained: 3737600\n",
      "    num_target_updates: 16100\n",
      "    opt_peak_throughput: 55827.809\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.209\n",
      "    sample_time_ms: 76.605\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.813333333333333\n",
      "    ram_util_percent: 69.42\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.20100459149816\n",
      "    mean_inference_ms: 0.5960751479679377\n",
      "    mean_processing_ms: 5.334267419293924\n",
      "  time_since_restore: 1643.2818641662598\n",
      "  time_this_iter_s: 10.194546937942505\n",
      "  time_total_s: 1643.2818641662598\n",
      "  timestamp: 1582122005\n",
      "  timesteps_since_restore: 16100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16100\n",
      "  training_iteration: 161\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         1643.28</td><td style=\"text-align: right;\">      16100</td><td style=\"text-align: right;\">    2.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-20-16\n",
      "  done: false\n",
      "  episode_len_mean: 22.26\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.126000031679869\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 826\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.45908338820277\n",
      "      mean_inference_ms: 0.6937959732210469\n",
      "      mean_processing_ms: 4.583274301581753\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.638\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.7743501663208\n",
      "        critic_loss: 0.002227570628747344\n",
      "        max_q: 15.977627754211426\n",
      "        mean_q: 15.746992111206055\n",
      "        min_q: 15.53283405303955\n",
      "        model: {}\n",
      "        td_error: 0.004455140791833401\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16200\n",
      "    num_steps_trained: 3763200\n",
      "    num_target_updates: 16200\n",
      "    opt_peak_throughput: 45403.841\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 22.14\n",
      "    sample_time_ms: 71.415\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.511764705882353\n",
      "    ram_util_percent: 69.4764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.19295720109311\n",
      "    mean_inference_ms: 0.5958870253098832\n",
      "    mean_processing_ms: 5.330667010393268\n",
      "  time_since_restore: 1653.3819971084595\n",
      "  time_this_iter_s: 10.100132942199707\n",
      "  time_total_s: 1653.3819971084595\n",
      "  timestamp: 1582122016\n",
      "  timesteps_since_restore: 16200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16200\n",
      "  training_iteration: 162\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         1653.38</td><td style=\"text-align: right;\">      16200</td><td style=\"text-align: right;\">   2.126</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 22.24\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.1240000316500662\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 831\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.44845939742339\n",
      "      mean_inference_ms: 0.6951632992526248\n",
      "      mean_processing_ms: 4.586517632023853\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 6.415\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.727620124816895\n",
      "        critic_loss: 0.0016707018949091434\n",
      "        max_q: 15.925722122192383\n",
      "        mean_q: 15.702924728393555\n",
      "        min_q: 15.513650894165039\n",
      "        model: {}\n",
      "        td_error: 0.0033414035569876432\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16300\n",
      "    num_steps_trained: 3788800\n",
      "    num_target_updates: 16300\n",
      "    opt_peak_throughput: 39905.075\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 25.212\n",
      "    sample_time_ms: 77.194\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.54705882352941\n",
      "    ram_util_percent: 69.66470588235293\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.18016151575512\n",
      "    mean_inference_ms: 0.5957191976664157\n",
      "    mean_processing_ms: 5.326139898608589\n",
      "  time_since_restore: 1663.5865819454193\n",
      "  time_this_iter_s: 10.204584836959839\n",
      "  time_total_s: 1663.5865819454193\n",
      "  timestamp: 1582122029\n",
      "  timesteps_since_restore: 16300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16300\n",
      "  training_iteration: 163\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         1663.59</td><td style=\"text-align: right;\">      16300</td><td style=\"text-align: right;\">   2.124</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-20-40\n",
      "  done: false\n",
      "  episode_len_mean: 22.31\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.1310000317543745\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 835\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.43801229097735\n",
      "      mean_inference_ms: 0.6959738367968922\n",
      "      mean_processing_ms: 4.589656778619578\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 7.669\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.716495513916016\n",
      "        critic_loss: 0.0019308063201606274\n",
      "        max_q: 15.969873428344727\n",
      "        mean_q: 15.692842483520508\n",
      "        min_q: 15.387070655822754\n",
      "        model: {}\n",
      "        td_error: 0.0038616126403212547\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16400\n",
      "    num_steps_trained: 3814400\n",
      "    num_target_updates: 16400\n",
      "    opt_peak_throughput: 33380.852\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 27.162\n",
      "    sample_time_ms: 63.567\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.15882352941177\n",
      "    ram_util_percent: 69.27058823529411\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.1663675072944\n",
      "    mean_inference_ms: 0.5956643714392843\n",
      "    mean_processing_ms: 5.32254293438066\n",
      "  time_since_restore: 1673.6932039260864\n",
      "  time_this_iter_s: 10.106621980667114\n",
      "  time_total_s: 1673.6932039260864\n",
      "  timestamp: 1582122040\n",
      "  timesteps_since_restore: 16400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16400\n",
      "  training_iteration: 164\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1673.69</td><td style=\"text-align: right;\">      16400</td><td style=\"text-align: right;\">   2.131</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-20-52\n",
      "  done: false\n",
      "  episode_len_mean: 22.27\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.1270000316947697\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 840\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.4571585929965\n",
      "      mean_inference_ms: 0.6954391989967292\n",
      "      mean_processing_ms: 4.583611562807969\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.265\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.635992050170898\n",
      "        critic_loss: 0.002175635192543268\n",
      "        max_q: 15.841790199279785\n",
      "        mean_q: 15.604719161987305\n",
      "        min_q: 15.40748405456543\n",
      "        model: {}\n",
      "        td_error: 0.004351270385086536\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16500\n",
      "    num_steps_trained: 3840000\n",
      "    num_target_updates: 16500\n",
      "    opt_peak_throughput: 60030.18\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.04\n",
      "    sample_time_ms: 87.981\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.700000000000003\n",
      "    ram_util_percent: 70.31764705882352\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.14841406592501\n",
      "    mean_inference_ms: 0.5956224000285918\n",
      "    mean_processing_ms: 5.3181854115974305\n",
      "  time_since_restore: 1683.88649392128\n",
      "  time_this_iter_s: 10.193289995193481\n",
      "  time_total_s: 1683.88649392128\n",
      "  timestamp: 1582122052\n",
      "  timesteps_since_restore: 16500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16500\n",
      "  training_iteration: 165\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         1683.89</td><td style=\"text-align: right;\">      16500</td><td style=\"text-align: right;\">   2.127</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-21-05\n",
      "  done: false\n",
      "  episode_len_mean: 22.26\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.126000031679869\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 843\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.47654583147585\n",
      "      mean_inference_ms: 0.6947206386554945\n",
      "      mean_processing_ms: 4.577658196178804\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.467\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.648815155029297\n",
      "        critic_loss: 0.0013626681175082922\n",
      "        max_q: 15.820487022399902\n",
      "        mean_q: 15.62582015991211\n",
      "        min_q: 15.410983085632324\n",
      "        model: {}\n",
      "        td_error: 0.0027253362350165844\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16600\n",
      "    num_steps_trained: 3865600\n",
      "    num_target_updates: 16600\n",
      "    opt_peak_throughput: 57310.245\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.115\n",
      "    sample_time_ms: 77.312\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.138888888888893\n",
      "    ram_util_percent: 70.60555555555555\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.13793850839843\n",
      "    mean_inference_ms: 0.5955925468121178\n",
      "    mean_processing_ms: 5.315363471933304\n",
      "  time_since_restore: 1693.880611896515\n",
      "  time_this_iter_s: 9.994117975234985\n",
      "  time_total_s: 1693.880611896515\n",
      "  timestamp: 1582122065\n",
      "  timesteps_since_restore: 16600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16600\n",
      "  training_iteration: 166\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1693.88</td><td style=\"text-align: right;\">      16600</td><td style=\"text-align: right;\">   2.126</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-21-18\n",
      "  done: false\n",
      "  episode_len_mean: 22.47\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.147000031992793\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 848\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.47170289256103\n",
      "      mean_inference_ms: 0.6941468838616168\n",
      "      mean_processing_ms: 4.5793221264301245\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.266\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.612403869628906\n",
      "        critic_loss: 0.001663524890318513\n",
      "        max_q: 15.762822151184082\n",
      "        mean_q: 15.58232593536377\n",
      "        min_q: 15.372068405151367\n",
      "        model: {}\n",
      "        td_error: 0.0033270500134676695\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16700\n",
      "    num_steps_trained: 3891200\n",
      "    num_target_updates: 16700\n",
      "    opt_peak_throughput: 60003.343\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.389\n",
      "    sample_time_ms: 89.282\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.86111111111111\n",
      "    ram_util_percent: 71.13333333333331\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.12121475524255\n",
      "    mean_inference_ms: 0.5955407639947046\n",
      "    mean_processing_ms: 5.310485918186417\n",
      "  time_since_restore: 1704.0765101909637\n",
      "  time_this_iter_s: 10.195898294448853\n",
      "  time_total_s: 1704.0765101909637\n",
      "  timestamp: 1582122078\n",
      "  timesteps_since_restore: 16700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16700\n",
      "  training_iteration: 167\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1704.08</td><td style=\"text-align: right;\">      16700</td><td style=\"text-align: right;\">   2.147</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-21-30\n",
      "  done: false\n",
      "  episode_len_mean: 22.63\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.1630000322312117\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 852\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.4550474610909\n",
      "      mean_inference_ms: 0.6937992635858122\n",
      "      mean_processing_ms: 4.58446741104126\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.16\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.570499420166016\n",
      "        critic_loss: 0.0017426768317818642\n",
      "        max_q: 15.786696434020996\n",
      "        mean_q: 15.549736976623535\n",
      "        min_q: 15.367438316345215\n",
      "        model: {}\n",
      "        td_error: 0.0034853536635637283\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16800\n",
      "    num_steps_trained: 3916800\n",
      "    num_target_updates: 16800\n",
      "    opt_peak_throughput: 61537.42\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.364\n",
      "    sample_time_ms: 88.864\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.123529411764707\n",
      "    ram_util_percent: 71.1235294117647\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.10802241748682\n",
      "    mean_inference_ms: 0.5954995119314954\n",
      "    mean_processing_ms: 5.306375046463924\n",
      "  time_since_restore: 1714.1648004055023\n",
      "  time_this_iter_s: 10.088290214538574\n",
      "  time_total_s: 1714.1648004055023\n",
      "  timestamp: 1582122090\n",
      "  timesteps_since_restore: 16800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16800\n",
      "  training_iteration: 168\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         1714.16</td><td style=\"text-align: right;\">      16800</td><td style=\"text-align: right;\">   2.163</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 22.79\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.1790000324696304\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 856\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.47332209353375\n",
      "      mean_inference_ms: 0.6933339411798412\n",
      "      mean_processing_ms: 4.579229808273295\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.634\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.523653030395508\n",
      "        critic_loss: 0.0015393132343888283\n",
      "        max_q: 15.693222045898438\n",
      "        mean_q: 15.507040023803711\n",
      "        min_q: 15.31639575958252\n",
      "        model: {}\n",
      "        td_error: 0.0030786267016083\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16900\n",
      "    num_steps_trained: 3942400\n",
      "    num_target_updates: 16900\n",
      "    opt_peak_throughput: 55248.411\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.114\n",
      "    sample_time_ms: 87.558\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.89411764705882\n",
      "    ram_util_percent: 71.14117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.09509580305927\n",
      "    mean_inference_ms: 0.5954496140912627\n",
      "    mean_processing_ms: 5.302057482270851\n",
      "  time_since_restore: 1724.2639594078064\n",
      "  time_this_iter_s: 10.099159002304077\n",
      "  time_total_s: 1724.2639594078064\n",
      "  timestamp: 1582122101\n",
      "  timesteps_since_restore: 16900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16900\n",
      "  training_iteration: 169\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1724.26</td><td style=\"text-align: right;\">      16900</td><td style=\"text-align: right;\">   2.179</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-21-54\n",
      "  done: false\n",
      "  episode_len_mean: 22.97\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.197000032737851\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 860\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.47204099432486\n",
      "      mean_inference_ms: 0.6930465703223511\n",
      "      mean_processing_ms: 4.579683295364759\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.21\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.501753807067871\n",
      "        critic_loss: 0.0016317752888426185\n",
      "        max_q: 15.672074317932129\n",
      "        mean_q: 15.478594779968262\n",
      "        min_q: 15.296257019042969\n",
      "        model: {}\n",
      "        td_error: 0.0032635503448545933\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 3968000\n",
      "    num_target_updates: 17000\n",
      "    opt_peak_throughput: 60812.835\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.886\n",
      "    sample_time_ms: 78.244\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.047058823529415\n",
      "    ram_util_percent: 71.29411764705883\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.0824661709696\n",
      "    mean_inference_ms: 0.5953923753973924\n",
      "    mean_processing_ms: 5.297526377322927\n",
      "  time_since_restore: 1734.3533952236176\n",
      "  time_this_iter_s: 10.089435815811157\n",
      "  time_total_s: 1734.3533952236176\n",
      "  timestamp: 1582122114\n",
      "  timesteps_since_restore: 17000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 170\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1734.35</td><td style=\"text-align: right;\">      17000</td><td style=\"text-align: right;\">   2.197</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 23.43\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.2430000334233045\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 864\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.4871691692664\n",
      "      mean_inference_ms: 0.6927845852229692\n",
      "      mean_processing_ms: 4.575507217189527\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.512\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.442329406738281\n",
      "        critic_loss: 0.0016301439609378576\n",
      "        max_q: 15.629230499267578\n",
      "        mean_q: 15.425688743591309\n",
      "        min_q: 14.951255798339844\n",
      "        model: {}\n",
      "        td_error: 0.0032602879218757153\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17100\n",
      "    num_steps_trained: 3993600\n",
      "    num_target_updates: 17100\n",
      "    opt_peak_throughput: 56739.386\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.627\n",
      "    sample_time_ms: 87.003\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.28888888888889\n",
      "    ram_util_percent: 71.35555555555555\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.06992700756338\n",
      "    mean_inference_ms: 0.5953334865682746\n",
      "    mean_processing_ms: 5.292773519509244\n",
      "  time_since_restore: 1744.4476890563965\n",
      "  time_this_iter_s: 10.09429383277893\n",
      "  time_total_s: 1744.4476890563965\n",
      "  timestamp: 1582122126\n",
      "  timesteps_since_restore: 17100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17100\n",
      "  training_iteration: 171\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         1744.45</td><td style=\"text-align: right;\">      17100</td><td style=\"text-align: right;\">   2.243</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 23.38\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.238000033348799\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 868\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.46197342238233\n",
      "      mean_inference_ms: 0.6925612572581539\n",
      "      mean_processing_ms: 4.582805801072314\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.706\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.420984268188477\n",
      "        critic_loss: 0.0014384689275175333\n",
      "        max_q: 15.640268325805664\n",
      "        mean_q: 15.403592109680176\n",
      "        min_q: 15.190393447875977\n",
      "        model: {}\n",
      "        td_error: 0.0028769378550350666\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17200\n",
      "    num_steps_trained: 4019200\n",
      "    num_target_updates: 17200\n",
      "    opt_peak_throughput: 54403.586\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.326\n",
      "    sample_time_ms: 76.479\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.852941176470587\n",
      "    ram_util_percent: 71.43529411764705\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.05718149662374\n",
      "    mean_inference_ms: 0.5952769810523418\n",
      "    mean_processing_ms: 5.287782198010027\n",
      "  time_since_restore: 1754.5429010391235\n",
      "  time_this_iter_s: 10.09521198272705\n",
      "  time_total_s: 1754.5429010391235\n",
      "  timestamp: 1582122138\n",
      "  timesteps_since_restore: 17200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17200\n",
      "  training_iteration: 172\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1754.54</td><td style=\"text-align: right;\">      17200</td><td style=\"text-align: right;\">   2.238</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-22-30\n",
      "  done: false\n",
      "  episode_len_mean: 23.74\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.2740000338852404\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 872\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.44899507537292\n",
      "      mean_inference_ms: 0.6927124945240952\n",
      "      mean_processing_ms: 4.5867544826321245\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.926\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.394973754882812\n",
      "        critic_loss: 0.0016258102841675282\n",
      "        max_q: 15.568729400634766\n",
      "        mean_q: 15.374002456665039\n",
      "        min_q: 15.032151222229004\n",
      "        model: {}\n",
      "        td_error: 0.0032516208011657\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17300\n",
      "    num_steps_trained: 4044800\n",
      "    num_target_updates: 17300\n",
      "    opt_peak_throughput: 51973.021\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.623\n",
      "    sample_time_ms: 85.087\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.67058823529412\n",
      "    ram_util_percent: 71.43529411764705\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.04393740014548\n",
      "    mean_inference_ms: 0.5952295598709045\n",
      "    mean_processing_ms: 5.282556985515227\n",
      "  time_since_restore: 1764.6532123088837\n",
      "  time_this_iter_s: 10.110311269760132\n",
      "  time_total_s: 1764.6532123088837\n",
      "  timestamp: 1582122150\n",
      "  timesteps_since_restore: 17300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17300\n",
      "  training_iteration: 173\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1764.65</td><td style=\"text-align: right;\">      17300</td><td style=\"text-align: right;\">   2.274</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 23.79\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.2790000339597465\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 877\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.43288945858896\n",
      "      mean_inference_ms: 0.6927749639645814\n",
      "      mean_processing_ms: 4.591720985170519\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.944\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.402141571044922\n",
      "        critic_loss: 0.0020042615942656994\n",
      "        max_q: 15.637640953063965\n",
      "        mean_q: 15.385554313659668\n",
      "        min_q: 15.21072006225586\n",
      "        model: {}\n",
      "        td_error: 0.004008523188531399\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17400\n",
      "    num_steps_trained: 4070400\n",
      "    num_target_updates: 17400\n",
      "    opt_peak_throughput: 51776.537\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.867\n",
      "    sample_time_ms: 84.441\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.994117647058825\n",
      "    ram_util_percent: 72.69411764705883\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.02647691685799\n",
      "    mean_inference_ms: 0.5951974661789814\n",
      "    mean_processing_ms: 5.275827478904496\n",
      "  time_since_restore: 1774.8380002975464\n",
      "  time_this_iter_s: 10.18478798866272\n",
      "  time_total_s: 1774.8380002975464\n",
      "  timestamp: 1582122162\n",
      "  timesteps_since_restore: 17400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17400\n",
      "  training_iteration: 174\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1774.84</td><td style=\"text-align: right;\">      17400</td><td style=\"text-align: right;\">   2.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 24.06\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.3060000343620777\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 880\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 36.0\n",
      "    episode_reward_max: 3.500000052154064\n",
      "    episode_reward_mean: 3.500000052154064\n",
      "    episode_reward_min: 3.500000052154064\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.48343266096367\n",
      "      mean_inference_ms: 0.6932084869903193\n",
      "      mean_processing_ms: 4.576284491266247\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.31\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.359874725341797\n",
      "        critic_loss: 0.00143015431240201\n",
      "        max_q: 15.536014556884766\n",
      "        mean_q: 15.333237648010254\n",
      "        min_q: 15.12569808959961\n",
      "        model: {}\n",
      "        td_error: 0.0028603083919733763\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17500\n",
      "    num_steps_trained: 4096000\n",
      "    num_target_updates: 17500\n",
      "    opt_peak_throughput: 48213.421\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.867\n",
      "    sample_time_ms: 73.829\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.2625\n",
      "    ram_util_percent: 71.66875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 75.01525550079754\n",
      "    mean_inference_ms: 0.5951939012586028\n",
      "    mean_processing_ms: 5.271486714057553\n",
      "  time_since_restore: 1784.8352015018463\n",
      "  time_this_iter_s: 9.997201204299927\n",
      "  time_total_s: 1784.8352015018463\n",
      "  timestamp: 1582122173\n",
      "  timesteps_since_restore: 17500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17500\n",
      "  training_iteration: 175\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         1784.84</td><td style=\"text-align: right;\">      17500</td><td style=\"text-align: right;\">   2.306</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 24.05\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.3050000343471764\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 884\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.47451361734139\n",
      "      mean_inference_ms: 0.693279125856463\n",
      "      mean_processing_ms: 4.578938698311506\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.809\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.26083755493164\n",
      "        critic_loss: 0.0019517900655046105\n",
      "        max_q: 15.414946556091309\n",
      "        mean_q: 15.240432739257812\n",
      "        min_q: 15.053088188171387\n",
      "        model: {}\n",
      "        td_error: 0.0039035803638398647\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17600\n",
      "    num_steps_trained: 4121600\n",
      "    num_target_updates: 17600\n",
      "    opt_peak_throughput: 53229.055\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.376\n",
      "    sample_time_ms: 75.248\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.321052631578947\n",
      "    ram_util_percent: 70.77368421052631\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.99928063468506\n",
      "    mean_inference_ms: 0.595206383167908\n",
      "    mean_processing_ms: 5.265708450762431\n",
      "  time_since_restore: 1794.9330112934113\n",
      "  time_this_iter_s: 10.097809791564941\n",
      "  time_total_s: 1794.9330112934113\n",
      "  timestamp: 1582122187\n",
      "  timesteps_since_restore: 17600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17600\n",
      "  training_iteration: 176\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         1794.93</td><td style=\"text-align: right;\">      17600</td><td style=\"text-align: right;\">   2.305</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-23-19\n",
      "  done: false\n",
      "  episode_len_mean: 24.31\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.3310000347346067\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 889\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.47305138124274\n",
      "      mean_inference_ms: 0.6935064121520047\n",
      "      mean_processing_ms: 4.579552860039894\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.095\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.25859546661377\n",
      "        critic_loss: 0.0015809398610144854\n",
      "        max_q: 15.410429954528809\n",
      "        mean_q: 15.238531112670898\n",
      "        min_q: 15.040658950805664\n",
      "        model: {}\n",
      "        td_error: 0.0031618797220289707\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17700\n",
      "    num_steps_trained: 4147200\n",
      "    num_target_updates: 17700\n",
      "    opt_peak_throughput: 50243.173\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.275\n",
      "    sample_time_ms: 84.088\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.205882352941178\n",
      "    ram_util_percent: 70.39411764705882\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.97904049007722\n",
      "    mean_inference_ms: 0.5952311282539\n",
      "    mean_processing_ms: 5.258625255775952\n",
      "  time_since_restore: 1805.136664390564\n",
      "  time_this_iter_s: 10.20365309715271\n",
      "  time_total_s: 1805.136664390564\n",
      "  timestamp: 1582122199\n",
      "  timesteps_since_restore: 17700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17700\n",
      "  training_iteration: 177\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         1805.14</td><td style=\"text-align: right;\">      17700</td><td style=\"text-align: right;\">   2.331</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 24.35\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.3350000347942115\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 893\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.46967497650215\n",
      "      mean_inference_ms: 0.6935751167413509\n",
      "      mean_processing_ms: 4.581953302892272\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.793\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.252680778503418\n",
      "        critic_loss: 0.001737740938551724\n",
      "        max_q: 15.425399780273438\n",
      "        mean_q: 15.228285789489746\n",
      "        min_q: 15.028874397277832\n",
      "        model: {}\n",
      "        td_error: 0.0034754823427647352\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17800\n",
      "    num_steps_trained: 4172800\n",
      "    num_target_updates: 17800\n",
      "    opt_peak_throughput: 53408.831\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.489\n",
      "    sample_time_ms: 75.123\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.052941176470586\n",
      "    ram_util_percent: 70.72352941176472\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.96215971374744\n",
      "    mean_inference_ms: 0.595261185889405\n",
      "    mean_processing_ms: 5.25315570599999\n",
      "  time_since_restore: 1815.233413219452\n",
      "  time_this_iter_s: 10.09674882888794\n",
      "  time_total_s: 1815.233413219452\n",
      "  timestamp: 1582122211\n",
      "  timesteps_since_restore: 17800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17800\n",
      "  training_iteration: 178\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         1815.23</td><td style=\"text-align: right;\">      17800</td><td style=\"text-align: right;\">   2.335</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-23-43\n",
      "  done: false\n",
      "  episode_len_mean: 24.31\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.3310000347346067\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 897\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.46160920681996\n",
      "      mean_inference_ms: 0.6938786232009782\n",
      "      mean_processing_ms: 4.5837471122893225\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.484\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.230204582214355\n",
      "        critic_loss: 0.0013377673458307981\n",
      "        max_q: 15.336005210876465\n",
      "        mean_q: 15.21176815032959\n",
      "        min_q: 14.98097038269043\n",
      "        model: {}\n",
      "        td_error: 0.0026755351573228836\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17900\n",
      "    num_steps_trained: 4198400\n",
      "    num_target_updates: 17900\n",
      "    opt_peak_throughput: 57095.098\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.467\n",
      "    sample_time_ms: 77.526\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.952941176470592\n",
      "    ram_util_percent: 70.92352941176469\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.94482824957316\n",
      "    mean_inference_ms: 0.5953041585443909\n",
      "    mean_processing_ms: 5.247627178595253\n",
      "  time_since_restore: 1825.3272681236267\n",
      "  time_this_iter_s: 10.093854904174805\n",
      "  time_total_s: 1825.3272681236267\n",
      "  timestamp: 1582122223\n",
      "  timesteps_since_restore: 17900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 17900\n",
      "  training_iteration: 179\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         1825.33</td><td style=\"text-align: right;\">      17900</td><td style=\"text-align: right;\">   2.331</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 24.5\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.350000035017729\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 901\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 36.0\n",
      "    episode_reward_max: 3.500000052154064\n",
      "    episode_reward_mean: 3.500000052154064\n",
      "    episode_reward_min: 3.500000052154064\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.51131150478453\n",
      "      mean_inference_ms: 0.6935333055774058\n",
      "      mean_processing_ms: 4.568566604862063\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.35\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.167304039001465\n",
      "        critic_loss: 0.0013022038619965315\n",
      "        max_q: 15.302726745605469\n",
      "        mean_q: 15.14826774597168\n",
      "        min_q: 14.913053512573242\n",
      "        model: {}\n",
      "        td_error: 0.002604407723993063\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 4224000\n",
      "    num_target_updates: 18000\n",
      "    opt_peak_throughput: 58854.84\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.086\n",
      "    sample_time_ms: 88.05\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.45555555555556\n",
      "    ram_util_percent: 69.99444444444444\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.92730278140077\n",
      "    mean_inference_ms: 0.5953493397095235\n",
      "    mean_processing_ms: 5.242048004666113\n",
      "  time_since_restore: 1835.4219031333923\n",
      "  time_this_iter_s: 10.094635009765625\n",
      "  time_total_s: 1835.4219031333923\n",
      "  timestamp: 1582122235\n",
      "  timesteps_since_restore: 18000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 180\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         1835.42</td><td style=\"text-align: right;\">      18000</td><td style=\"text-align: right;\">    2.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-24-09\n",
      "  done: false\n",
      "  episode_len_mean: 24.24\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.3240000346302985\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 906\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 38.0\n",
      "    episode_reward_max: 3.7000000551342964\n",
      "    episode_reward_mean: 3.7000000551342964\n",
      "    episode_reward_min: 3.7000000551342964\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.56710052164749\n",
      "      mean_inference_ms: 0.6932646748148947\n",
      "      mean_processing_ms: 4.551742205905775\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.647\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.17798900604248\n",
      "        critic_loss: 0.0015334575437009335\n",
      "        max_q: 15.380880355834961\n",
      "        mean_q: 15.152135848999023\n",
      "        min_q: 14.915800094604492\n",
      "        model: {}\n",
      "        td_error: 0.003066915087401867\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18100\n",
      "    num_steps_trained: 4249600\n",
      "    num_target_updates: 18100\n",
      "    opt_peak_throughput: 55094.197\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.738\n",
      "    sample_time_ms: 87.552\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.600000000000005\n",
      "    ram_util_percent: 70.19999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.90488831997499\n",
      "    mean_inference_ms: 0.5954217414450154\n",
      "    mean_processing_ms: 5.235466632061906\n",
      "  time_since_restore: 1845.6321110725403\n",
      "  time_this_iter_s: 10.21020793914795\n",
      "  time_total_s: 1845.6321110725403\n",
      "  timestamp: 1582122249\n",
      "  timesteps_since_restore: 18100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18100\n",
      "  training_iteration: 181\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         1845.63</td><td style=\"text-align: right;\">      18100</td><td style=\"text-align: right;\">   2.324</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 23.94\n",
      "  episode_reward_max: 6.100000090897083\n",
      "  episode_reward_mean: 2.294000034183264\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 910\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.54022750363409\n",
      "      mean_inference_ms: 0.6934208924370192\n",
      "      mean_processing_ms: 4.559785033534793\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.951\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.147241592407227\n",
      "        critic_loss: 0.0016965994145721197\n",
      "        max_q: 15.27864933013916\n",
      "        mean_q: 15.128046035766602\n",
      "        min_q: 14.97476863861084\n",
      "        model: {}\n",
      "        td_error: 0.0033931992948055267\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18200\n",
      "    num_steps_trained: 4275200\n",
      "    num_target_updates: 18200\n",
      "    opt_peak_throughput: 51705.479\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.557\n",
      "    sample_time_ms: 84.815\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.389473684210525\n",
      "    ram_util_percent: 70.31578947368422\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.88609383613104\n",
      "    mean_inference_ms: 0.5955086286661484\n",
      "    mean_processing_ms: 5.230507160756536\n",
      "  time_since_restore: 1855.7139403820038\n",
      "  time_this_iter_s: 10.081829309463501\n",
      "  time_total_s: 1855.7139403820038\n",
      "  timestamp: 1582122262\n",
      "  timesteps_since_restore: 18200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18200\n",
      "  training_iteration: 182\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         1855.71</td><td style=\"text-align: right;\">      18200</td><td style=\"text-align: right;\">   2.294</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 23.35\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2350000333040954\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 915\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.51716853145707\n",
      "      mean_inference_ms: 0.6935982389193945\n",
      "      mean_processing_ms: 4.566736784334365\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.085\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.055636405944824\n",
      "        critic_loss: 0.0013772612437605858\n",
      "        max_q: 15.315058708190918\n",
      "        mean_q: 15.033573150634766\n",
      "        min_q: 14.844232559204102\n",
      "        model: {}\n",
      "        td_error: 0.0027545224875211716\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18300\n",
      "    num_steps_trained: 4300800\n",
      "    num_target_updates: 18300\n",
      "    opt_peak_throughput: 50342.346\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.806\n",
      "    sample_time_ms: 73.264\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.38823529411765\n",
      "    ram_util_percent: 70.02941176470587\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.86139665427032\n",
      "    mean_inference_ms: 0.5956652656326693\n",
      "    mean_processing_ms: 5.225261199073891\n",
      "  time_since_restore: 1865.9105474948883\n",
      "  time_this_iter_s: 10.196607112884521\n",
      "  time_total_s: 1865.9105474948883\n",
      "  timestamp: 1582122274\n",
      "  timesteps_since_restore: 18300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18300\n",
      "  training_iteration: 183\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         1865.91</td><td style=\"text-align: right;\">      18300</td><td style=\"text-align: right;\">   2.235</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-24-45\n",
      "  done: false\n",
      "  episode_len_mean: 23.33\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2330000332742928\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 920\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 46.0\n",
      "    episode_reward_max: 4.500000067055225\n",
      "    episode_reward_mean: 4.500000067055225\n",
      "    episode_reward_min: 4.500000067055225\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.59935920835825\n",
      "      mean_inference_ms: 0.6941512715562618\n",
      "      mean_processing_ms: 4.541870109428484\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.301\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.025525093078613\n",
      "        critic_loss: 0.0018594242865219712\n",
      "        max_q: 15.253802299499512\n",
      "        mean_q: 15.005847930908203\n",
      "        min_q: 14.843417167663574\n",
      "        model: {}\n",
      "        td_error: 0.003718848805874586\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18400\n",
      "    num_steps_trained: 4326400\n",
      "    num_target_updates: 18400\n",
      "    opt_peak_throughput: 48289.963\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.487\n",
      "    sample_time_ms: 73.09\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.5375\n",
      "    ram_util_percent: 70.0\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.83588921884675\n",
      "    mean_inference_ms: 0.5958466664405876\n",
      "    mean_processing_ms: 5.220235070626124\n",
      "  time_since_restore: 1876.1083934307098\n",
      "  time_this_iter_s: 10.197845935821533\n",
      "  time_total_s: 1876.1083934307098\n",
      "  timestamp: 1582122285\n",
      "  timesteps_since_restore: 18400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18400\n",
      "  training_iteration: 184\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         1876.11</td><td style=\"text-align: right;\">      18400</td><td style=\"text-align: right;\">   2.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-25-00\n",
      "  done: false\n",
      "  episode_len_mean: 23.35\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2350000333040954\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 925\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.58728702821409\n",
      "      mean_inference_ms: 0.693875985693057\n",
      "      mean_processing_ms: 4.545611617336846\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.586\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.001837730407715\n",
      "        critic_loss: 0.0014619464054703712\n",
      "        max_q: 15.15383243560791\n",
      "        mean_q: 14.989760398864746\n",
      "        min_q: 14.815225601196289\n",
      "        model: {}\n",
      "        td_error: 0.0029238928109407425\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18500\n",
      "    num_steps_trained: 4352000\n",
      "    num_target_updates: 18500\n",
      "    opt_peak_throughput: 55816.49\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.675\n",
      "    sample_time_ms: 86.908\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.047619047619047\n",
      "    ram_util_percent: 69.87619047619047\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.81101577109001\n",
      "    mean_inference_ms: 0.5960120477717511\n",
      "    mean_processing_ms: 5.215464236190508\n",
      "  time_since_restore: 1886.3014783859253\n",
      "  time_this_iter_s: 10.193084955215454\n",
      "  time_total_s: 1886.3014783859253\n",
      "  timestamp: 1582122300\n",
      "  timesteps_since_restore: 18500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18500\n",
      "  training_iteration: 185\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">          1886.3</td><td style=\"text-align: right;\">      18500</td><td style=\"text-align: right;\">   2.235</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 23.33\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2330000332742928\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 929\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 26.0\n",
      "    episode_reward_max: 2.500000037252903\n",
      "    episode_reward_mean: 2.500000037252903\n",
      "    episode_reward_min: 2.500000037252903\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.59943521654967\n",
      "      mean_inference_ms: 0.69341023988796\n",
      "      mean_processing_ms: 4.54181173082554\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.577\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -15.006905555725098\n",
      "        critic_loss: 0.0013244012370705605\n",
      "        max_q: 15.206998825073242\n",
      "        mean_q: 14.98483657836914\n",
      "        min_q: 14.813491821289062\n",
      "        model: {}\n",
      "        td_error: 0.002648802474141121\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18600\n",
      "    num_steps_trained: 4377600\n",
      "    num_target_updates: 18600\n",
      "    opt_peak_throughput: 55929.879\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.449\n",
      "    sample_time_ms: 76.727\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.400000000000002\n",
      "    ram_util_percent: 70.12352941176471\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.79333810953814\n",
      "    mean_inference_ms: 0.5960939625047917\n",
      "    mean_processing_ms: 5.21164712640571\n",
      "  time_since_restore: 1896.3993065357208\n",
      "  time_this_iter_s: 10.097828149795532\n",
      "  time_total_s: 1896.3993065357208\n",
      "  timestamp: 1582122312\n",
      "  timesteps_since_restore: 18600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18600\n",
      "  training_iteration: 186\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">          1896.4</td><td style=\"text-align: right;\">      18600</td><td style=\"text-align: right;\">   2.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-25-24\n",
      "  done: false\n",
      "  episode_len_mean: 23.29\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2290000332146884\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 934\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.62855706623942\n",
      "      mean_inference_ms: 0.6932502347630143\n",
      "      mean_processing_ms: 4.534900566665883\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.514\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.951379776000977\n",
      "        critic_loss: 0.001324131852015853\n",
      "        max_q: 15.12965202331543\n",
      "        mean_q: 14.930755615234375\n",
      "        min_q: 14.753680229187012\n",
      "        model: {}\n",
      "        td_error: 0.002648263704031706\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18700\n",
      "    num_steps_trained: 4403200\n",
      "    num_target_updates: 18700\n",
      "    opt_peak_throughput: 56709.12\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.389\n",
      "    sample_time_ms: 87.503\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.88235294117647\n",
      "    ram_util_percent: 70.45882352941176\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.77460691333151\n",
      "    mean_inference_ms: 0.5961254761659126\n",
      "    mean_processing_ms: 5.207122866787212\n",
      "  time_since_restore: 1906.5915672779083\n",
      "  time_this_iter_s: 10.1922607421875\n",
      "  time_total_s: 1906.5915672779083\n",
      "  timestamp: 1582122324\n",
      "  timesteps_since_restore: 18700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18700\n",
      "  training_iteration: 187\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         1906.59</td><td style=\"text-align: right;\">      18700</td><td style=\"text-align: right;\">   2.229</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 23.38\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.238000033348799\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 938\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.60739645618625\n",
      "      mean_inference_ms: 0.6934908333789097\n",
      "      mean_processing_ms: 4.54165270266015\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.826\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.914651870727539\n",
      "        critic_loss: 0.0013502102810889482\n",
      "        max_q: 15.043132781982422\n",
      "        mean_q: 14.895740509033203\n",
      "        min_q: 14.705968856811523\n",
      "        model: {}\n",
      "        td_error: 0.0027004205621778965\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18800\n",
      "    num_steps_trained: 4428800\n",
      "    num_target_updates: 18800\n",
      "    opt_peak_throughput: 53044.196\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.44\n",
      "    sample_time_ms: 77.787\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.7\n",
      "    ram_util_percent: 70.32631578947368\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.76145496253147\n",
      "    mean_inference_ms: 0.5961200593797489\n",
      "    mean_processing_ms: 5.203404255750176\n",
      "  time_since_restore: 1916.7015614509583\n",
      "  time_this_iter_s: 10.109994173049927\n",
      "  time_total_s: 1916.7015614509583\n",
      "  timestamp: 1582122337\n",
      "  timesteps_since_restore: 18800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18800\n",
      "  training_iteration: 188\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">          1916.7</td><td style=\"text-align: right;\">      18800</td><td style=\"text-align: right;\">   2.238</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 23.22\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.22200003311038\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 943\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.60790664131066\n",
      "      mean_inference_ms: 0.6936127148629345\n",
      "      mean_processing_ms: 4.541145631126253\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.652\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.886935234069824\n",
      "        critic_loss: 0.001507813110947609\n",
      "        max_q: 15.110001564025879\n",
      "        mean_q: 14.873191833496094\n",
      "        min_q: 14.657612800598145\n",
      "        model: {}\n",
      "        td_error: 0.0030156259890645742\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18900\n",
      "    num_steps_trained: 4454400\n",
      "    num_target_updates: 18900\n",
      "    opt_peak_throughput: 55035.742\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.47\n",
      "    sample_time_ms: 88.194\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.393749999999997\n",
      "    ram_util_percent: 69.92500000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.74492250671582\n",
      "    mean_inference_ms: 0.5961138165197621\n",
      "    mean_processing_ms: 5.199139855514038\n",
      "  time_since_restore: 1926.8986084461212\n",
      "  time_this_iter_s: 10.197046995162964\n",
      "  time_total_s: 1926.8986084461212\n",
      "  timestamp: 1582122349\n",
      "  timesteps_since_restore: 18900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 18900\n",
      "  training_iteration: 189\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          1926.9</td><td style=\"text-align: right;\">      18900</td><td style=\"text-align: right;\">   2.222</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 23.08\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2080000329017637\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 948\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.59597546909494\n",
      "      mean_inference_ms: 0.6936082256354278\n",
      "      mean_processing_ms: 4.544693781481775\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.048\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.822532653808594\n",
      "        critic_loss: 0.001617078436538577\n",
      "        max_q: 15.05790901184082\n",
      "        mean_q: 14.805749893188477\n",
      "        min_q: 14.613204956054688\n",
      "        model: {}\n",
      "        td_error: 0.0032341566402465105\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 4480000\n",
      "    num_target_updates: 19000\n",
      "    opt_peak_throughput: 50710.152\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.999\n",
      "    sample_time_ms: 85.273\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.77777777777778\n",
      "    ram_util_percent: 70.19999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.72746797856519\n",
      "    mean_inference_ms: 0.5961356833791678\n",
      "    mean_processing_ms: 5.195442217718024\n",
      "  time_since_restore: 1937.0800795555115\n",
      "  time_this_iter_s: 10.181471109390259\n",
      "  time_total_s: 1937.0800795555115\n",
      "  timestamp: 1582122361\n",
      "  timesteps_since_restore: 19000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 190\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         1937.08</td><td style=\"text-align: right;\">      19000</td><td style=\"text-align: right;\">   2.208</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 22.88\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.1880000326037408\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 952\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.62350749859739\n",
      "      mean_inference_ms: 0.6944220688867075\n",
      "      mean_processing_ms: 4.53653450802363\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.494\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.859957695007324\n",
      "        critic_loss: 0.001603356096893549\n",
      "        max_q: 15.133007049560547\n",
      "        mean_q: 14.845874786376953\n",
      "        min_q: 14.684942245483398\n",
      "        model: {}\n",
      "        td_error: 0.003206712193787098\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19100\n",
      "    num_steps_trained: 4505600\n",
      "    num_target_updates: 19100\n",
      "    opt_peak_throughput: 46593.064\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.682\n",
      "    sample_time_ms: 72.315\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.682352941176468\n",
      "    ram_util_percent: 70.74705882352943\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.7126052447962\n",
      "    mean_inference_ms: 0.5961760358961268\n",
      "    mean_processing_ms: 5.192537461633418\n",
      "  time_since_restore: 1947.1814393997192\n",
      "  time_this_iter_s: 10.101359844207764\n",
      "  time_total_s: 1947.1814393997192\n",
      "  timestamp: 1582122373\n",
      "  timesteps_since_restore: 19100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19100\n",
      "  training_iteration: 191\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         1947.18</td><td style=\"text-align: right;\">      19100</td><td style=\"text-align: right;\">   2.188</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 22.79\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.1790000324696304\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 956\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.64362733594129\n",
      "      mean_inference_ms: 0.6954086715356367\n",
      "      mean_processing_ms: 4.5303476418214785\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.684\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.795125961303711\n",
      "        critic_loss: 0.0014117015525698662\n",
      "        max_q: 14.987200736999512\n",
      "        mean_q: 14.777327537536621\n",
      "        min_q: 14.577263832092285\n",
      "        model: {}\n",
      "        td_error: 0.0028234031051397324\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19200\n",
      "    num_steps_trained: 4531200\n",
      "    num_target_updates: 19200\n",
      "    opt_peak_throughput: 45042.529\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 22.524\n",
      "    sample_time_ms: 71.041\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.538888888888884\n",
      "    ram_util_percent: 71.05000000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.69643991491657\n",
      "    mean_inference_ms: 0.5962560179317639\n",
      "    mean_processing_ms: 5.189490498330595\n",
      "  time_since_restore: 1957.2821803092957\n",
      "  time_this_iter_s: 10.100740909576416\n",
      "  time_total_s: 1957.2821803092957\n",
      "  timestamp: 1582122386\n",
      "  timesteps_since_restore: 19200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19200\n",
      "  training_iteration: 192\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         1957.28</td><td style=\"text-align: right;\">      19200</td><td style=\"text-align: right;\">   2.179</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-26-39\n",
      "  done: false\n",
      "  episode_len_mean: 22.7\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.17000003233552\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 961\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.64457719250649\n",
      "      mean_inference_ms: 0.6955979438106771\n",
      "      mean_processing_ms: 4.529785291156851\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.314\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.7880220413208\n",
      "        critic_loss: 0.0014426710549741983\n",
      "        max_q: 14.984148025512695\n",
      "        mean_q: 14.768157958984375\n",
      "        min_q: 14.576688766479492\n",
      "        model: {}\n",
      "        td_error: 0.0028853421099483967\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19300\n",
      "    num_steps_trained: 4556800\n",
      "    num_target_updates: 19300\n",
      "    opt_peak_throughput: 48170.595\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.738\n",
      "    sample_time_ms: 72.175\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.583333333333332\n",
      "    ram_util_percent: 71.63333333333333\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.67499155138763\n",
      "    mean_inference_ms: 0.5963961763626593\n",
      "    mean_processing_ms: 5.185820107722432\n",
      "  time_since_restore: 1967.4800622463226\n",
      "  time_this_iter_s: 10.197881937026978\n",
      "  time_total_s: 1967.4800622463226\n",
      "  timestamp: 1582122399\n",
      "  timesteps_since_restore: 19300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19300\n",
      "  training_iteration: 193\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         1967.48</td><td style=\"text-align: right;\">      19300</td><td style=\"text-align: right;\">    2.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-26-51\n",
      "  done: false\n",
      "  episode_len_mean: 22.85\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.1850000325590373\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 965\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.63249863057897\n",
      "      mean_inference_ms: 0.6957932550838029\n",
      "      mean_processing_ms: 4.534893655690594\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.732\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.759726524353027\n",
      "        critic_loss: 0.0013580837985500693\n",
      "        max_q: 14.882286071777344\n",
      "        mean_q: 14.741409301757812\n",
      "        min_q: 14.541616439819336\n",
      "        model: {}\n",
      "        td_error: 0.0027161675971001387\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19400\n",
      "    num_steps_trained: 4582400\n",
      "    num_target_updates: 19400\n",
      "    opt_peak_throughput: 54104.507\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.375\n",
      "    sample_time_ms: 86.372\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.938888888888886\n",
      "    ram_util_percent: 71.7888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.6574851274135\n",
      "    mean_inference_ms: 0.5965232641620297\n",
      "    mean_processing_ms: 5.182941048799048\n",
      "  time_since_restore: 1977.5883433818817\n",
      "  time_this_iter_s: 10.108281135559082\n",
      "  time_total_s: 1977.5883433818817\n",
      "  timestamp: 1582122411\n",
      "  timesteps_since_restore: 19400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19400\n",
      "  training_iteration: 194\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         1977.59</td><td style=\"text-align: right;\">      19400</td><td style=\"text-align: right;\">   2.185</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 22.58\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1580000321567057\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 970\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.61705262885741\n",
      "      mean_inference_ms: 0.6961706365396915\n",
      "      mean_processing_ms: 4.539698537104722\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.479\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.72115707397461\n",
      "        critic_loss: 0.001027456484735012\n",
      "        max_q: 14.960165977478027\n",
      "        mean_q: 14.709145545959473\n",
      "        min_q: 14.554644584655762\n",
      "        model: {}\n",
      "        td_error: 0.002054912969470024\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19500\n",
      "    num_steps_trained: 4608000\n",
      "    num_target_updates: 19500\n",
      "    opt_peak_throughput: 57154.969\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.606\n",
      "    sample_time_ms: 86.289\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.676470588235293\n",
      "    ram_util_percent: 71.62352941176471\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.63587560939438\n",
      "    mean_inference_ms: 0.5966849192881968\n",
      "    mean_processing_ms: 5.179684980504868\n",
      "  time_since_restore: 1987.7737395763397\n",
      "  time_this_iter_s: 10.185396194458008\n",
      "  time_total_s: 1987.7737395763397\n",
      "  timestamp: 1582122423\n",
      "  timesteps_since_restore: 19500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19500\n",
      "  training_iteration: 195\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         1987.77</td><td style=\"text-align: right;\">      19500</td><td style=\"text-align: right;\">   2.158</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 22.44\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1440000319480896\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 974\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.59759604874634\n",
      "      mean_inference_ms: 0.6959121218158325\n",
      "      mean_processing_ms: 4.545143282137622\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.335\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.703939437866211\n",
      "        critic_loss: 0.0012892542872577906\n",
      "        max_q: 14.864400863647461\n",
      "        mean_q: 14.690923690795898\n",
      "        min_q: 14.465274810791016\n",
      "        model: {}\n",
      "        td_error: 0.002578508574515581\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19600\n",
      "    num_steps_trained: 4633600\n",
      "    num_target_updates: 19600\n",
      "    opt_peak_throughput: 59055.859\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.891\n",
      "    sample_time_ms: 78.094\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.35\n",
      "    ram_util_percent: 71.90625\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.61927005239292\n",
      "    mean_inference_ms: 0.5968015975428332\n",
      "    mean_processing_ms: 5.17714873699736\n",
      "  time_since_restore: 1997.8674936294556\n",
      "  time_this_iter_s: 10.093754053115845\n",
      "  time_total_s: 1997.8674936294556\n",
      "  timestamp: 1582122435\n",
      "  timesteps_since_restore: 19600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19600\n",
      "  training_iteration: 196\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         1997.87</td><td style=\"text-align: right;\">      19600</td><td style=\"text-align: right;\">   2.144</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 22.31\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1310000317543745\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 979\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.59570059807221\n",
      "      mean_inference_ms: 0.6961001663327404\n",
      "      mean_processing_ms: 4.546943724675118\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.657\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.664087295532227\n",
      "        critic_loss: 0.0010522876400500536\n",
      "        max_q: 14.82834243774414\n",
      "        mean_q: 14.65093994140625\n",
      "        min_q: 14.431437492370605\n",
      "        model: {}\n",
      "        td_error: 0.0021045750472694635\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19700\n",
      "    num_steps_trained: 4659200\n",
      "    num_target_updates: 19700\n",
      "    opt_peak_throughput: 54974.315\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.148\n",
      "    sample_time_ms: 85.543\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.205882352941178\n",
      "    ram_util_percent: 72.11764705882354\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.59905520170462\n",
      "    mean_inference_ms: 0.5969322859271374\n",
      "    mean_processing_ms: 5.174332012526078\n",
      "  time_since_restore: 2008.064311504364\n",
      "  time_this_iter_s: 10.196817874908447\n",
      "  time_total_s: 2008.064311504364\n",
      "  timestamp: 1582122446\n",
      "  timesteps_since_restore: 19700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19700\n",
      "  training_iteration: 197\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         2008.06</td><td style=\"text-align: right;\">      19700</td><td style=\"text-align: right;\">   2.131</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-27-38\n",
      "  done: false\n",
      "  episode_len_mean: 22.38\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1380000318586827\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 982\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.58881989340279\n",
      "      mean_inference_ms: 0.6964508565097158\n",
      "      mean_processing_ms: 4.548734010035163\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.239\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.645116806030273\n",
      "        critic_loss: 0.0012997767189517617\n",
      "        max_q: 14.81064224243164\n",
      "        mean_q: 14.629286766052246\n",
      "        min_q: 14.456432342529297\n",
      "        model: {}\n",
      "        td_error: 0.00259955320507288\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19800\n",
      "    num_steps_trained: 4684800\n",
      "    num_target_updates: 19800\n",
      "    opt_peak_throughput: 48866.419\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.338\n",
      "    sample_time_ms: 72.808\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.099999999999998\n",
      "    ram_util_percent: 72.51764705882353\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.58722347476964\n",
      "    mean_inference_ms: 0.5970096124815825\n",
      "    mean_processing_ms: 5.172656159164542\n",
      "  time_since_restore: 2018.064504623413\n",
      "  time_this_iter_s: 10.000193119049072\n",
      "  time_total_s: 2018.064504623413\n",
      "  timestamp: 1582122458\n",
      "  timesteps_since_restore: 19800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19800\n",
      "  training_iteration: 198\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         2018.06</td><td style=\"text-align: right;\">      19800</td><td style=\"text-align: right;\">   2.138</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 22.48\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1480000320076944\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 986\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 39.0\n",
      "    episode_reward_max: 3.8000000566244125\n",
      "    episode_reward_mean: 3.8000000566244125\n",
      "    episode_reward_min: 3.8000000566244125\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.6411595406607\n",
      "      mean_inference_ms: 0.69692149058152\n",
      "      mean_processing_ms: 4.532555988916834\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.206\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.583457946777344\n",
      "        critic_loss: 0.00107546616345644\n",
      "        max_q: 14.83439826965332\n",
      "        mean_q: 14.567718505859375\n",
      "        min_q: 14.375758171081543\n",
      "        model: {}\n",
      "        td_error: 0.00215093232691288\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19900\n",
      "    num_steps_trained: 4710400\n",
      "    num_target_updates: 19900\n",
      "    opt_peak_throughput: 49176.387\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.218\n",
      "    sample_time_ms: 72.858\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.24705882352941\n",
      "    ram_util_percent: 72.75882352941177\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.57089873845588\n",
      "    mean_inference_ms: 0.5971249552801183\n",
      "    mean_processing_ms: 5.170313858857872\n",
      "  time_since_restore: 2028.1643948554993\n",
      "  time_this_iter_s: 10.099890232086182\n",
      "  time_total_s: 2028.1643948554993\n",
      "  timestamp: 1582122470\n",
      "  timesteps_since_restore: 19900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 19900\n",
      "  training_iteration: 199\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         2028.16</td><td style=\"text-align: right;\">      19900</td><td style=\"text-align: right;\">   2.148</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-28-04\n",
      "  done: false\n",
      "  episode_len_mean: 22.51\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.151000032052398\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 991\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.64260259357013\n",
      "      mean_inference_ms: 0.6967550102716167\n",
      "      mean_processing_ms: 4.532076581933186\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.716\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.568426132202148\n",
      "        critic_loss: 0.00143510103225708\n",
      "        max_q: 14.727924346923828\n",
      "        mean_q: 14.553528785705566\n",
      "        min_q: 14.366372108459473\n",
      "        model: {}\n",
      "        td_error: 0.00287020206451416\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 4736000\n",
      "    num_target_updates: 20000\n",
      "    opt_peak_throughput: 54284.766\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.642\n",
      "    sample_time_ms: 85.783\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.794999999999998\n",
      "    ram_util_percent: 72.61999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.5506263736713\n",
      "    mean_inference_ms: 0.5972713580982101\n",
      "    mean_processing_ms: 5.167423732825302\n",
      "  time_since_restore: 2038.35848903656\n",
      "  time_this_iter_s: 10.194094181060791\n",
      "  time_total_s: 2038.35848903656\n",
      "  timestamp: 1582122484\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 200\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         2038.36</td><td style=\"text-align: right;\">      20000</td><td style=\"text-align: right;\">   2.151</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 22.63\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1630000322312117\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 994\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.66620259133167\n",
      "      mean_inference_ms: 0.6964435444302399\n",
      "      mean_processing_ms: 4.527663955821566\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.475\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.535919189453125\n",
      "        critic_loss: 0.0012957879807800055\n",
      "        max_q: 14.654945373535156\n",
      "        mean_q: 14.524822235107422\n",
      "        min_q: 14.366142272949219\n",
      "        model: {}\n",
      "        td_error: 0.002591575961560011\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20100\n",
      "    num_steps_trained: 4761600\n",
      "    num_target_updates: 20100\n",
      "    opt_peak_throughput: 57206.126\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.694\n",
      "    sample_time_ms: 77.234\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.741176470588236\n",
      "    ram_util_percent: 72.47058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.5386824895436\n",
      "    mean_inference_ms: 0.5973579963405583\n",
      "    mean_processing_ms: 5.165554844111199\n",
      "  time_since_restore: 2048.3531291484833\n",
      "  time_this_iter_s: 9.994640111923218\n",
      "  time_total_s: 2048.3531291484833\n",
      "  timestamp: 1582122496\n",
      "  timesteps_since_restore: 20100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20100\n",
      "  training_iteration: 201\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         2048.35</td><td style=\"text-align: right;\">      20100</td><td style=\"text-align: right;\">   2.163</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 22.84\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.184000032544136\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 998\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 40.0\n",
      "    episode_reward_max: 3.9000000581145287\n",
      "    episode_reward_mean: 3.9000000581145287\n",
      "    episode_reward_min: 3.9000000581145287\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.71800468723546\n",
      "      mean_inference_ms: 0.6964036102064028\n",
      "      mean_processing_ms: 4.511033772288387\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.474\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.501583099365234\n",
      "        critic_loss: 0.00113062490709126\n",
      "        max_q: 14.712638854980469\n",
      "        mean_q: 14.490157127380371\n",
      "        min_q: 14.254111289978027\n",
      "        model: {}\n",
      "        td_error: 0.00226124981418252\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20200\n",
      "    num_steps_trained: 4787200\n",
      "    num_target_updates: 20200\n",
      "    opt_peak_throughput: 57225.639\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.707\n",
      "    sample_time_ms: 77.309\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16111111111111\n",
      "    ram_util_percent: 72.58888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.52294602929739\n",
      "    mean_inference_ms: 0.597471973746047\n",
      "    mean_processing_ms: 5.163076529679138\n",
      "  time_since_restore: 2058.4477484226227\n",
      "  time_this_iter_s: 10.094619274139404\n",
      "  time_total_s: 2058.4477484226227\n",
      "  timestamp: 1582122509\n",
      "  timesteps_since_restore: 20200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20200\n",
      "  training_iteration: 202\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         2058.45</td><td style=\"text-align: right;\">      20200</td><td style=\"text-align: right;\">   2.184</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-28-43\n",
      "  done: false\n",
      "  episode_len_mean: 22.42\n",
      "  episode_reward_max: 4.6000000685453415\n",
      "  episode_reward_mean: 2.1420000319182875\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1003\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.72145931695097\n",
      "      mean_inference_ms: 0.6968935074344758\n",
      "      mean_processing_ms: 4.509651020009031\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.041\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.507261276245117\n",
      "        critic_loss: 0.001363965799100697\n",
      "        max_q: 14.637727737426758\n",
      "        mean_q: 14.493431091308594\n",
      "        min_q: 14.320241928100586\n",
      "        model: {}\n",
      "        td_error: 0.002727931598201394\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20300\n",
      "    num_steps_trained: 4812800\n",
      "    num_target_updates: 20300\n",
      "    opt_peak_throughput: 50786.184\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.838\n",
      "    sample_time_ms: 73.681\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.31\n",
      "    ram_util_percent: 72.93499999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.50310202611354\n",
      "    mean_inference_ms: 0.5976247814288683\n",
      "    mean_processing_ms: 5.160178347508091\n",
      "  time_since_restore: 2068.6473903656006\n",
      "  time_this_iter_s: 10.199641942977905\n",
      "  time_total_s: 2068.6473903656006\n",
      "  timestamp: 1582122523\n",
      "  timesteps_since_restore: 20300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20300\n",
      "  training_iteration: 203\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         2068.65</td><td style=\"text-align: right;\">      20300</td><td style=\"text-align: right;\">   2.142</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 22.68\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.1680000323057174\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1007\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.70647413708149\n",
      "      mean_inference_ms: 0.6971765810746664\n",
      "      mean_processing_ms: 4.514368610087888\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.089\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.450211524963379\n",
      "        critic_loss: 0.0011131258215755224\n",
      "        max_q: 14.6105375289917\n",
      "        mean_q: 14.438218116760254\n",
      "        min_q: 14.222891807556152\n",
      "        model: {}\n",
      "        td_error: 0.002226251643151045\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20400\n",
      "    num_steps_trained: 4838400\n",
      "    num_target_updates: 20400\n",
      "    opt_peak_throughput: 50307.909\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.75\n",
      "    sample_time_ms: 83.53\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.01111111111111\n",
      "    ram_util_percent: 73.20555555555556\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.48661098570251\n",
      "    mean_inference_ms: 0.597764484070132\n",
      "    mean_processing_ms: 5.1576662877910895\n",
      "  time_since_restore: 2078.7462224960327\n",
      "  time_this_iter_s: 10.098832130432129\n",
      "  time_total_s: 2078.7462224960327\n",
      "  timestamp: 1582122536\n",
      "  timesteps_since_restore: 20400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20400\n",
      "  training_iteration: 204\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         2078.75</td><td style=\"text-align: right;\">      20400</td><td style=\"text-align: right;\">   2.168</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-29-07\n",
      "  done: false\n",
      "  episode_len_mean: 22.87\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.1870000325888395\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1010\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.71316974248913\n",
      "      mean_inference_ms: 0.6972834064638896\n",
      "      mean_processing_ms: 4.512371042504789\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.942\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.3958740234375\n",
      "        critic_loss: 0.0015404621371999383\n",
      "        max_q: 14.600367546081543\n",
      "        mean_q: 14.386487007141113\n",
      "        min_q: 14.22050666809082\n",
      "        model: {}\n",
      "        td_error: 0.0030809245072305202\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20500\n",
      "    num_steps_trained: 4864000\n",
      "    num_target_updates: 20500\n",
      "    opt_peak_throughput: 51801.016\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.801\n",
      "    sample_time_ms: 74.522\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.675\n",
      "    ram_util_percent: 73.41874999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.47399432378197\n",
      "    mean_inference_ms: 0.5978751929124804\n",
      "    mean_processing_ms: 5.15562761063208\n",
      "  time_since_restore: 2088.743509531021\n",
      "  time_this_iter_s: 9.997287034988403\n",
      "  time_total_s: 2088.743509531021\n",
      "  timestamp: 1582122547\n",
      "  timesteps_since_restore: 20500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20500\n",
      "  training_iteration: 205\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         2088.74</td><td style=\"text-align: right;\">      20500</td><td style=\"text-align: right;\">   2.187</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-29-20\n",
      "  done: false\n",
      "  episode_len_mean: 22.84\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.184000032544136\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1016\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.71368676631303\n",
      "      mean_inference_ms: 0.6973155986666174\n",
      "      mean_processing_ms: 4.512143751844903\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.25\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.376172065734863\n",
      "        critic_loss: 0.0010884137591347098\n",
      "        max_q: 14.590476036071777\n",
      "        mean_q: 14.358827590942383\n",
      "        min_q: 14.197673797607422\n",
      "        model: {}\n",
      "        td_error: 0.0021768277511000633\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20600\n",
      "    num_steps_trained: 4889600\n",
      "    num_target_updates: 20600\n",
      "    opt_peak_throughput: 48757.91\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.679\n",
      "    sample_time_ms: 72.131\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.605555555555558\n",
      "    ram_util_percent: 71.95\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.44944414469975\n",
      "    mean_inference_ms: 0.598086664959547\n",
      "    mean_processing_ms: 5.151746701787738\n",
      "  time_since_restore: 2099.0423035621643\n",
      "  time_this_iter_s: 10.298794031143188\n",
      "  time_total_s: 2099.0423035621643\n",
      "  timestamp: 1582122560\n",
      "  timesteps_since_restore: 20600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20600\n",
      "  training_iteration: 206\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         2099.04</td><td style=\"text-align: right;\">      20600</td><td style=\"text-align: right;\">   2.184</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 23.08\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2080000329017637\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1020\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.7147470990624\n",
      "      mean_inference_ms: 0.6970559325643823\n",
      "      mean_processing_ms: 4.5117889685227315\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.272\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.386303901672363\n",
      "        critic_loss: 0.0009429875062778592\n",
      "        max_q: 14.570417404174805\n",
      "        mean_q: 14.370820045471191\n",
      "        min_q: 14.199063301086426\n",
      "        model: {}\n",
      "        td_error: 0.0018859748961403966\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20700\n",
      "    num_steps_trained: 4915200\n",
      "    num_target_updates: 20700\n",
      "    opt_peak_throughput: 59930.334\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.487\n",
      "    sample_time_ms: 87.554\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.48235294117647\n",
      "    ram_util_percent: 70.40588235294118\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.4336853155519\n",
      "    mean_inference_ms: 0.5982155488041966\n",
      "    mean_processing_ms: 5.148961543227097\n",
      "  time_since_restore: 2109.1365025043488\n",
      "  time_this_iter_s: 10.094198942184448\n",
      "  time_total_s: 2109.1365025043488\n",
      "  timestamp: 1582122572\n",
      "  timesteps_since_restore: 20700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20700\n",
      "  training_iteration: 207\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         2109.14</td><td style=\"text-align: right;\">      20700</td><td style=\"text-align: right;\">   2.208</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 23.2\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.220000033080578\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1023\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.70662813974194\n",
      "      mean_inference_ms: 0.6968266173905999\n",
      "      mean_processing_ms: 4.514343424438554\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.425\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.335760116577148\n",
      "        critic_loss: 0.001031558378599584\n",
      "        max_q: 14.487516403198242\n",
      "        mean_q: 14.320125579833984\n",
      "        min_q: 14.164214134216309\n",
      "        model: {}\n",
      "        td_error: 0.002063116757199168\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20800\n",
      "    num_steps_trained: 4940800\n",
      "    num_target_updates: 20800\n",
      "    opt_peak_throughput: 57854.963\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.47\n",
      "    sample_time_ms: 77.724\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.376470588235293\n",
      "    ram_util_percent: 70.16470588235293\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.42221203206725\n",
      "    mean_inference_ms: 0.5983055783817843\n",
      "    mean_processing_ms: 5.146590765977506\n",
      "  time_since_restore: 2119.1317417621613\n",
      "  time_this_iter_s: 9.9952392578125\n",
      "  time_total_s: 2119.1317417621613\n",
      "  timestamp: 1582122584\n",
      "  timesteps_since_restore: 20800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20800\n",
      "  training_iteration: 208\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         2119.13</td><td style=\"text-align: right;\">      20800</td><td style=\"text-align: right;\">    2.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 23.34\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.234000033289194\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1028\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.71957664832479\n",
      "      mean_inference_ms: 0.6965852610695278\n",
      "      mean_processing_ms: 4.510566822614722\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.288\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.31838321685791\n",
      "        critic_loss: 0.0010200385004281998\n",
      "        max_q: 14.49654769897461\n",
      "        mean_q: 14.300345420837402\n",
      "        min_q: 14.138803482055664\n",
      "        model: {}\n",
      "        td_error: 0.0020400770008563995\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20900\n",
      "    num_steps_trained: 4966400\n",
      "    num_target_updates: 20900\n",
      "    opt_peak_throughput: 59699.752\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.951\n",
      "    sample_time_ms: 88.174\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.770588235294117\n",
      "    ram_util_percent: 70.24117647058824\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.40314517650664\n",
      "    mean_inference_ms: 0.5984516166784369\n",
      "    mean_processing_ms: 5.142818847101053\n",
      "  time_since_restore: 2129.3248388767242\n",
      "  time_this_iter_s: 10.193097114562988\n",
      "  time_total_s: 2129.3248388767242\n",
      "  timestamp: 1582122596\n",
      "  timesteps_since_restore: 20900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20900\n",
      "  training_iteration: 209\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         2129.32</td><td style=\"text-align: right;\">      20900</td><td style=\"text-align: right;\">   2.234</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 23.44\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.2440000334382058\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1032\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 41.0\n",
      "    episode_reward_max: 4.000000059604645\n",
      "    episode_reward_mean: 4.000000059604645\n",
      "    episode_reward_min: 4.000000059604645\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.7746564755658\n",
      "      mean_inference_ms: 0.6961283141444973\n",
      "      mean_processing_ms: 4.493633165155999\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.609\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.290618896484375\n",
      "        critic_loss: 0.0010024466319009662\n",
      "        max_q: 14.409085273742676\n",
      "        mean_q: 14.278356552124023\n",
      "        min_q: 14.08000659942627\n",
      "        model: {}\n",
      "        td_error: 0.0020048932638019323\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 4992000\n",
      "    num_target_updates: 21000\n",
      "    opt_peak_throughput: 55548.522\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.604\n",
      "    sample_time_ms: 87.069\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.644444444444442\n",
      "    ram_util_percent: 70.13333333333334\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.38795456614058\n",
      "    mean_inference_ms: 0.5985741494164959\n",
      "    mean_processing_ms: 5.139698593250537\n",
      "  time_since_restore: 2139.4196388721466\n",
      "  time_this_iter_s: 10.094799995422363\n",
      "  time_total_s: 2139.4196388721466\n",
      "  timestamp: 1582122609\n",
      "  timesteps_since_restore: 21000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 210\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         2139.42</td><td style=\"text-align: right;\">      21000</td><td style=\"text-align: right;\">   2.244</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 23.47\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.247000033482909\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1036\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.76036871918953\n",
      "      mean_inference_ms: 0.6960765496127007\n",
      "      mean_processing_ms: 4.497993496795434\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.496\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.266372680664062\n",
      "        critic_loss: 0.0009907034691423178\n",
      "        max_q: 14.411537170410156\n",
      "        mean_q: 14.2493257522583\n",
      "        min_q: 14.113473892211914\n",
      "        model: {}\n",
      "        td_error: 0.0019814069382846355\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21100\n",
      "    num_steps_trained: 5017600\n",
      "    num_target_updates: 21100\n",
      "    opt_peak_throughput: 56936.455\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.46\n",
      "    sample_time_ms: 86.542\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.525\n",
      "    ram_util_percent: 70.1\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.37265124184448\n",
      "    mean_inference_ms: 0.5986972029136184\n",
      "    mean_processing_ms: 5.136504439182261\n",
      "  time_since_restore: 2149.5154027938843\n",
      "  time_this_iter_s: 10.095763921737671\n",
      "  time_total_s: 2149.5154027938843\n",
      "  timestamp: 1582122623\n",
      "  timesteps_since_restore: 21100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21100\n",
      "  training_iteration: 211\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         2149.52</td><td style=\"text-align: right;\">      21100</td><td style=\"text-align: right;\">   2.247</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 23.53\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.253000033572316\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1040\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.78169744442671\n",
      "      mean_inference_ms: 0.6960834600986578\n",
      "      mean_processing_ms: 4.491624294183193\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.894\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.214911460876465\n",
      "        critic_loss: 0.0012567569501698017\n",
      "        max_q: 14.37868595123291\n",
      "        mean_q: 14.202945709228516\n",
      "        min_q: 14.015159606933594\n",
      "        model: {}\n",
      "        td_error: 0.0025135139003396034\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21200\n",
      "    num_steps_trained: 5043200\n",
      "    num_target_updates: 21200\n",
      "    opt_peak_throughput: 52309.265\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.229\n",
      "    sample_time_ms: 75.241\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.935294117647057\n",
      "    ram_util_percent: 70.3\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3572144777803\n",
      "    mean_inference_ms: 0.5988303877477381\n",
      "    mean_processing_ms: 5.133244518834359\n",
      "  time_since_restore: 2159.6114659309387\n",
      "  time_this_iter_s: 10.096063137054443\n",
      "  time_total_s: 2159.6114659309387\n",
      "  timestamp: 1582122634\n",
      "  timesteps_since_restore: 21200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21200\n",
      "  training_iteration: 212\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         2159.61</td><td style=\"text-align: right;\">      21200</td><td style=\"text-align: right;\">   2.253</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 23.76\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.276000033915043\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1044\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.79683432753126\n",
      "      mean_inference_ms: 0.6966617626242314\n",
      "      mean_processing_ms: 4.486904566370174\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.981\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.19877815246582\n",
      "        critic_loss: 0.0009192950674332678\n",
      "        max_q: 14.363672256469727\n",
      "        mean_q: 14.185110092163086\n",
      "        min_q: 14.017438888549805\n",
      "        model: {}\n",
      "        td_error: 0.0018385900184512138\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21300\n",
      "    num_steps_trained: 5068800\n",
      "    num_target_updates: 21300\n",
      "    opt_peak_throughput: 42801.238\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 22.649\n",
      "    sample_time_ms: 80.71\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.350000000000005\n",
      "    ram_util_percent: 70.25555555555555\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.34099492842202\n",
      "    mean_inference_ms: 0.5989826786429113\n",
      "    mean_processing_ms: 5.1297617781945135\n",
      "  time_since_restore: 2169.7120428085327\n",
      "  time_this_iter_s: 10.100576877593994\n",
      "  time_total_s: 2169.7120428085327\n",
      "  timestamp: 1582122647\n",
      "  timesteps_since_restore: 21300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21300\n",
      "  training_iteration: 213\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         2169.71</td><td style=\"text-align: right;\">      21300</td><td style=\"text-align: right;\">   2.276</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 23.9\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.290000034123659\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1047\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.77635765540985\n",
      "      mean_inference_ms: 0.6966798812980985\n",
      "      mean_processing_ms: 4.493043635477499\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.152\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.146653175354004\n",
      "        critic_loss: 0.0013958149356767535\n",
      "        max_q: 14.346853256225586\n",
      "        mean_q: 14.133350372314453\n",
      "        min_q: 13.959453582763672\n",
      "        model: {}\n",
      "        td_error: 0.0027916296385228634\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21400\n",
      "    num_steps_trained: 5094400\n",
      "    num_target_updates: 21400\n",
      "    opt_peak_throughput: 49690.715\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 20.226\n",
      "    sample_time_ms: 74.069\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.294444444444444\n",
      "    ram_util_percent: 70.77222222222221\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32822617110611\n",
      "    mean_inference_ms: 0.5991062190942159\n",
      "    mean_processing_ms: 5.126893137232702\n",
      "  time_since_restore: 2179.7106609344482\n",
      "  time_this_iter_s: 9.998618125915527\n",
      "  time_total_s: 2179.7106609344482\n",
      "  timestamp: 1582122660\n",
      "  timesteps_since_restore: 21400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21400\n",
      "  training_iteration: 214\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         2179.71</td><td style=\"text-align: right;\">      21400</td><td style=\"text-align: right;\">    2.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 24.29\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.3290000347048045\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1051\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.79423554751212\n",
      "      mean_inference_ms: 0.6966037719405844\n",
      "      mean_processing_ms: 4.487841863639912\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.999\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.150335311889648\n",
      "        critic_loss: 0.0008878714870661497\n",
      "        max_q: 14.276739120483398\n",
      "        mean_q: 14.136480331420898\n",
      "        min_q: 13.951812744140625\n",
      "        model: {}\n",
      "        td_error: 0.0017757427413016558\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21500\n",
      "    num_steps_trained: 5120000\n",
      "    num_target_updates: 21500\n",
      "    opt_peak_throughput: 51206.397\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.101\n",
      "    sample_time_ms: 85.01\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.81875\n",
      "    ram_util_percent: 71.01875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31136072092505\n",
      "    mean_inference_ms: 0.599268198654363\n",
      "    mean_processing_ms: 5.123005519973366\n",
      "  time_since_restore: 2189.804962873459\n",
      "  time_this_iter_s: 10.09430193901062\n",
      "  time_total_s: 2189.804962873459\n",
      "  timestamp: 1582122671\n",
      "  timesteps_since_restore: 21500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21500\n",
      "  training_iteration: 215\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">          2189.8</td><td style=\"text-align: right;\">      21500</td><td style=\"text-align: right;\">   2.329</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-31-24\n",
      "  done: false\n",
      "  episode_len_mean: 24.32\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.332000034749508\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1055\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.80071077015039\n",
      "      mean_inference_ms: 0.6966529514812114\n",
      "      mean_processing_ms: 4.485842545411727\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.83\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.108774185180664\n",
      "        critic_loss: 0.0007428632816299796\n",
      "        max_q: 14.241263389587402\n",
      "        mean_q: 14.095961570739746\n",
      "        min_q: 13.963349342346191\n",
      "        model: {}\n",
      "        td_error: 0.0014857265632599592\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21600\n",
      "    num_steps_trained: 5145600\n",
      "    num_target_updates: 21600\n",
      "    opt_peak_throughput: 53005.965\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.436\n",
      "    sample_time_ms: 74.914\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.23157894736842\n",
      "    ram_util_percent: 71.0157894736842\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.29532000236921\n",
      "    mean_inference_ms: 0.5994171538885261\n",
      "    mean_processing_ms: 5.119272772693717\n",
      "  time_since_restore: 2199.9011600017548\n",
      "  time_this_iter_s: 10.096197128295898\n",
      "  time_total_s: 2199.9011600017548\n",
      "  timestamp: 1582122684\n",
      "  timesteps_since_restore: 21600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21600\n",
      "  training_iteration: 216\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">          2199.9</td><td style=\"text-align: right;\">      21600</td><td style=\"text-align: right;\">   2.332</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-31-37\n",
      "  done: false\n",
      "  episode_len_mean: 24.34\n",
      "  episode_reward_max: 4.700000070035458\n",
      "  episode_reward_mean: 2.33400003477931\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1059\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.81817610594616\n",
      "      mean_inference_ms: 0.6965498407570565\n",
      "      mean_processing_ms: 4.480453406939836\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.522\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.058845520019531\n",
      "        critic_loss: 0.0011512322816997766\n",
      "        max_q: 14.195635795593262\n",
      "        mean_q: 14.050655364990234\n",
      "        min_q: 13.897257804870605\n",
      "        model: {}\n",
      "        td_error: 0.0023024645633995533\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21700\n",
      "    num_steps_trained: 5171200\n",
      "    num_target_updates: 21700\n",
      "    opt_peak_throughput: 56615.828\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.349\n",
      "    sample_time_ms: 76.61\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.11176470588235\n",
      "    ram_util_percent: 71.14117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.28015904863315\n",
      "    mean_inference_ms: 0.5995447374191479\n",
      "    mean_processing_ms: 5.115614873164373\n",
      "  time_since_restore: 2209.998028278351\n",
      "  time_this_iter_s: 10.09686827659607\n",
      "  time_total_s: 2209.998028278351\n",
      "  timestamp: 1582122697\n",
      "  timesteps_since_restore: 21700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21700\n",
      "  training_iteration: 217\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">            2210</td><td style=\"text-align: right;\">      21700</td><td style=\"text-align: right;\">   2.334</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 24.8\n",
      "  episode_reward_max: 4.90000007301569\n",
      "  episode_reward_mean: 2.3800000354647635\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1062\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.8382336157115\n",
      "      mean_inference_ms: 0.6965606788762613\n",
      "      mean_processing_ms: 4.474480362499461\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.831\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.025927543640137\n",
      "        critic_loss: 0.0013226662995293736\n",
      "        max_q: 14.188591003417969\n",
      "        mean_q: 14.012033462524414\n",
      "        min_q: 13.780591011047363\n",
      "        model: {}\n",
      "        td_error: 0.0026453323662281036\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21800\n",
      "    num_steps_trained: 5196800\n",
      "    num_target_updates: 21800\n",
      "    opt_peak_throughput: 52988.962\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.296\n",
      "    sample_time_ms: 75.141\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.3\n",
      "    ram_util_percent: 71.08888888888887\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26871821711997\n",
      "    mean_inference_ms: 0.5996376430650779\n",
      "    mean_processing_ms: 5.1127032135220425\n",
      "  time_since_restore: 2219.9948632717133\n",
      "  time_this_iter_s: 9.996834993362427\n",
      "  time_total_s: 2219.9948632717133\n",
      "  timestamp: 1582122709\n",
      "  timesteps_since_restore: 21800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21800\n",
      "  training_iteration: 218\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         2219.99</td><td style=\"text-align: right;\">      21800</td><td style=\"text-align: right;\">    2.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 24.73\n",
      "  episode_reward_max: 4.90000007301569\n",
      "  episode_reward_mean: 2.3730000353604557\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1066\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.84932117533792\n",
      "      mean_inference_ms: 0.6967394407098533\n",
      "      mean_processing_ms: 4.4711764845176045\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.693\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.022478103637695\n",
      "        critic_loss: 0.0012589305406436324\n",
      "        max_q: 14.13613224029541\n",
      "        mean_q: 14.007904052734375\n",
      "        min_q: 13.860066413879395\n",
      "        model: {}\n",
      "        td_error: 0.002517860848456621\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21900\n",
      "    num_steps_trained: 5222400\n",
      "    num_target_updates: 21900\n",
      "    opt_peak_throughput: 54545.64\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.425\n",
      "    sample_time_ms: 86.274\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.59444444444445\n",
      "    ram_util_percent: 71.17777777777776\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.25357006273276\n",
      "    mean_inference_ms: 0.5997589327691835\n",
      "    mean_processing_ms: 5.108794318653965\n",
      "  time_since_restore: 2230.0912024974823\n",
      "  time_this_iter_s: 10.096339225769043\n",
      "  time_total_s: 2230.0912024974823\n",
      "  timestamp: 1582122722\n",
      "  timesteps_since_restore: 21900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 21900\n",
      "  training_iteration: 219\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         2230.09</td><td style=\"text-align: right;\">      21900</td><td style=\"text-align: right;\">   2.373</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 24.9\n",
      "  episode_reward_max: 4.90000007301569\n",
      "  episode_reward_mean: 2.390000035613775\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1070\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.85224273714663\n",
      "      mean_inference_ms: 0.6966412172949785\n",
      "      mean_processing_ms: 4.470296152787675\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.101\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -14.016928672790527\n",
      "        critic_loss: 0.0009933297988027334\n",
      "        max_q: 14.121427536010742\n",
      "        mean_q: 14.006462097167969\n",
      "        min_q: 13.846269607543945\n",
      "        model: {}\n",
      "        td_error: 0.001986659597605467\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 5248000\n",
      "    num_target_updates: 22000\n",
      "    opt_peak_throughput: 50183.294\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.771\n",
      "    sample_time_ms: 74.862\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.0\n",
      "    ram_util_percent: 71.28333333333333\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.23828696855763\n",
      "    mean_inference_ms: 0.5998828944525929\n",
      "    mean_processing_ms: 5.104718005157886\n",
      "  time_since_restore: 2240.1857035160065\n",
      "  time_this_iter_s: 10.09450101852417\n",
      "  time_total_s: 2240.1857035160065\n",
      "  timestamp: 1582122735\n",
      "  timesteps_since_restore: 22000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 220\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         2240.19</td><td style=\"text-align: right;\">      22000</td><td style=\"text-align: right;\">    2.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 25.19\n",
      "  episode_reward_max: 4.90000007301569\n",
      "  episode_reward_mean: 2.419000036045909\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1073\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 54.0\n",
      "    episode_reward_max: 5.300000078976154\n",
      "    episode_reward_mean: 5.300000078976154\n",
      "    episode_reward_min: 5.300000078976154\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.93863420076168\n",
      "      mean_inference_ms: 0.6967727047751473\n",
      "      mean_processing_ms: 4.4439717074277825\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.156\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.980415344238281\n",
      "        critic_loss: 0.0011002314276993275\n",
      "        max_q: 14.116315841674805\n",
      "        mean_q: 13.968049049377441\n",
      "        min_q: 13.80038833618164\n",
      "        model: {}\n",
      "        td_error: 0.002200462855398655\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22100\n",
      "    num_steps_trained: 5273600\n",
      "    num_target_updates: 22100\n",
      "    opt_peak_throughput: 49652.801\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.888\n",
      "    sample_time_ms: 74.257\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.616666666666667\n",
      "    ram_util_percent: 70.37777777777778\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.22640770590891\n",
      "    mean_inference_ms: 0.5999892554552732\n",
      "    mean_processing_ms: 5.101535520669727\n",
      "  time_since_restore: 2250.184794664383\n",
      "  time_this_iter_s: 9.999091148376465\n",
      "  time_total_s: 2250.184794664383\n",
      "  timestamp: 1582122747\n",
      "  timesteps_since_restore: 22100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22100\n",
      "  training_iteration: 221\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         2250.18</td><td style=\"text-align: right;\">      22100</td><td style=\"text-align: right;\">   2.419</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 25.32\n",
      "  episode_reward_max: 4.90000007301569\n",
      "  episode_reward_mean: 2.432000036239624\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1077\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 40.0\n",
      "    episode_reward_max: 3.9000000581145287\n",
      "    episode_reward_mean: 3.9000000581145287\n",
      "    episode_reward_min: 3.9000000581145287\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 84.98600589876726\n",
      "      mean_inference_ms: 0.6971698666668914\n",
      "      mean_processing_ms: 4.429786896553651\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.81\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.932022094726562\n",
      "        critic_loss: 0.0010016725864261389\n",
      "        max_q: 14.025537490844727\n",
      "        mean_q: 13.919296264648438\n",
      "        min_q: 13.775753021240234\n",
      "        model: {}\n",
      "        td_error: 0.0020033451728522778\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22200\n",
      "    num_steps_trained: 5299200\n",
      "    num_target_updates: 22200\n",
      "    opt_peak_throughput: 53217.183\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.303\n",
      "    sample_time_ms: 75.189\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.942857142857143\n",
      "    ram_util_percent: 70.51904761904763\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.21024595296036\n",
      "    mean_inference_ms: 0.6001392942551814\n",
      "    mean_processing_ms: 5.097139268452676\n",
      "  time_since_restore: 2260.280951023102\n",
      "  time_this_iter_s: 10.096156358718872\n",
      "  time_total_s: 2260.280951023102\n",
      "  timestamp: 1582122762\n",
      "  timesteps_since_restore: 22200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22200\n",
      "  training_iteration: 222\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         2260.28</td><td style=\"text-align: right;\">      22200</td><td style=\"text-align: right;\">   2.432</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-32-56\n",
      "  done: false\n",
      "  episode_len_mean: 25.8\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.4800000369548796\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1080\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.00656050654234\n",
      "      mean_inference_ms: 0.6967453821097285\n",
      "      mean_processing_ms: 4.423409242600706\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.235\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.915668487548828\n",
      "        critic_loss: 0.0008665011264383793\n",
      "        max_q: 14.074430465698242\n",
      "        mean_q: 13.90204906463623\n",
      "        min_q: 13.685261726379395\n",
      "        model: {}\n",
      "        td_error: 0.0017330022528767586\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22300\n",
      "    num_steps_trained: 5324800\n",
      "    num_target_updates: 22300\n",
      "    opt_peak_throughput: 60450.267\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.534\n",
      "    sample_time_ms: 87.585\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.18\n",
      "    ram_util_percent: 70.45500000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.19819428131794\n",
      "    mean_inference_ms: 0.6002485725986856\n",
      "    mean_processing_ms: 5.0936987395971025\n",
      "  time_since_restore: 2270.274817466736\n",
      "  time_this_iter_s: 9.993866443634033\n",
      "  time_total_s: 2270.274817466736\n",
      "  timestamp: 1582122776\n",
      "  timesteps_since_restore: 22300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22300\n",
      "  training_iteration: 223\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         2270.27</td><td style=\"text-align: right;\">      22300</td><td style=\"text-align: right;\">    2.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 25.61\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.4610000366717575\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1084\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 37.0\n",
      "    episode_reward_max: 3.6000000536441803\n",
      "    episode_reward_mean: 3.6000000536441803\n",
      "    episode_reward_min: 3.6000000536441803\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.04416272115208\n",
      "      mean_inference_ms: 0.6962926644615268\n",
      "      mean_processing_ms: 4.41197098949276\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.174\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.921582221984863\n",
      "        critic_loss: 0.0010171181056648493\n",
      "        max_q: 14.056482315063477\n",
      "        mean_q: 13.907896041870117\n",
      "        min_q: 13.778129577636719\n",
      "        model: {}\n",
      "        td_error: 0.0020342362113296986\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22400\n",
      "    num_steps_trained: 5350400\n",
      "    num_target_updates: 22400\n",
      "    opt_peak_throughput: 61336.347\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.473\n",
      "    sample_time_ms: 87.554\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.816666666666666\n",
      "    ram_util_percent: 70.70555555555556\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.18335663508772\n",
      "    mean_inference_ms: 0.6003693852497449\n",
      "    mean_processing_ms: 5.089306438751296\n",
      "  time_since_restore: 2280.3673284053802\n",
      "  time_this_iter_s: 10.09251093864441\n",
      "  time_total_s: 2280.3673284053802\n",
      "  timestamp: 1582122789\n",
      "  timesteps_since_restore: 22400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22400\n",
      "  training_iteration: 224\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         2280.37</td><td style=\"text-align: right;\">      22400</td><td style=\"text-align: right;\">   2.461</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-33-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.72\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.4720000368356705\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1088\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 12.0\n",
      "    episode_reward_max: 1.1000000163912773\n",
      "    episode_reward_mean: 1.1000000163912773\n",
      "    episode_reward_min: 1.1000000163912773\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.01393449880199\n",
      "      mean_inference_ms: 0.6961269661662287\n",
      "      mean_processing_ms: 4.4212730887363865\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.16\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.893320083618164\n",
      "        critic_loss: 0.0006925016641616821\n",
      "        max_q: 14.0045747756958\n",
      "        mean_q: 13.883302688598633\n",
      "        min_q: 13.72336196899414\n",
      "        model: {}\n",
      "        td_error: 0.001385003444738686\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22500\n",
      "    num_steps_trained: 5376000\n",
      "    num_target_updates: 22500\n",
      "    opt_peak_throughput: 61542.358\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.062\n",
      "    sample_time_ms: 88.201\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.964999999999996\n",
      "    ram_util_percent: 70.89000000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.16955772055385\n",
      "    mean_inference_ms: 0.6004665893196994\n",
      "    mean_processing_ms: 5.084877550447263\n",
      "  time_since_restore: 2290.4601423740387\n",
      "  time_this_iter_s: 10.092813968658447\n",
      "  time_total_s: 2290.4601423740387\n",
      "  timestamp: 1582122803\n",
      "  timesteps_since_restore: 22500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22500\n",
      "  training_iteration: 225\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         2290.46</td><td style=\"text-align: right;\">      22500</td><td style=\"text-align: right;\">   2.472</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-33-34\n",
      "  done: false\n",
      "  episode_len_mean: 25.6\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.4600000366568566\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1092\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.02695734137731\n",
      "      mean_inference_ms: 0.6958440890713694\n",
      "      mean_processing_ms: 4.417235736251441\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.225\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.836069107055664\n",
      "        critic_loss: 0.0010120257502421737\n",
      "        max_q: 13.960240364074707\n",
      "        mean_q: 13.821479797363281\n",
      "        min_q: 13.65829086303711\n",
      "        model: {}\n",
      "        td_error: 0.0020240515004843473\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22600\n",
      "    num_steps_trained: 5401600\n",
      "    num_target_updates: 22600\n",
      "    opt_peak_throughput: 60597.647\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.858\n",
      "    sample_time_ms: 88.425\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.956249999999997\n",
      "    ram_util_percent: 71.11250000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.15622650243735\n",
      "    mean_inference_ms: 0.6005518197049058\n",
      "    mean_processing_ms: 5.080426802429327\n",
      "  time_since_restore: 2300.554055452347\n",
      "  time_this_iter_s: 10.093913078308105\n",
      "  time_total_s: 2300.554055452347\n",
      "  timestamp: 1582122814\n",
      "  timesteps_since_restore: 22600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22600\n",
      "  training_iteration: 226\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         2300.55</td><td style=\"text-align: right;\">      22600</td><td style=\"text-align: right;\">    2.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 25.54\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.4540000365674497\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1096\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.02661818027856\n",
      "      mean_inference_ms: 0.6955676089576428\n",
      "      mean_processing_ms: 4.417171088393325\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.429\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.817253112792969\n",
      "        critic_loss: 0.0011578917037695646\n",
      "        max_q: 13.98995304107666\n",
      "        mean_q: 13.806386947631836\n",
      "        min_q: 13.643291473388672\n",
      "        model: {}\n",
      "        td_error: 0.0023157834075391293\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22700\n",
      "    num_steps_trained: 5427200\n",
      "    num_target_updates: 22700\n",
      "    opt_peak_throughput: 57799.528\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.415\n",
      "    sample_time_ms: 87.553\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.516666666666666\n",
      "    ram_util_percent: 71.10555555555555\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.14332105222144\n",
      "    mean_inference_ms: 0.6006297093934423\n",
      "    mean_processing_ms: 5.076200688329204\n",
      "  time_since_restore: 2310.647845506668\n",
      "  time_this_iter_s: 10.093790054321289\n",
      "  time_total_s: 2310.647845506668\n",
      "  timestamp: 1582122826\n",
      "  timesteps_since_restore: 22700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22700\n",
      "  training_iteration: 227\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         2310.65</td><td style=\"text-align: right;\">      22700</td><td style=\"text-align: right;\">   2.454</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-33-59\n",
      "  done: false\n",
      "  episode_len_mean: 25.41\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.4410000363737345\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1101\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 32.0\n",
      "    episode_reward_max: 3.1000000461935997\n",
      "    episode_reward_mean: 3.1000000461935997\n",
      "    episode_reward_min: 3.1000000461935997\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.05008582318767\n",
      "      mean_inference_ms: 0.6953672076878923\n",
      "      mean_processing_ms: 4.410304424914528\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.665\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.819099426269531\n",
      "        critic_loss: 0.0008601576555520296\n",
      "        max_q: 13.947945594787598\n",
      "        mean_q: 13.81102180480957\n",
      "        min_q: 13.624345779418945\n",
      "        model: {}\n",
      "        td_error: 0.0017203150782734156\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22800\n",
      "    num_steps_trained: 5452800\n",
      "    num_target_updates: 22800\n",
      "    opt_peak_throughput: 54877.101\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.882\n",
      "    sample_time_ms: 86.777\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.647058823529413\n",
      "    ram_util_percent: 71.14117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.12735645468592\n",
      "    mean_inference_ms: 0.6007260147437579\n",
      "    mean_processing_ms: 5.071058363891574\n",
      "  time_since_restore: 2320.841445684433\n",
      "  time_this_iter_s: 10.193600177764893\n",
      "  time_total_s: 2320.841445684433\n",
      "  timestamp: 1582122839\n",
      "  timesteps_since_restore: 22800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22800\n",
      "  training_iteration: 228\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         2320.84</td><td style=\"text-align: right;\">      22800</td><td style=\"text-align: right;\">   2.441</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 25.88\n",
      "  episode_reward_max: 6.800000101327896\n",
      "  episode_reward_mean: 2.488000037074089\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1103\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 51.0\n",
      "    episode_reward_max: 5.000000074505806\n",
      "    episode_reward_mean: 5.000000074505806\n",
      "    episode_reward_min: 5.000000074505806\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.12268922608011\n",
      "      mean_inference_ms: 0.6952762957562146\n",
      "      mean_processing_ms: 4.388250903945269\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.585\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.811141967773438\n",
      "        critic_loss: 0.000816304876934737\n",
      "        max_q: 13.952413558959961\n",
      "        mean_q: 13.797893524169922\n",
      "        min_q: 13.659433364868164\n",
      "        model: {}\n",
      "        td_error: 0.0016326098702847958\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22900\n",
      "    num_steps_trained: 5478400\n",
      "    num_target_updates: 22900\n",
      "    opt_peak_throughput: 55833.615\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.653\n",
      "    sample_time_ms: 75.973\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.788888888888888\n",
      "    ram_util_percent: 71.2388888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.12090734034022\n",
      "    mean_inference_ms: 0.6007635200384397\n",
      "    mean_processing_ms: 5.068758586234426\n",
      "  time_since_restore: 2330.7382204532623\n",
      "  time_this_iter_s: 9.896774768829346\n",
      "  time_total_s: 2330.7382204532623\n",
      "  timestamp: 1582122852\n",
      "  timesteps_since_restore: 22900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 22900\n",
      "  training_iteration: 229\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         2330.74</td><td style=\"text-align: right;\">      22900</td><td style=\"text-align: right;\">   2.488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-34-26\n",
      "  done: false\n",
      "  episode_len_mean: 26.35\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5350000377744437\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1105\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 13.0\n",
      "    episode_reward_max: 1.2000000178813934\n",
      "    episode_reward_mean: 1.2000000178813934\n",
      "    episode_reward_min: 1.2000000178813934\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.09520430815476\n",
      "      mean_inference_ms: 0.6954710982977065\n",
      "      mean_processing_ms: 4.396597117869436\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.155\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.777978897094727\n",
      "        critic_loss: 0.0007844960782676935\n",
      "        max_q: 13.868480682373047\n",
      "        mean_q: 13.765259742736816\n",
      "        min_q: 13.611536026000977\n",
      "        model: {}\n",
      "        td_error: 0.0015689920401200652\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 5504000\n",
      "    num_target_updates: 23000\n",
      "    opt_peak_throughput: 49659.001\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.742\n",
      "    sample_time_ms: 74.639\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.376190476190477\n",
      "    ram_util_percent: 71.25714285714287\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.11471610643181\n",
      "    mean_inference_ms: 0.6007945799856409\n",
      "    mean_processing_ms: 5.066335588666044\n",
      "  time_since_restore: 2340.63875412941\n",
      "  time_this_iter_s: 9.900533676147461\n",
      "  time_total_s: 2340.63875412941\n",
      "  timestamp: 1582122866\n",
      "  timesteps_since_restore: 23000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 230\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         2340.64</td><td style=\"text-align: right;\">      23000</td><td style=\"text-align: right;\">   2.535</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-34-37\n",
      "  done: false\n",
      "  episode_len_mean: 26.16\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5160000374913216\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1109\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 55.0\n",
      "    episode_reward_max: 5.4000000804662704\n",
      "    episode_reward_mean: 5.4000000804662704\n",
      "    episode_reward_min: 5.4000000804662704\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.17656009217762\n",
      "      mean_inference_ms: 0.6956150951479835\n",
      "      mean_processing_ms: 4.371857595085595\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.775\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.734676361083984\n",
      "        critic_loss: 0.0008048119489103556\n",
      "        max_q: 13.84796142578125\n",
      "        mean_q: 13.720125198364258\n",
      "        min_q: 13.546126365661621\n",
      "        model: {}\n",
      "        td_error: 0.0016096238978207111\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23100\n",
      "    num_steps_trained: 5529600\n",
      "    num_target_updates: 23100\n",
      "    opt_peak_throughput: 53607.216\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.145\n",
      "    sample_time_ms: 75.342\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.3\n",
      "    ram_util_percent: 71.5375\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.10228338665398\n",
      "    mean_inference_ms: 0.6008545865476039\n",
      "    mean_processing_ms: 5.061544244038252\n",
      "  time_since_restore: 2350.732684135437\n",
      "  time_this_iter_s: 10.093930006027222\n",
      "  time_total_s: 2350.732684135437\n",
      "  timestamp: 1582122877\n",
      "  timesteps_since_restore: 23100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23100\n",
      "  training_iteration: 231\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         2350.73</td><td style=\"text-align: right;\">      23100</td><td style=\"text-align: right;\">   2.516</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 26.31\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.531000037714839\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1115\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.1729495141263\n",
      "      mean_inference_ms: 0.6954320654601995\n",
      "      mean_processing_ms: 4.37289240412024\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.479\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.695878982543945\n",
      "        critic_loss: 0.0008852962637320161\n",
      "        max_q: 13.838871955871582\n",
      "        mean_q: 13.68026351928711\n",
      "        min_q: 13.532626152038574\n",
      "        model: {}\n",
      "        td_error: 0.0017705922946333885\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23200\n",
      "    num_steps_trained: 5555200\n",
      "    num_target_updates: 23200\n",
      "    opt_peak_throughput: 57159.229\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.281\n",
      "    sample_time_ms: 87.62\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.404545454545456\n",
      "    ram_util_percent: 70.22272727272725\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.08524007142616\n",
      "    mean_inference_ms: 0.6009153649069126\n",
      "    mean_processing_ms: 5.054729108591051\n",
      "  time_since_restore: 2361.027974128723\n",
      "  time_this_iter_s: 10.295289993286133\n",
      "  time_total_s: 2361.027974128723\n",
      "  timestamp: 1582122893\n",
      "  timesteps_since_restore: 23200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23200\n",
      "  training_iteration: 232\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         2361.03</td><td style=\"text-align: right;\">      23200</td><td style=\"text-align: right;\">   2.531</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-35-05\n",
      "  done: false\n",
      "  episode_len_mean: 26.23\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.52300003759563\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1119\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 52.0\n",
      "    episode_reward_max: 5.100000075995922\n",
      "    episode_reward_mean: 5.100000075995922\n",
      "    episode_reward_min: 5.100000075995922\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.24522155701048\n",
      "      mean_inference_ms: 0.6954237756775663\n",
      "      mean_processing_ms: 4.350865833069432\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.408\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.6892671585083\n",
      "        critic_loss: 0.0007693591760471463\n",
      "        max_q: 13.838251113891602\n",
      "        mean_q: 13.678035736083984\n",
      "        min_q: 13.540205955505371\n",
      "        model: {}\n",
      "        td_error: 0.0015387183520942926\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23300\n",
      "    num_steps_trained: 5580800\n",
      "    num_target_updates: 23300\n",
      "    opt_peak_throughput: 58074.944\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.001\n",
      "    sample_time_ms: 87.048\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.999999999999996\n",
      "    ram_util_percent: 70.13333333333334\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.07426798225784\n",
      "    mean_inference_ms: 0.6009475615146408\n",
      "    mean_processing_ms: 5.050149997629606\n",
      "  time_since_restore: 2371.1208560466766\n",
      "  time_this_iter_s: 10.092881917953491\n",
      "  time_total_s: 2371.1208560466766\n",
      "  timestamp: 1582122905\n",
      "  timesteps_since_restore: 23300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23300\n",
      "  training_iteration: 233\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         2371.12</td><td style=\"text-align: right;\">      23300</td><td style=\"text-align: right;\">   2.523</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-35-20\n",
      "  done: false\n",
      "  episode_len_mean: 26.33\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.533000037744641\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1122\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 49.0\n",
      "    episode_reward_max: 4.800000071525574\n",
      "    episode_reward_mean: 4.800000071525574\n",
      "    episode_reward_min: 4.800000071525574\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.30796697514752\n",
      "      mean_inference_ms: 0.6959293103055773\n",
      "      mean_processing_ms: 4.331704665580047\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.061\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.656441688537598\n",
      "        critic_loss: 0.0008175506955012679\n",
      "        max_q: 13.751222610473633\n",
      "        mean_q: 13.641408920288086\n",
      "        min_q: 13.471478462219238\n",
      "        model: {}\n",
      "        td_error: 0.001635101274587214\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23400\n",
      "    num_steps_trained: 5606400\n",
      "    num_target_updates: 23400\n",
      "    opt_peak_throughput: 50582.823\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.146\n",
      "    sample_time_ms: 72.887\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.566666666666666\n",
      "    ram_util_percent: 70.36190476190475\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.06577947635067\n",
      "    mean_inference_ms: 0.6009822121805837\n",
      "    mean_processing_ms: 5.04670315074111\n",
      "  time_since_restore: 2381.120889186859\n",
      "  time_this_iter_s: 10.000033140182495\n",
      "  time_total_s: 2381.120889186859\n",
      "  timestamp: 1582122920\n",
      "  timesteps_since_restore: 23400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23400\n",
      "  training_iteration: 234\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         2381.12</td><td style=\"text-align: right;\">      23400</td><td style=\"text-align: right;\">   2.533</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 26.28\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5280000376701355\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1127\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.29873402448094\n",
      "      mean_inference_ms: 0.6957920224953447\n",
      "      mean_processing_ms: 4.33451684191158\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.451\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.616283416748047\n",
      "        critic_loss: 0.0007942887605167925\n",
      "        max_q: 13.779979705810547\n",
      "        mean_q: 13.605812072753906\n",
      "        min_q: 13.386422157287598\n",
      "        model: {}\n",
      "        td_error: 0.0015885774046182632\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23500\n",
      "    num_steps_trained: 5632000\n",
      "    num_target_updates: 23500\n",
      "    opt_peak_throughput: 57513.154\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.159\n",
      "    sample_time_ms: 86.487\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.91904761904762\n",
      "    ram_util_percent: 70.83809523809525\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.0510827406259\n",
      "    mean_inference_ms: 0.6010574898299835\n",
      "    mean_processing_ms: 5.041133342998215\n",
      "  time_since_restore: 2391.31391787529\n",
      "  time_this_iter_s: 10.193028688430786\n",
      "  time_total_s: 2391.31391787529\n",
      "  timestamp: 1582122935\n",
      "  timesteps_since_restore: 23500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23500\n",
      "  training_iteration: 235\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         2391.31</td><td style=\"text-align: right;\">      23500</td><td style=\"text-align: right;\">   2.528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 26.29\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.529000037685037\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1131\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 38.0\n",
      "    episode_reward_max: 3.7000000551342964\n",
      "    episode_reward_mean: 3.7000000551342964\n",
      "    episode_reward_min: 3.7000000551342964\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.33450407339326\n",
      "      mean_inference_ms: 0.6952144152729224\n",
      "      mean_processing_ms: 4.323723722011485\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.362\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.617767333984375\n",
      "        critic_loss: 0.0007989860023371875\n",
      "        max_q: 13.716167449951172\n",
      "        mean_q: 13.60599136352539\n",
      "        min_q: 13.424600601196289\n",
      "        model: {}\n",
      "        td_error: 0.0015979718882590532\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23600\n",
      "    num_steps_trained: 5657600\n",
      "    num_target_updates: 23600\n",
      "    opt_peak_throughput: 58684.678\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.499\n",
      "    sample_time_ms: 88.684\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.429411764705883\n",
      "    ram_util_percent: 70.97058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.03953820975093\n",
      "    mean_inference_ms: 0.601114864363784\n",
      "    mean_processing_ms: 5.03664741687241\n",
      "  time_since_restore: 2401.4081099033356\n",
      "  time_this_iter_s: 10.094192028045654\n",
      "  time_total_s: 2401.4081099033356\n",
      "  timestamp: 1582122947\n",
      "  timesteps_since_restore: 23600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23600\n",
      "  training_iteration: 236\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         2401.41</td><td style=\"text-align: right;\">      23600</td><td style=\"text-align: right;\">   2.529</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 26.31\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.531000037714839\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1134\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 66.0\n",
      "    episode_reward_max: 6.500000096857548\n",
      "    episode_reward_mean: 6.500000096857548\n",
      "    episode_reward_min: 6.500000096857548\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.43789758625425\n",
      "      mean_inference_ms: 0.694419390855235\n",
      "      mean_processing_ms: 4.292437957539042\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.093\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.596643447875977\n",
      "        critic_loss: 0.0006664326065219939\n",
      "        max_q: 13.699581146240234\n",
      "        mean_q: 13.585417747497559\n",
      "        min_q: 13.42347240447998\n",
      "        model: {}\n",
      "        td_error: 0.0013328652130439878\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23700\n",
      "    num_steps_trained: 5683200\n",
      "    num_target_updates: 23700\n",
      "    opt_peak_throughput: 62551.225\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.346\n",
      "    sample_time_ms: 80.144\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.594736842105263\n",
      "    ram_util_percent: 71.04736842105261\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.03136971000322\n",
      "    mean_inference_ms: 0.601148272125727\n",
      "    mean_processing_ms: 5.033165464648461\n",
      "  time_since_restore: 2411.400992155075\n",
      "  time_this_iter_s: 9.992882251739502\n",
      "  time_total_s: 2411.400992155075\n",
      "  timestamp: 1582122961\n",
      "  timesteps_since_restore: 23700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23700\n",
      "  training_iteration: 237\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">          2411.4</td><td style=\"text-align: right;\">      23700</td><td style=\"text-align: right;\">   2.531</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 26.59\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5590000381320714\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1137\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 36.0\n",
      "    episode_reward_max: 3.500000052154064\n",
      "    episode_reward_mean: 3.500000052154064\n",
      "    episode_reward_min: 3.500000052154064\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.46702876733762\n",
      "      mean_inference_ms: 0.6939807198928732\n",
      "      mean_processing_ms: 4.283677032707212\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.074\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.536436080932617\n",
      "        critic_loss: 0.0008101792773231864\n",
      "        max_q: 13.688176155090332\n",
      "        mean_q: 13.52609634399414\n",
      "        min_q: 13.3751802444458\n",
      "        model: {}\n",
      "        td_error: 0.001620358438231051\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23800\n",
      "    num_steps_trained: 5708800\n",
      "    num_target_updates: 23800\n",
      "    opt_peak_throughput: 62843.002\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.749\n",
      "    sample_time_ms: 80.446\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.400000000000002\n",
      "    ram_util_percent: 71.10434782608692\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.02371693712398\n",
      "    mean_inference_ms: 0.6011701134170652\n",
      "    mean_processing_ms: 5.029543168676242\n",
      "  time_since_restore: 2421.3908171653748\n",
      "  time_this_iter_s: 9.989825010299683\n",
      "  time_total_s: 2421.3908171653748\n",
      "  timestamp: 1582122977\n",
      "  timesteps_since_restore: 23800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23800\n",
      "  training_iteration: 238\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         2421.39</td><td style=\"text-align: right;\">      23800</td><td style=\"text-align: right;\">   2.559</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-36-30\n",
      "  done: false\n",
      "  episode_len_mean: 26.76\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5760000383853914\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1141\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 14.0\n",
      "    episode_reward_max: 1.3000000193715096\n",
      "    episode_reward_mean: 1.3000000193715096\n",
      "    episode_reward_min: 1.3000000193715096\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.44341651327996\n",
      "      mean_inference_ms: 0.693612831014987\n",
      "      mean_processing_ms: 4.2908284463676996\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.143\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.526260375976562\n",
      "        critic_loss: 0.0007214001379907131\n",
      "        max_q: 13.63878059387207\n",
      "        mean_q: 13.517784118652344\n",
      "        min_q: 13.32742691040039\n",
      "        model: {}\n",
      "        td_error: 0.0014428002759814262\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23900\n",
      "    num_steps_trained: 5734400\n",
      "    num_target_updates: 23900\n",
      "    opt_peak_throughput: 61790.267\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.44\n",
      "    sample_time_ms: 80.794\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.014999999999997\n",
      "    ram_util_percent: 71.11999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.01469742678879\n",
      "    mean_inference_ms: 0.601173419830879\n",
      "    mean_processing_ms: 5.024580085274986\n",
      "  time_since_restore: 2431.480185985565\n",
      "  time_this_iter_s: 10.08936882019043\n",
      "  time_total_s: 2431.480185985565\n",
      "  timestamp: 1582122990\n",
      "  timesteps_since_restore: 23900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 23900\n",
      "  training_iteration: 239\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         2431.48</td><td style=\"text-align: right;\">      23900</td><td style=\"text-align: right;\">   2.576</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 26.61\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5610000381618736\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1144\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.43414760824716\n",
      "      mean_inference_ms: 0.6933618037654422\n",
      "      mean_processing_ms: 4.293669046127235\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.13\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.515636444091797\n",
      "        critic_loss: 0.0008723354549147189\n",
      "        max_q: 13.646495819091797\n",
      "        mean_q: 13.506322860717773\n",
      "        min_q: 13.359201431274414\n",
      "        model: {}\n",
      "        td_error: 0.0017446709098294377\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 5760000\n",
      "    num_target_updates: 24000\n",
      "    opt_peak_throughput: 61979.66\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.694\n",
      "    sample_time_ms: 80.858\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.36\n",
      "    ram_util_percent: 71.20000000000002\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.00882460854571\n",
      "    mean_inference_ms: 0.6011525739343738\n",
      "    mean_processing_ms: 5.020762105948441\n",
      "  time_since_restore: 2441.4748129844666\n",
      "  time_this_iter_s: 9.994626998901367\n",
      "  time_total_s: 2441.4748129844666\n",
      "  timestamp: 1582123002\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 240\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         2441.47</td><td style=\"text-align: right;\">      24000</td><td style=\"text-align: right;\">   2.561</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-36-53\n",
      "  done: false\n",
      "  episode_len_mean: 26.9\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.5900000385940074\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1148\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.44875217888465\n",
      "      mean_inference_ms: 0.6930329575441724\n",
      "      mean_processing_ms: 4.28918772962559\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.482889175415039\n",
      "        critic_loss: 0.0006343453424051404\n",
      "        max_q: 13.609277725219727\n",
      "        mean_q: 13.471925735473633\n",
      "        min_q: 13.334574699401855\n",
      "        model: {}\n",
      "        td_error: 0.0012686906848102808\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24100\n",
      "    num_steps_trained: 5785600\n",
      "    num_target_updates: 24100\n",
      "    opt_peak_throughput: 63607.66\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.79\n",
      "    sample_time_ms: 89.609\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.835294117647063\n",
      "    ram_util_percent: 71.23529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.0025884580349\n",
      "    mean_inference_ms: 0.601092261480911\n",
      "    mean_processing_ms: 5.01592803211095\n",
      "  time_since_restore: 2451.5647797584534\n",
      "  time_this_iter_s: 10.089966773986816\n",
      "  time_total_s: 2451.5647797584534\n",
      "  timestamp: 1582123013\n",
      "  timesteps_since_restore: 24100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24100\n",
      "  training_iteration: 241\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         2451.56</td><td style=\"text-align: right;\">      24100</td><td style=\"text-align: right;\">    2.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 26.98\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.598000038713217\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1151\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.45875316374017\n",
      "      mean_inference_ms: 0.6925115693216255\n",
      "      mean_processing_ms: 4.286414881377013\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.012\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.417896270751953\n",
      "        critic_loss: 0.0017314464785158634\n",
      "        max_q: 13.558039665222168\n",
      "        mean_q: 13.405898094177246\n",
      "        min_q: 13.243142127990723\n",
      "        model: {}\n",
      "        td_error: 0.003462892957031727\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24200\n",
      "    num_steps_trained: 5811200\n",
      "    num_target_updates: 24200\n",
      "    opt_peak_throughput: 63807.238\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.633\n",
      "    sample_time_ms: 89.895\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.283333333333335\n",
      "    ram_util_percent: 70.57777777777775\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.99838265131649\n",
      "    mean_inference_ms: 0.6010337784656969\n",
      "    mean_processing_ms: 5.012204612366458\n",
      "  time_since_restore: 2461.5556387901306\n",
      "  time_this_iter_s: 9.990859031677246\n",
      "  time_total_s: 2461.5556387901306\n",
      "  timestamp: 1582123026\n",
      "  timesteps_since_restore: 24200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24200\n",
      "  training_iteration: 242\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         2461.56</td><td style=\"text-align: right;\">      24200</td><td style=\"text-align: right;\">   2.598</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-37-19\n",
      "  done: false\n",
      "  episode_len_mean: 27.17\n",
      "  episode_reward_max: 7.600000113248825\n",
      "  episode_reward_mean: 2.617000038996339\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1155\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 44.0\n",
      "    episode_reward_max: 4.300000064074993\n",
      "    episode_reward_mean: 4.300000064074993\n",
      "    episode_reward_min: 4.300000064074993\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.50583221101972\n",
      "      mean_inference_ms: 0.6920132518585298\n",
      "      mean_processing_ms: 4.272148697584735\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.299\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.44556713104248\n",
      "        critic_loss: 0.0006711997557431459\n",
      "        max_q: 13.616009712219238\n",
      "        mean_q: 13.43532943725586\n",
      "        min_q: 13.289048194885254\n",
      "        model: {}\n",
      "        td_error: 0.0013423995114862919\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24300\n",
      "    num_steps_trained: 5836800\n",
      "    num_target_updates: 24300\n",
      "    opt_peak_throughput: 59554.059\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.964\n",
      "    sample_time_ms: 88.162\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.81578947368421\n",
      "    ram_util_percent: 70.23157894736842\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.99330094698443\n",
      "    mean_inference_ms: 0.6009383602089936\n",
      "    mean_processing_ms: 5.007302214259999\n",
      "  time_since_restore: 2471.649881839752\n",
      "  time_this_iter_s: 10.094243049621582\n",
      "  time_total_s: 2471.649881839752\n",
      "  timestamp: 1582123039\n",
      "  timesteps_since_restore: 24300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24300\n",
      "  training_iteration: 243\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         2471.65</td><td style=\"text-align: right;\">      24300</td><td style=\"text-align: right;\">   2.617</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-37-33\n",
      "  done: false\n",
      "  episode_len_mean: 27.68\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.668000039756298\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1157\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.49701039691665\n",
      "      mean_inference_ms: 0.6921164421115176\n",
      "      mean_processing_ms: 4.274160710784612\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.276\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.444864273071289\n",
      "        critic_loss: 0.0008698968449607491\n",
      "        max_q: 13.565109252929688\n",
      "        mean_q: 13.431358337402344\n",
      "        min_q: 13.276142120361328\n",
      "        model: {}\n",
      "        td_error: 0.0017397938063368201\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24400\n",
      "    num_steps_trained: 5862400\n",
      "    num_target_updates: 24400\n",
      "    opt_peak_throughput: 59870.518\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.125\n",
      "    sample_time_ms: 88.145\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.735000000000003\n",
      "    ram_util_percent: 70.21499999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.99098479001192\n",
      "    mean_inference_ms: 0.6008858419819816\n",
      "    mean_processing_ms: 5.004701321519209\n",
      "  time_since_restore: 2481.543843984604\n",
      "  time_this_iter_s: 9.893962144851685\n",
      "  time_total_s: 2481.543843984604\n",
      "  timestamp: 1582123053\n",
      "  timesteps_since_restore: 24400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24400\n",
      "  training_iteration: 244\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         2481.54</td><td style=\"text-align: right;\">      24400</td><td style=\"text-align: right;\">   2.668</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 27.42\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.642000039368868\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1161\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.5045020429394\n",
      "      mean_inference_ms: 0.6915955201482974\n",
      "      mean_processing_ms: 4.271815295963851\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.047\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.388084411621094\n",
      "        critic_loss: 0.0007084891549311578\n",
      "        max_q: 13.480863571166992\n",
      "        mean_q: 13.376504898071289\n",
      "        min_q: 13.2103853225708\n",
      "        model: {}\n",
      "        td_error: 0.0014169781934469938\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24500\n",
      "    num_steps_trained: 5888000\n",
      "    num_target_updates: 24500\n",
      "    opt_peak_throughput: 63253.56\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.502\n",
      "    sample_time_ms: 89.519\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.3\n",
      "    ram_util_percent: 70.10624999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98685220054466\n",
      "    mean_inference_ms: 0.6007683067348549\n",
      "    mean_processing_ms: 4.99955278186482\n",
      "  time_since_restore: 2491.633819103241\n",
      "  time_this_iter_s: 10.089975118637085\n",
      "  time_total_s: 2491.633819103241\n",
      "  timestamp: 1582123065\n",
      "  timesteps_since_restore: 24500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24500\n",
      "  training_iteration: 245\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         2491.63</td><td style=\"text-align: right;\">      24500</td><td style=\"text-align: right;\">   2.642</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-37-57\n",
      "  done: false\n",
      "  episode_len_mean: 27.57\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.6570000395923854\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1163\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.48843118326707\n",
      "      mean_inference_ms: 0.6913272510350091\n",
      "      mean_processing_ms: 4.276707893828678\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.03\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.349336624145508\n",
      "        critic_loss: 0.0008192101959139109\n",
      "        max_q: 13.458860397338867\n",
      "        mean_q: 13.337343215942383\n",
      "        min_q: 13.192876815795898\n",
      "        model: {}\n",
      "        td_error: 0.0016384202754125\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24600\n",
      "    num_steps_trained: 5913600\n",
      "    num_target_updates: 24600\n",
      "    opt_peak_throughput: 63522.24\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.053\n",
      "    sample_time_ms: 80.487\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.505555555555556\n",
      "    ram_util_percent: 70.19444444444444\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98524329280707\n",
      "    mean_inference_ms: 0.6006995939382105\n",
      "    mean_processing_ms: 4.996876060760083\n",
      "  time_since_restore: 2501.525752067566\n",
      "  time_this_iter_s: 9.891932964324951\n",
      "  time_total_s: 2501.525752067566\n",
      "  timestamp: 1582123077\n",
      "  timesteps_since_restore: 24600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24600\n",
      "  training_iteration: 246\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         2501.53</td><td style=\"text-align: right;\">      24600</td><td style=\"text-align: right;\">   2.657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-38-09\n",
      "  done: false\n",
      "  episode_len_mean: 27.89\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.6890000400692227\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1166\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 65.0\n",
      "    episode_reward_max: 6.400000095367432\n",
      "    episode_reward_mean: 6.400000095367432\n",
      "    episode_reward_min: 6.400000095367432\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.58322356547772\n",
      "      mean_inference_ms: 0.6900819978798133\n",
      "      mean_processing_ms: 4.2481707399411786\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.903\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.333351135253906\n",
      "        critic_loss: 0.0008791933651082218\n",
      "        max_q: 13.429848670959473\n",
      "        mean_q: 13.321416854858398\n",
      "        min_q: 13.135299682617188\n",
      "        model: {}\n",
      "        td_error: 0.0017583868466317654\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24700\n",
      "    num_steps_trained: 5939200\n",
      "    num_target_updates: 24700\n",
      "    opt_peak_throughput: 65583.234\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.824\n",
      "    sample_time_ms: 80.702\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.0375\n",
      "    ram_util_percent: 70.13749999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98342394943407\n",
      "    mean_inference_ms: 0.6005822072957734\n",
      "    mean_processing_ms: 4.992592716164392\n",
      "  time_since_restore: 2511.516176223755\n",
      "  time_this_iter_s: 9.990424156188965\n",
      "  time_total_s: 2511.516176223755\n",
      "  timestamp: 1582123089\n",
      "  timesteps_since_restore: 24700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24700\n",
      "  training_iteration: 247\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         2511.52</td><td style=\"text-align: right;\">      24700</td><td style=\"text-align: right;\">   2.689</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-38-25\n",
      "  done: false\n",
      "  episode_len_mean: 28.23\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.723000040575862\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1169\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 39.0\n",
      "    episode_reward_max: 3.8000000566244125\n",
      "    episode_reward_mean: 3.8000000566244125\n",
      "    episode_reward_min: 3.8000000566244125\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.61689444342237\n",
      "      mean_inference_ms: 0.6896500314579395\n",
      "      mean_processing_ms: 4.237934567460626\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.02\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.315345764160156\n",
      "        critic_loss: 0.0006607307004742324\n",
      "        max_q: 13.403139114379883\n",
      "        mean_q: 13.301599502563477\n",
      "        min_q: 13.124581336975098\n",
      "        model: {}\n",
      "        td_error: 0.0013214614009484649\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24800\n",
      "    num_steps_trained: 5964800\n",
      "    num_target_updates: 24800\n",
      "    opt_peak_throughput: 63679.334\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.952\n",
      "    sample_time_ms: 79.475\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.217391304347824\n",
      "    ram_util_percent: 70.06521739130434\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98212691472513\n",
      "    mean_inference_ms: 0.6004516837534001\n",
      "    mean_processing_ms: 4.988222149989354\n",
      "  time_since_restore: 2521.508369207382\n",
      "  time_this_iter_s: 9.99219298362732\n",
      "  time_total_s: 2521.508369207382\n",
      "  timestamp: 1582123105\n",
      "  timesteps_since_restore: 24800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24800\n",
      "  training_iteration: 248\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         2521.51</td><td style=\"text-align: right;\">      24800</td><td style=\"text-align: right;\">   2.723</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-38-39\n",
      "  done: false\n",
      "  episode_len_mean: 28.29\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.729000040665269\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1172\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.6121453550511\n",
      "      mean_inference_ms: 0.6893578138332581\n",
      "      mean_processing_ms: 4.239430809461057\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.327\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.31193733215332\n",
      "        critic_loss: 0.0007555318879894912\n",
      "        max_q: 13.437209129333496\n",
      "        mean_q: 13.300203323364258\n",
      "        min_q: 13.157790184020996\n",
      "        model: {}\n",
      "        td_error: 0.0015110637759789824\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24900\n",
      "    num_steps_trained: 5990400\n",
      "    num_target_updates: 24900\n",
      "    opt_peak_throughput: 59160.959\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.26\n",
      "    sample_time_ms: 78.902\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.370000000000005\n",
      "    ram_util_percent: 70.3\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98123983156401\n",
      "    mean_inference_ms: 0.6003083196677622\n",
      "    mean_processing_ms: 4.98381597744033\n",
      "  time_since_restore: 2531.501454114914\n",
      "  time_this_iter_s: 9.993084907531738\n",
      "  time_total_s: 2531.501454114914\n",
      "  timestamp: 1582123119\n",
      "  timesteps_since_restore: 24900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 24900\n",
      "  training_iteration: 249\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">          2531.5</td><td style=\"text-align: right;\">      24900</td><td style=\"text-align: right;\">   2.729</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-38-51\n",
      "  done: false\n",
      "  episode_len_mean: 28.39\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.7390000408142807\n",
      "  episode_reward_min: 1.1000000163912773\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1175\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.62319748359018\n",
      "      mean_inference_ms: 0.6888755837756922\n",
      "      mean_processing_ms: 4.23623097222497\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.204\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.273134231567383\n",
      "        critic_loss: 0.0006414346280507743\n",
      "        max_q: 13.390933990478516\n",
      "        mean_q: 13.263784408569336\n",
      "        min_q: 13.123336791992188\n",
      "        model: {}\n",
      "        td_error: 0.0012828693725168705\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 6016000\n",
      "    num_target_updates: 25000\n",
      "    opt_peak_throughput: 60889.738\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.88\n",
      "    sample_time_ms: 79.277\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.72941176470588\n",
      "    ram_util_percent: 70.45294117647059\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98083190907793\n",
      "    mean_inference_ms: 0.6001548021714105\n",
      "    mean_processing_ms: 4.9793851621589225\n",
      "  time_since_restore: 2541.4933161735535\n",
      "  time_this_iter_s: 9.991862058639526\n",
      "  time_total_s: 2541.4933161735535\n",
      "  timestamp: 1582123131\n",
      "  timesteps_since_restore: 25000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 250\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         2541.49</td><td style=\"text-align: right;\">      25000</td><td style=\"text-align: right;\">   2.739</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-39-03\n",
      "  done: false\n",
      "  episode_len_mean: 28.64\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.7640000411868098\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1179\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 32.0\n",
      "    episode_reward_max: 3.1000000461935997\n",
      "    episode_reward_mean: 3.1000000461935997\n",
      "    episode_reward_min: 3.1000000461935997\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.64066556251191\n",
      "      mean_inference_ms: 0.6883834466048838\n",
      "      mean_processing_ms: 4.23103706115882\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.017\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.245073318481445\n",
      "        critic_loss: 0.0006585553055629134\n",
      "        max_q: 13.34349536895752\n",
      "        mean_q: 13.23031997680664\n",
      "        min_q: 13.087172508239746\n",
      "        model: {}\n",
      "        td_error: 0.0013171106111258268\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25100\n",
      "    num_steps_trained: 6041600\n",
      "    num_target_updates: 25100\n",
      "    opt_peak_throughput: 63725.063\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.729\n",
      "    sample_time_ms: 89.659\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.33333333333333\n",
      "    ram_util_percent: 70.49444444444445\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98091869706609\n",
      "    mean_inference_ms: 0.5999367116187279\n",
      "    mean_processing_ms: 4.973575426220025\n",
      "  time_since_restore: 2551.5836231708527\n",
      "  time_this_iter_s: 10.090306997299194\n",
      "  time_total_s: 2551.5836231708527\n",
      "  timestamp: 1582123143\n",
      "  timesteps_since_restore: 25100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25100\n",
      "  training_iteration: 251\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         2551.58</td><td style=\"text-align: right;\">      25100</td><td style=\"text-align: right;\">   2.764</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 28.45\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.7450000409036877\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1182\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 34.0\n",
      "    episode_reward_max: 3.300000049173832\n",
      "    episode_reward_mean: 3.300000049173832\n",
      "    episode_reward_min: 3.300000049173832\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.66246217631412\n",
      "      mean_inference_ms: 0.687775820005294\n",
      "      mean_processing_ms: 4.224549789775333\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.652\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.242128372192383\n",
      "        critic_loss: 0.000582780921831727\n",
      "        max_q: 13.345093727111816\n",
      "        mean_q: 13.230508804321289\n",
      "        min_q: 13.063733100891113\n",
      "        model: {}\n",
      "        td_error: 0.0011655619600787759\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25200\n",
      "    num_steps_trained: 6067200\n",
      "    num_target_updates: 25200\n",
      "    opt_peak_throughput: 70104.518\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.606\n",
      "    sample_time_ms: 81.281\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.5\n",
      "    ram_util_percent: 70.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98143564381289\n",
      "    mean_inference_ms: 0.5997656008474663\n",
      "    mean_processing_ms: 4.969206548438186\n",
      "  time_since_restore: 2561.574072122574\n",
      "  time_this_iter_s: 9.990448951721191\n",
      "  time_total_s: 2561.574072122574\n",
      "  timestamp: 1582123156\n",
      "  timesteps_since_restore: 25200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25200\n",
      "  training_iteration: 252\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         2561.57</td><td style=\"text-align: right;\">      25200</td><td style=\"text-align: right;\">   2.745</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-39-30\n",
      "  done: false\n",
      "  episode_len_mean: 28.52\n",
      "  episode_reward_max: 8.00000011920929\n",
      "  episode_reward_mean: 2.7520000410079954\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1185\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.67739068056869\n",
      "      mean_inference_ms: 0.6873603016474881\n",
      "      mean_processing_ms: 4.219997456507179\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.852\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.223066329956055\n",
      "        critic_loss: 0.0006591783603653312\n",
      "        max_q: 13.320558547973633\n",
      "        mean_q: 13.210211753845215\n",
      "        min_q: 13.063114166259766\n",
      "        model: {}\n",
      "        td_error: 0.0013183567207306623\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25300\n",
      "    num_steps_trained: 6092800\n",
      "    num_target_updates: 25300\n",
      "    opt_peak_throughput: 66451.822\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.972\n",
      "    sample_time_ms: 80.686\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.566666666666666\n",
      "    ram_util_percent: 70.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98230442618924\n",
      "    mean_inference_ms: 0.5995857635242509\n",
      "    mean_processing_ms: 4.964709610900682\n",
      "  time_since_restore: 2571.5649111270905\n",
      "  time_this_iter_s: 9.990839004516602\n",
      "  time_total_s: 2571.5649111270905\n",
      "  timestamp: 1582123170\n",
      "  timesteps_since_restore: 25300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25300\n",
      "  training_iteration: 253\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         2571.56</td><td style=\"text-align: right;\">      25300</td><td style=\"text-align: right;\">   2.752</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 29.01\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.8010000417381526\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1187\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.68745480843455\n",
      "      mean_inference_ms: 0.687254024973334\n",
      "      mean_processing_ms: 4.216987527721319\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.385\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.196466445922852\n",
      "        critic_loss: 0.0006913761608302593\n",
      "        max_q: 13.302412033081055\n",
      "        mean_q: 13.182659149169922\n",
      "        min_q: 13.055547714233398\n",
      "        model: {}\n",
      "        td_error: 0.0013827524380758405\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25400\n",
      "    num_steps_trained: 6118400\n",
      "    num_target_updates: 25400\n",
      "    opt_peak_throughput: 58376.791\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.057\n",
      "    sample_time_ms: 76.994\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.866666666666664\n",
      "    ram_util_percent: 70.71111111111112\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98291477660442\n",
      "    mean_inference_ms: 0.5994647351026121\n",
      "    mean_processing_ms: 4.961553124405038\n",
      "  time_since_restore: 2581.4599492549896\n",
      "  time_this_iter_s: 9.89503812789917\n",
      "  time_total_s: 2581.4599492549896\n",
      "  timestamp: 1582123182\n",
      "  timesteps_since_restore: 25400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25400\n",
      "  training_iteration: 254\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         2581.46</td><td style=\"text-align: right;\">      25400</td><td style=\"text-align: right;\">   2.801</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 28.92\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.7920000416040422\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1192\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 28.0\n",
      "    episode_reward_max: 2.7000000402331352\n",
      "    episode_reward_mean: 2.7000000402331352\n",
      "    episode_reward_min: 2.7000000402331352\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.69571362020797\n",
      "      mean_inference_ms: 0.6869158502998896\n",
      "      mean_processing_ms: 4.214509966430273\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.033\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.15806770324707\n",
      "        critic_loss: 0.0006301241228356957\n",
      "        max_q: 13.307534217834473\n",
      "        mean_q: 13.149333953857422\n",
      "        min_q: 13.017622947692871\n",
      "        model: {}\n",
      "        td_error: 0.0012602483620867133\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25500\n",
      "    num_steps_trained: 6144000\n",
      "    num_target_updates: 25500\n",
      "    opt_peak_throughput: 63479.052\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.658\n",
      "    sample_time_ms: 79.562\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.794736842105266\n",
      "    ram_util_percent: 70.85789473684211\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98486258048295\n",
      "    mean_inference_ms: 0.5991540870350928\n",
      "    mean_processing_ms: 4.953697201886756\n",
      "  time_since_restore: 2591.6500623226166\n",
      "  time_this_iter_s: 10.190113067626953\n",
      "  time_total_s: 2591.6500623226166\n",
      "  timestamp: 1582123195\n",
      "  timesteps_since_restore: 25500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25500\n",
      "  training_iteration: 255\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         2591.65</td><td style=\"text-align: right;\">      25500</td><td style=\"text-align: right;\">   2.792</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 28.75\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.7750000413507223\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1197\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.70135057485192\n",
      "      mean_inference_ms: 0.6866430013412956\n",
      "      mean_processing_ms: 4.212964108963232\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.258\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.140279769897461\n",
      "        critic_loss: 0.000530536868609488\n",
      "        max_q: 13.225692749023438\n",
      "        mean_q: 13.130424499511719\n",
      "        min_q: 13.00912857055664\n",
      "        model: {}\n",
      "        td_error: 0.001061073737218976\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25600\n",
      "    num_steps_trained: 6169600\n",
      "    num_target_updates: 25600\n",
      "    opt_peak_throughput: 60122.953\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.639\n",
      "    sample_time_ms: 78.975\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.927777777777777\n",
      "    ram_util_percent: 70.85000000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.9871485455131\n",
      "    mean_inference_ms: 0.598840658540485\n",
      "    mean_processing_ms: 4.94612331728227\n",
      "  time_since_restore: 2601.846972465515\n",
      "  time_this_iter_s: 10.19691014289856\n",
      "  time_total_s: 2601.846972465515\n",
      "  timestamp: 1582123208\n",
      "  timesteps_since_restore: 25600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25600\n",
      "  training_iteration: 256\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         2601.85</td><td style=\"text-align: right;\">      25600</td><td style=\"text-align: right;\">   2.775</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-40-21\n",
      "  done: false\n",
      "  episode_len_mean: 29.08\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.8080000418424604\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1200\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.69209898945338\n",
      "      mean_inference_ms: 0.6862713746246212\n",
      "      mean_processing_ms: 4.215771677229813\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.107\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.129339218139648\n",
      "        critic_loss: 0.0005652231629937887\n",
      "        max_q: 13.231940269470215\n",
      "        mean_q: 13.119329452514648\n",
      "        min_q: 12.943224906921387\n",
      "        model: {}\n",
      "        td_error: 0.0011304464424028993\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25700\n",
      "    num_steps_trained: 6195200\n",
      "    num_target_updates: 25700\n",
      "    opt_peak_throughput: 62336.607\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.449\n",
      "    sample_time_ms: 79.612\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.672222222222224\n",
      "    ram_util_percent: 70.76666666666668\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.98887561295457\n",
      "    mean_inference_ms: 0.5986461534542022\n",
      "    mean_processing_ms: 4.9413631961862885\n",
      "  time_since_restore: 2611.8367886543274\n",
      "  time_this_iter_s: 9.989816188812256\n",
      "  time_total_s: 2611.8367886543274\n",
      "  timestamp: 1582123221\n",
      "  timesteps_since_restore: 25700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25700\n",
      "  training_iteration: 257\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         2611.84</td><td style=\"text-align: right;\">      25700</td><td style=\"text-align: right;\">   2.808</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 29.29\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.829000042155385\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1203\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 37.0\n",
      "    episode_reward_max: 3.6000000536441803\n",
      "    episode_reward_mean: 3.6000000536441803\n",
      "    episode_reward_min: 3.6000000536441803\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.71930334336568\n",
      "      mean_inference_ms: 0.6859588547776605\n",
      "      mean_processing_ms: 4.207673069051428\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.093\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.118951797485352\n",
      "        critic_loss: 0.0006179737392812967\n",
      "        max_q: 13.246912002563477\n",
      "        mean_q: 13.108999252319336\n",
      "        min_q: 12.979944229125977\n",
      "        model: {}\n",
      "        td_error: 0.0012359475949779153\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25800\n",
      "    num_steps_trained: 6220800\n",
      "    num_target_updates: 25800\n",
      "    opt_peak_throughput: 62542.117\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.532\n",
      "    sample_time_ms: 89.856\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.125\n",
      "    ram_util_percent: 71.11875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.99091343736143\n",
      "    mean_inference_ms: 0.5984439617483175\n",
      "    mean_processing_ms: 4.936657996701942\n",
      "  time_since_restore: 2621.8275117874146\n",
      "  time_this_iter_s: 9.990723133087158\n",
      "  time_total_s: 2621.8275117874146\n",
      "  timestamp: 1582123232\n",
      "  timesteps_since_restore: 25800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25800\n",
      "  training_iteration: 258\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         2621.83</td><td style=\"text-align: right;\">      25800</td><td style=\"text-align: right;\">   2.829</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 28.78\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.778000041395426\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1205\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 53.0\n",
      "    episode_reward_max: 5.200000077486038\n",
      "    episode_reward_mean: 5.200000077486038\n",
      "    episode_reward_min: 5.200000077486038\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.78011257318533\n",
      "      mean_inference_ms: 0.6852912469886584\n",
      "      mean_processing_ms: 4.189363777002993\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.129\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.103924751281738\n",
      "        critic_loss: 0.0007594311609864235\n",
      "        max_q: 13.197460174560547\n",
      "        mean_q: 13.091119766235352\n",
      "        min_q: 12.959924697875977\n",
      "        model: {}\n",
      "        td_error: 0.001518862321972847\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25900\n",
      "    num_steps_trained: 6246400\n",
      "    num_target_updates: 25900\n",
      "    opt_peak_throughput: 61998.627\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.916\n",
      "    sample_time_ms: 79.48\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.726315789473684\n",
      "    ram_util_percent: 71.2578947368421\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.9926822023022\n",
      "    mean_inference_ms: 0.5983035661872951\n",
      "    mean_processing_ms: 4.933635124612333\n",
      "  time_since_restore: 2631.7198808193207\n",
      "  time_this_iter_s: 9.892369031906128\n",
      "  time_total_s: 2631.7198808193207\n",
      "  timestamp: 1582123246\n",
      "  timesteps_since_restore: 25900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 25900\n",
      "  training_iteration: 259\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         2631.72</td><td style=\"text-align: right;\">      25900</td><td style=\"text-align: right;\">   2.778</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 29.17\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.8170000419765713\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1209\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 37.0\n",
      "    episode_reward_max: 3.6000000536441803\n",
      "    episode_reward_mean: 3.6000000536441803\n",
      "    episode_reward_min: 3.6000000536441803\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.80639167518467\n",
      "      mean_inference_ms: 0.6849734977989346\n",
      "      mean_processing_ms: 4.182797294646385\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.819\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.058011054992676\n",
      "        critic_loss: 0.0005664641503244638\n",
      "        max_q: 13.144450187683105\n",
      "        mean_q: 13.04475212097168\n",
      "        min_q: 12.914029121398926\n",
      "        model: {}\n",
      "        td_error: 0.0011329284170642495\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 6272000\n",
      "    num_target_updates: 26000\n",
      "    opt_peak_throughput: 67036.799\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.798\n",
      "    sample_time_ms: 80.865\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.69090909090909\n",
      "    ram_util_percent: 71.20000000000002\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 73.99703462540165\n",
      "    mean_inference_ms: 0.5980044590589327\n",
      "    mean_processing_ms: 4.9276211946516435\n",
      "  time_since_restore: 2641.8108797073364\n",
      "  time_this_iter_s: 10.090998888015747\n",
      "  time_total_s: 2641.8108797073364\n",
      "  timestamp: 1582123261\n",
      "  timesteps_since_restore: 26000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 260\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         2641.81</td><td style=\"text-align: right;\">      26000</td><td style=\"text-align: right;\">   2.817</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 29.25\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.8250000420957804\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1213\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 33.0\n",
      "    episode_reward_max: 3.200000047683716\n",
      "    episode_reward_mean: 3.200000047683716\n",
      "    episode_reward_min: 3.200000047683716\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.82294600118458\n",
      "      mean_inference_ms: 0.6845583299912694\n",
      "      mean_processing_ms: 4.178458708807129\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.063\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.043098449707031\n",
      "        critic_loss: 0.00045824411790817976\n",
      "        max_q: 13.166400909423828\n",
      "        mean_q: 13.03553581237793\n",
      "        min_q: 12.905027389526367\n",
      "        model: {}\n",
      "        td_error: 0.0009164881776086986\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26100\n",
      "    num_steps_trained: 6297600\n",
      "    num_target_updates: 26100\n",
      "    opt_peak_throughput: 63010.06\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.505\n",
      "    sample_time_ms: 90.968\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.78421052631579\n",
      "    ram_util_percent: 71.46315789473682\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.00192047459493\n",
      "    mean_inference_ms: 0.597694335950744\n",
      "    mean_processing_ms: 4.921297092639636\n",
      "  time_since_restore: 2651.9017465114594\n",
      "  time_this_iter_s: 10.090866804122925\n",
      "  time_total_s: 2651.9017465114594\n",
      "  timestamp: 1582123275\n",
      "  timesteps_since_restore: 26100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26100\n",
      "  training_iteration: 261\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">          2651.9</td><td style=\"text-align: right;\">      26100</td><td style=\"text-align: right;\">   2.825</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-41-28\n",
      "  done: false\n",
      "  episode_len_mean: 29.29\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.829000042155385\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1217\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.82675230898302\n",
      "      mean_inference_ms: 0.6844388583851669\n",
      "      mean_processing_ms: 4.176930565050959\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.141\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -13.028167724609375\n",
      "        critic_loss: 0.000599611725192517\n",
      "        max_q: 13.1950044631958\n",
      "        mean_q: 13.018622398376465\n",
      "        min_q: 12.893030166625977\n",
      "        model: {}\n",
      "        td_error: 0.001199223566800356\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26200\n",
      "    num_steps_trained: 6323200\n",
      "    num_target_updates: 26200\n",
      "    opt_peak_throughput: 61822.642\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.522\n",
      "    sample_time_ms: 79.749\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.021052631578947\n",
      "    ram_util_percent: 70.23157894736842\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.00701717870699\n",
      "    mean_inference_ms: 0.5973770417275861\n",
      "    mean_processing_ms: 4.914954566192207\n",
      "  time_since_restore: 2661.992731809616\n",
      "  time_this_iter_s: 10.090985298156738\n",
      "  time_total_s: 2661.992731809616\n",
      "  timestamp: 1582123288\n",
      "  timesteps_since_restore: 26200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26200\n",
      "  training_iteration: 262\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         2661.99</td><td style=\"text-align: right;\">      26200</td><td style=\"text-align: right;\">   2.829</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 29.66\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.866000042706728\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1220\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.82791564285114\n",
      "      mean_inference_ms: 0.6840468551706059\n",
      "      mean_processing_ms: 4.176587809432304\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.139\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.984588623046875\n",
      "        critic_loss: 0.0006473054527305067\n",
      "        max_q: 13.100176811218262\n",
      "        mean_q: 12.976656913757324\n",
      "        min_q: 12.869532585144043\n",
      "        model: {}\n",
      "        td_error: 0.0012946110218763351\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26300\n",
      "    num_steps_trained: 6348800\n",
      "    num_target_updates: 26300\n",
      "    opt_peak_throughput: 61851.131\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.225\n",
      "    sample_time_ms: 78.929\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.6\n",
      "    ram_util_percent: 70.20555555555556\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.01111638149615\n",
      "    mean_inference_ms: 0.597132387202348\n",
      "    mean_processing_ms: 4.910135876393135\n",
      "  time_since_restore: 2671.9839828014374\n",
      "  time_this_iter_s: 9.991250991821289\n",
      "  time_total_s: 2671.9839828014374\n",
      "  timestamp: 1582123300\n",
      "  timesteps_since_restore: 26300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26300\n",
      "  training_iteration: 263\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         2671.98</td><td style=\"text-align: right;\">      26300</td><td style=\"text-align: right;\">   2.866</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 29.93\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.8930000431090592\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1223\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.84154783959528\n",
      "      mean_inference_ms: 0.6835649473152108\n",
      "      mean_processing_ms: 4.172452234963068\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.938\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.960899353027344\n",
      "        critic_loss: 0.0006992947310209274\n",
      "        max_q: 13.050599098205566\n",
      "        mean_q: 12.951131820678711\n",
      "        min_q: 12.802128791809082\n",
      "        model: {}\n",
      "        td_error: 0.0013985895784571767\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26400\n",
      "    num_steps_trained: 6374400\n",
      "    num_target_updates: 26400\n",
      "    opt_peak_throughput: 65014.975\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.116\n",
      "    sample_time_ms: 89.413\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.67647058823529\n",
      "    ram_util_percent: 70.10588235294118\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.0156893659741\n",
      "    mean_inference_ms: 0.5968771207333167\n",
      "    mean_processing_ms: 4.905255966610161\n",
      "  time_since_restore: 2681.975841999054\n",
      "  time_this_iter_s: 9.991859197616577\n",
      "  time_total_s: 2681.975841999054\n",
      "  timestamp: 1582123312\n",
      "  timesteps_since_restore: 26400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26400\n",
      "  training_iteration: 264\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         2681.98</td><td style=\"text-align: right;\">      26400</td><td style=\"text-align: right;\">   2.893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 30.39\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.939000043794513\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1225\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 66.0\n",
      "    episode_reward_max: 6.500000096857548\n",
      "    episode_reward_mean: 6.500000096857548\n",
      "    episode_reward_min: 6.500000096857548\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.9256624291375\n",
      "      mean_inference_ms: 0.6830362828322396\n",
      "      mean_processing_ms: 4.1468966579191875\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.864\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.95242691040039\n",
      "        critic_loss: 0.0005769323906861246\n",
      "        max_q: 13.036704063415527\n",
      "        mean_q: 12.941076278686523\n",
      "        min_q: 12.779582977294922\n",
      "        model: {}\n",
      "        td_error: 0.0011538647813722491\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26500\n",
      "    num_steps_trained: 6400000\n",
      "    num_target_updates: 26500\n",
      "    opt_peak_throughput: 66250.506\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.404\n",
      "    sample_time_ms: 89.897\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.016666666666666\n",
      "    ram_util_percent: 70.26666666666667\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.01902491692819\n",
      "    mean_inference_ms: 0.5967002713077127\n",
      "    mean_processing_ms: 4.9017927413235425\n",
      "  time_since_restore: 2691.8652069568634\n",
      "  time_this_iter_s: 9.889364957809448\n",
      "  time_total_s: 2691.8652069568634\n",
      "  timestamp: 1582123325\n",
      "  timesteps_since_restore: 26500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26500\n",
      "  training_iteration: 265\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         2691.87</td><td style=\"text-align: right;\">      26500</td><td style=\"text-align: right;\">   2.939</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 30.61\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.961000044122338\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1227\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.92238285962273\n",
      "      mean_inference_ms: 0.6825917806380835\n",
      "      mean_processing_ms: 4.147906181139824\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.76\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.939229011535645\n",
      "        critic_loss: 0.0006409448105841875\n",
      "        max_q: 13.011454582214355\n",
      "        mean_q: 12.929161071777344\n",
      "        min_q: 12.816606521606445\n",
      "        model: {}\n",
      "        td_error: 0.0012818895047530532\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26600\n",
      "    num_steps_trained: 6425600\n",
      "    num_target_updates: 26600\n",
      "    opt_peak_throughput: 68077.695\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.833\n",
      "    sample_time_ms: 81.09\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.821739130434782\n",
      "    ram_util_percent: 70.31739130434784\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.02251267899787\n",
      "    mean_inference_ms: 0.596516459106047\n",
      "    mean_processing_ms: 4.898138323807164\n",
      "  time_since_restore: 2701.7572960853577\n",
      "  time_this_iter_s: 9.892089128494263\n",
      "  time_total_s: 2701.7572960853577\n",
      "  timestamp: 1582123342\n",
      "  timesteps_since_restore: 26600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26600\n",
      "  training_iteration: 266\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         2701.76</td><td style=\"text-align: right;\">      26600</td><td style=\"text-align: right;\">   2.961</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 30.83\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.9830000444501636\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1231\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 23.0\n",
      "    episode_reward_max: 2.2000000327825546\n",
      "    episode_reward_mean: 2.2000000327825546\n",
      "    episode_reward_min: 2.2000000327825546\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.91894234067136\n",
      "      mean_inference_ms: 0.6823044118532897\n",
      "      mean_processing_ms: 4.149023185327468\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.945\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.899338722229004\n",
      "        critic_loss: 0.0008595255785621703\n",
      "        max_q: 12.987733840942383\n",
      "        mean_q: 12.891050338745117\n",
      "        min_q: 12.768482208251953\n",
      "        model: {}\n",
      "        td_error: 0.0017190509242936969\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26700\n",
      "    num_steps_trained: 6451200\n",
      "    num_target_updates: 26700\n",
      "    opt_peak_throughput: 64888.462\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.384\n",
      "    sample_time_ms: 78.911\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.266666666666666\n",
      "    ram_util_percent: 70.33888888888889\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.02967015882082\n",
      "    mean_inference_ms: 0.5961438541783959\n",
      "    mean_processing_ms: 4.890868859860672\n",
      "  time_since_restore: 2711.846515893936\n",
      "  time_this_iter_s: 10.089219808578491\n",
      "  time_total_s: 2711.846515893936\n",
      "  timestamp: 1582123354\n",
      "  timesteps_since_restore: 26700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26700\n",
      "  training_iteration: 267\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         2711.85</td><td style=\"text-align: right;\">      26700</td><td style=\"text-align: right;\">   2.983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-42-46\n",
      "  done: false\n",
      "  episode_len_mean: 30.97\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 2.99700004465878\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1234\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 18.0\n",
      "    episode_reward_max: 1.700000025331974\n",
      "    episode_reward_mean: 1.700000025331974\n",
      "    episode_reward_min: 1.700000025331974\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.90561631308059\n",
      "      mean_inference_ms: 0.6820628747037997\n",
      "      mean_processing_ms: 4.15414506537395\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.423\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.912667274475098\n",
      "        critic_loss: 0.0005946137243881822\n",
      "        max_q: 12.994897842407227\n",
      "        mean_q: 12.899981498718262\n",
      "        min_q: 12.763508796691895\n",
      "        model: {}\n",
      "        td_error: 0.0011892274487763643\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26800\n",
      "    num_steps_trained: 6476800\n",
      "    num_target_updates: 26800\n",
      "    opt_peak_throughput: 57880.536\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.227\n",
      "    sample_time_ms: 78.128\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.735294117647058\n",
      "    ram_util_percent: 70.39999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.03504306599359\n",
      "    mean_inference_ms: 0.5958678978409822\n",
      "    mean_processing_ms: 4.8853698549783715\n",
      "  time_since_restore: 2721.8513238430023\n",
      "  time_this_iter_s: 10.004807949066162\n",
      "  time_total_s: 2721.8513238430023\n",
      "  timestamp: 1582123366\n",
      "  timesteps_since_restore: 26800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26800\n",
      "  training_iteration: 268\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         2721.85</td><td style=\"text-align: right;\">      26800</td><td style=\"text-align: right;\">   2.997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 31.05\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.005000044777989\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1237\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.88522082490547\n",
      "      mean_inference_ms: 0.6819609714203383\n",
      "      mean_processing_ms: 4.160147885737075\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.915\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.912606239318848\n",
      "        critic_loss: 0.0010483853984624147\n",
      "        max_q: 13.003064155578613\n",
      "        mean_q: 12.905920028686523\n",
      "        min_q: 12.753838539123535\n",
      "        model: {}\n",
      "        td_error: 0.002096771029755473\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26900\n",
      "    num_steps_trained: 6502400\n",
      "    num_target_updates: 26900\n",
      "    opt_peak_throughput: 65390.723\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.287\n",
      "    sample_time_ms: 80.193\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.2625\n",
      "    ram_util_percent: 70.41875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.04027776814033\n",
      "    mean_inference_ms: 0.5955925701751656\n",
      "    mean_processing_ms: 4.879916677860924\n",
      "  time_since_restore: 2731.8324959278107\n",
      "  time_this_iter_s: 9.98117208480835\n",
      "  time_total_s: 2731.8324959278107\n",
      "  timestamp: 1582123377\n",
      "  timesteps_since_restore: 26900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 26900\n",
      "  training_iteration: 269\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         2731.83</td><td style=\"text-align: right;\">      26900</td><td style=\"text-align: right;\">   3.005</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 31.46\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.046000045388937\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1239\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 64.0\n",
      "    episode_reward_max: 6.3000000938773155\n",
      "    episode_reward_mean: 6.3000000938773155\n",
      "    episode_reward_min: 6.3000000938773155\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.96314857624195\n",
      "      mean_inference_ms: 0.6814319822523329\n",
      "      mean_processing_ms: 4.136425265559444\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.855657577514648\n",
      "        critic_loss: 0.0005585879553109407\n",
      "        max_q: 12.945135116577148\n",
      "        mean_q: 12.84493637084961\n",
      "        min_q: 12.70529842376709\n",
      "        model: {}\n",
      "        td_error: 0.0011171759106218815\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 6528000\n",
      "    num_target_updates: 27000\n",
      "    opt_peak_throughput: 61446.563\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.277\n",
      "    sample_time_ms: 79.94\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.424999999999997\n",
      "    ram_util_percent: 70.4125\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.04355326159917\n",
      "    mean_inference_ms: 0.5954123973933403\n",
      "    mean_processing_ms: 4.876236657952892\n",
      "  time_since_restore: 2741.7229356765747\n",
      "  time_this_iter_s: 9.890439748764038\n",
      "  time_total_s: 2741.7229356765747\n",
      "  timestamp: 1582123389\n",
      "  timesteps_since_restore: 27000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 270\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         2741.72</td><td style=\"text-align: right;\">      27000</td><td style=\"text-align: right;\">   3.046</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-43-25\n",
      "  done: false\n",
      "  episode_len_mean: 31.85\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.0850000459700824\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1242\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 85.9617093758911\n",
      "      mean_inference_ms: 0.6811888055319909\n",
      "      mean_processing_ms: 4.136785420478886\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.833\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.83056640625\n",
      "        critic_loss: 0.0004887868417426944\n",
      "        max_q: 12.921683311462402\n",
      "        mean_q: 12.820432662963867\n",
      "        min_q: 12.689554214477539\n",
      "        model: {}\n",
      "        td_error: 0.0009775736834853888\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27100\n",
      "    num_steps_trained: 6553600\n",
      "    num_target_updates: 27100\n",
      "    opt_peak_throughput: 66782.049\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.764\n",
      "    sample_time_ms: 90.586\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.365217391304352\n",
      "    ram_util_percent: 70.69565217391302\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.04854217631559\n",
      "    mean_inference_ms: 0.5951389341262218\n",
      "    mean_processing_ms: 4.8706206797778755\n",
      "  time_since_restore: 2751.7144017219543\n",
      "  time_this_iter_s: 9.991466045379639\n",
      "  time_total_s: 2751.7144017219543\n",
      "  timestamp: 1582123405\n",
      "  timesteps_since_restore: 27100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27100\n",
      "  training_iteration: 271\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         2751.71</td><td style=\"text-align: right;\">      27100</td><td style=\"text-align: right;\">   3.085</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-43-37\n",
      "  done: false\n",
      "  episode_len_mean: 32.04\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.1040000462532045\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1244\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 50.0\n",
      "    episode_reward_max: 4.90000007301569\n",
      "    episode_reward_mean: 4.90000007301569\n",
      "    episode_reward_min: 4.90000007301569\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.01096464628911\n",
      "      mean_inference_ms: 0.6807048882577232\n",
      "      mean_processing_ms: 4.121967699435504\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.685\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.797981262207031\n",
      "        critic_loss: 0.0007113124011084437\n",
      "        max_q: 12.892411231994629\n",
      "        mean_q: 12.786638259887695\n",
      "        min_q: 12.643839836120605\n",
      "        model: {}\n",
      "        td_error: 0.0014226246858015656\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27200\n",
      "    num_steps_trained: 6579200\n",
      "    num_target_updates: 27200\n",
      "    opt_peak_throughput: 69467.728\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.734\n",
      "    sample_time_ms: 80.99\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.08235294117647\n",
      "    ram_util_percent: 70.64117647058823\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.05191083937456\n",
      "    mean_inference_ms: 0.5949557140132657\n",
      "    mean_processing_ms: 4.866785974585474\n",
      "  time_since_restore: 2761.6031305789948\n",
      "  time_this_iter_s: 9.888728857040405\n",
      "  time_total_s: 2761.6031305789948\n",
      "  timestamp: 1582123417\n",
      "  timesteps_since_restore: 27200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27200\n",
      "  training_iteration: 272\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">          2761.6</td><td style=\"text-align: right;\">      27200</td><td style=\"text-align: right;\">   3.104</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 32.02\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.102000046223402\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1246\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 52.0\n",
      "    episode_reward_max: 5.100000075995922\n",
      "    episode_reward_mean: 5.100000075995922\n",
      "    episode_reward_min: 5.100000075995922\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.06349218716935\n",
      "      mean_inference_ms: 0.6800183312292359\n",
      "      mean_processing_ms: 4.106052813660343\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.048\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.82216739654541\n",
      "        critic_loss: 0.0005504438886418939\n",
      "        max_q: 12.922926902770996\n",
      "        mean_q: 12.814043998718262\n",
      "        min_q: 12.714715957641602\n",
      "        model: {}\n",
      "        td_error: 0.0011008877772837877\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27300\n",
      "    num_steps_trained: 6604800\n",
      "    num_target_updates: 27300\n",
      "    opt_peak_throughput: 63244.245\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.714\n",
      "    sample_time_ms: 80.634\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.923809523809524\n",
      "    ram_util_percent: 70.69047619047618\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.05531143703632\n",
      "    mean_inference_ms: 0.5947754871843044\n",
      "    mean_processing_ms: 4.862754234390417\n",
      "  time_since_restore: 2771.493578672409\n",
      "  time_this_iter_s: 9.890448093414307\n",
      "  time_total_s: 2771.493578672409\n",
      "  timestamp: 1582123431\n",
      "  timesteps_since_restore: 27300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27300\n",
      "  training_iteration: 273\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         2771.49</td><td style=\"text-align: right;\">      27300</td><td style=\"text-align: right;\">   3.102</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 32.46\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.146000046879053\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1250\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 78.0\n",
      "    episode_reward_max: 7.700000114738941\n",
      "    episode_reward_mean: 7.700000114738941\n",
      "    episode_reward_min: 7.700000114738941\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.16462791312314\n",
      "      mean_inference_ms: 0.6789945822390188\n",
      "      mean_processing_ms: 4.075481591980314\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.028\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.794069290161133\n",
      "        critic_loss: 0.0008454539929516613\n",
      "        max_q: 12.885677337646484\n",
      "        mean_q: 12.782964706420898\n",
      "        min_q: 12.661290168762207\n",
      "        model: {}\n",
      "        td_error: 0.0016909079859033227\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27400\n",
      "    num_steps_trained: 6630400\n",
      "    num_target_updates: 27400\n",
      "    opt_peak_throughput: 63555.703\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.407\n",
      "    sample_time_ms: 90.882\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.331818181818182\n",
      "    ram_util_percent: 70.64090909090908\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.06218799379252\n",
      "    mean_inference_ms: 0.5944116474518232\n",
      "    mean_processing_ms: 4.85476353858736\n",
      "  time_since_restore: 2781.5827486515045\n",
      "  time_this_iter_s: 10.089169979095459\n",
      "  time_total_s: 2781.5827486515045\n",
      "  timestamp: 1582123447\n",
      "  timesteps_since_restore: 27400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27400\n",
      "  training_iteration: 274\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         2781.58</td><td style=\"text-align: right;\">      27400</td><td style=\"text-align: right;\">   3.146</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 32.51\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.151000046953559\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1253\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 52.0\n",
      "    episode_reward_max: 5.100000075995922\n",
      "    episode_reward_mean: 5.100000075995922\n",
      "    episode_reward_min: 5.100000075995922\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.21494626440798\n",
      "      mean_inference_ms: 0.6784128183097523\n",
      "      mean_processing_ms: 4.060365083113487\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.325\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.75710391998291\n",
      "        critic_loss: 0.00042277760803699493\n",
      "        max_q: 12.854060173034668\n",
      "        mean_q: 12.748054504394531\n",
      "        min_q: 12.574621200561523\n",
      "        model: {}\n",
      "        td_error: 0.0008455552742816508\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27500\n",
      "    num_steps_trained: 6656000\n",
      "    num_target_updates: 27500\n",
      "    opt_peak_throughput: 59187.7\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.559\n",
      "    sample_time_ms: 90.704\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.464000000000002\n",
      "    ram_util_percent: 70.244\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.06743095244332\n",
      "    mean_inference_ms: 0.5941412171450107\n",
      "    mean_processing_ms: 4.848764266462005\n",
      "  time_since_restore: 2791.575530767441\n",
      "  time_this_iter_s: 9.99278211593628\n",
      "  time_total_s: 2791.575530767441\n",
      "  timestamp: 1582123464\n",
      "  timesteps_since_restore: 27500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27500\n",
      "  training_iteration: 275\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         2791.58</td><td style=\"text-align: right;\">      27500</td><td style=\"text-align: right;\">   3.151</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-44-39\n",
      "  done: false\n",
      "  episode_len_mean: 31.78\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.078000045865774\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1257\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 17.0\n",
      "    episode_reward_max: 1.600000023841858\n",
      "    episode_reward_mean: 1.600000023841858\n",
      "    episode_reward_min: 1.600000023841858\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.19968096415202\n",
      "      mean_inference_ms: 0.6781155340321533\n",
      "      mean_processing_ms: 4.065011312492074\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.284\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.746597290039062\n",
      "        critic_loss: 0.0007180662360042334\n",
      "        max_q: 12.864770889282227\n",
      "        mean_q: 12.733572006225586\n",
      "        min_q: 12.609272003173828\n",
      "        model: {}\n",
      "        td_error: 0.0014361325884237885\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27600\n",
      "    num_steps_trained: 6681600\n",
      "    num_target_updates: 27600\n",
      "    opt_peak_throughput: 59759.226\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.231\n",
      "    sample_time_ms: 79.018\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.14761904761905\n",
      "    ram_util_percent: 70.1285714285714\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.07468028514506\n",
      "    mean_inference_ms: 0.5937752442688956\n",
      "    mean_processing_ms: 4.840948296006853\n",
      "  time_since_restore: 2801.667922973633\n",
      "  time_this_iter_s: 10.092392206192017\n",
      "  time_total_s: 2801.667922973633\n",
      "  timestamp: 1582123479\n",
      "  timesteps_since_restore: 27600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27600\n",
      "  training_iteration: 276\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         2801.67</td><td style=\"text-align: right;\">      27600</td><td style=\"text-align: right;\">   3.078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-44-51\n",
      "  done: false\n",
      "  episode_len_mean: 32.27\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.127000046595931\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1260\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.1881935809763\n",
      "      mean_inference_ms: 0.6778007947189908\n",
      "      mean_processing_ms: 4.068515917619024\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.331\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.729645729064941\n",
      "        critic_loss: 0.00044810015242546797\n",
      "        max_q: 12.829887390136719\n",
      "        mean_q: 12.720407485961914\n",
      "        min_q: 12.580327033996582\n",
      "        model: {}\n",
      "        td_error: 0.0008962003048509359\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27700\n",
      "    num_steps_trained: 6707200\n",
      "    num_target_updates: 27700\n",
      "    opt_peak_throughput: 59114.057\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.538\n",
      "    sample_time_ms: 89.827\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.6\n",
      "    ram_util_percent: 70.1\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.08021692192136\n",
      "    mean_inference_ms: 0.5935015872783944\n",
      "    mean_processing_ms: 4.83516939075034\n",
      "  time_since_restore: 2811.658070087433\n",
      "  time_this_iter_s: 9.990147113800049\n",
      "  time_total_s: 2811.658070087433\n",
      "  timestamp: 1582123491\n",
      "  timesteps_since_restore: 27700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27700\n",
      "  training_iteration: 277\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         2811.66</td><td style=\"text-align: right;\">      27700</td><td style=\"text-align: right;\">   3.127</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-45-02\n",
      "  done: false\n",
      "  episode_len_mean: 32.6\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.1600000470876695\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1262\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 32.0\n",
      "    episode_reward_max: 3.1000000461935997\n",
      "    episode_reward_mean: 3.1000000461935997\n",
      "    episode_reward_min: 3.1000000461935997\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.20091515272257\n",
      "      mean_inference_ms: 0.6775439779806313\n",
      "      mean_processing_ms: 4.064613086947419\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.076\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.708234786987305\n",
      "        critic_loss: 0.0006354774814099073\n",
      "        max_q: 12.855262756347656\n",
      "        mean_q: 12.69795036315918\n",
      "        min_q: 12.514139175415039\n",
      "        model: {}\n",
      "        td_error: 0.0012709549628198147\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27800\n",
      "    num_steps_trained: 6732800\n",
      "    num_target_updates: 27800\n",
      "    opt_peak_throughput: 62809.918\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.751\n",
      "    sample_time_ms: 79.805\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.752941176470586\n",
      "    ram_util_percent: 70.10588235294118\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.08399031220348\n",
      "    mean_inference_ms: 0.5933162488538097\n",
      "    mean_processing_ms: 4.831247273979256\n",
      "  time_since_restore: 2821.5506381988525\n",
      "  time_this_iter_s: 9.892568111419678\n",
      "  time_total_s: 2821.5506381988525\n",
      "  timestamp: 1582123502\n",
      "  timesteps_since_restore: 27800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27800\n",
      "  training_iteration: 278\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         2821.55</td><td style=\"text-align: right;\">      27800</td><td style=\"text-align: right;\">    3.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 32.46\n",
      "  episode_reward_max: 8.200000122189522\n",
      "  episode_reward_mean: 3.146000046879053\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1264\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.20997853912749\n",
      "      mean_inference_ms: 0.6771396073672149\n",
      "      mean_processing_ms: 4.061777748771616\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.333\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.69727897644043\n",
      "        critic_loss: 0.00048222061013802886\n",
      "        max_q: 12.84626293182373\n",
      "        mean_q: 12.689624786376953\n",
      "        min_q: 12.526514053344727\n",
      "        model: {}\n",
      "        td_error: 0.0009644412202760577\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27900\n",
      "    num_steps_trained: 6758400\n",
      "    num_target_updates: 27900\n",
      "    opt_peak_throughput: 59081.53\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.582\n",
      "    sample_time_ms: 78.153\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.305555555555554\n",
      "    ram_util_percent: 70.25555555555555\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.08777161628106\n",
      "    mean_inference_ms: 0.5931319194606123\n",
      "    mean_processing_ms: 4.82730095171458\n",
      "  time_since_restore: 2831.446613073349\n",
      "  time_this_iter_s: 9.89597487449646\n",
      "  time_total_s: 2831.446613073349\n",
      "  timestamp: 1582123515\n",
      "  timesteps_since_restore: 27900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 27900\n",
      "  training_iteration: 279\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         2831.45</td><td style=\"text-align: right;\">      27900</td><td style=\"text-align: right;\">   3.146</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 32.88\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.188000047504902\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1267\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 14.0\n",
      "    episode_reward_max: 1.3000000193715096\n",
      "    episode_reward_mean: 1.3000000193715096\n",
      "    episode_reward_min: 1.3000000193715096\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.18938319969124\n",
      "      mean_inference_ms: 0.6770391951922989\n",
      "      mean_processing_ms: 4.068872204453934\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.21\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.666156768798828\n",
      "        critic_loss: 0.0005099308909848332\n",
      "        max_q: 12.783607482910156\n",
      "        mean_q: 12.656817436218262\n",
      "        min_q: 12.531859397888184\n",
      "        model: {}\n",
      "        td_error: 0.0010198617819696665\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 6784000\n",
      "    num_target_updates: 28000\n",
      "    opt_peak_throughput: 60801.127\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.66\n",
      "    sample_time_ms: 89.365\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.533333333333335\n",
      "    ram_util_percent: 70.40555555555554\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.09339379783141\n",
      "    mean_inference_ms: 0.5928589118947114\n",
      "    mean_processing_ms: 4.821444317118336\n",
      "  time_since_restore: 2841.4350838661194\n",
      "  time_this_iter_s: 9.988470792770386\n",
      "  time_total_s: 2841.4350838661194\n",
      "  timestamp: 1582123528\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 280\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         2841.44</td><td style=\"text-align: right;\">      28000</td><td style=\"text-align: right;\">   3.188</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-45-39\n",
      "  done: false\n",
      "  episode_len_mean: 32.37\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.1370000467449426\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1271\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.16998331686833\n",
      "      mean_inference_ms: 0.6769559467956894\n",
      "      mean_processing_ms: 4.07447635116374\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.087\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.647408485412598\n",
      "        critic_loss: 0.0004860718618147075\n",
      "        max_q: 12.754322052001953\n",
      "        mean_q: 12.638391494750977\n",
      "        min_q: 12.513997077941895\n",
      "        model: {}\n",
      "        td_error: 0.0009721438400447369\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28100\n",
      "    num_steps_trained: 6809600\n",
      "    num_target_updates: 28100\n",
      "    opt_peak_throughput: 62638.072\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.362\n",
      "    sample_time_ms: 91.041\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.2\n",
      "    ram_util_percent: 70.50625\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.10099491398252\n",
      "    mean_inference_ms: 0.5924976503809455\n",
      "    mean_processing_ms: 4.813875658647055\n",
      "  time_since_restore: 2851.5266819000244\n",
      "  time_this_iter_s: 10.09159803390503\n",
      "  time_total_s: 2851.5266819000244\n",
      "  timestamp: 1582123539\n",
      "  timesteps_since_restore: 28100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28100\n",
      "  training_iteration: 281\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         2851.53</td><td style=\"text-align: right;\">      28100</td><td style=\"text-align: right;\">   3.137</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 32.62\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.1620000471174716\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1273\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 24.0\n",
      "    episode_reward_max: 2.3000000342726707\n",
      "    episode_reward_mean: 2.3000000342726707\n",
      "    episode_reward_min: 2.3000000342726707\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.16784835837815\n",
      "      mean_inference_ms: 0.6767722736511614\n",
      "      mean_processing_ms: 4.0750629128699565\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.067\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.626727104187012\n",
      "        critic_loss: 0.0004470727581065148\n",
      "        max_q: 12.710855484008789\n",
      "        mean_q: 12.615457534790039\n",
      "        min_q: 12.449460983276367\n",
      "        model: {}\n",
      "        td_error: 0.0008941454580053687\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28200\n",
      "    num_steps_trained: 6835200\n",
      "    num_target_updates: 28200\n",
      "    opt_peak_throughput: 62939.146\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.84\n",
      "    sample_time_ms: 90.604\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.643749999999997\n",
      "    ram_util_percent: 70.4875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.10492426994571\n",
      "    mean_inference_ms: 0.5923169780476659\n",
      "    mean_processing_ms: 4.810035558480709\n",
      "  time_since_restore: 2861.4177429676056\n",
      "  time_this_iter_s: 9.891061067581177\n",
      "  time_total_s: 2861.4177429676056\n",
      "  timestamp: 1582123550\n",
      "  timesteps_since_restore: 28200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28200\n",
      "  training_iteration: 282\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         2861.42</td><td style=\"text-align: right;\">      28200</td><td style=\"text-align: right;\">   3.162</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-46-03\n",
      "  done: false\n",
      "  episode_len_mean: 33.03\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.2030000477284193\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1275\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.15862487733439\n",
      "      mean_inference_ms: 0.676511007380449\n",
      "      mean_processing_ms: 4.077800921083609\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.317\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.591348648071289\n",
      "        critic_loss: 0.0005366115365177393\n",
      "        max_q: 12.690767288208008\n",
      "        mean_q: 12.58549690246582\n",
      "        min_q: 12.411144256591797\n",
      "        model: {}\n",
      "        td_error: 0.0010732230730354786\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28300\n",
      "    num_steps_trained: 6860800\n",
      "    num_target_updates: 28300\n",
      "    opt_peak_throughput: 59295.234\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.652\n",
      "    sample_time_ms: 77.45\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.74705882352941\n",
      "    ram_util_percent: 70.58823529411765\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.10893998291155\n",
      "    mean_inference_ms: 0.5921340790799441\n",
      "    mean_processing_ms: 4.8060763471103884\n",
      "  time_since_restore: 2871.3086519241333\n",
      "  time_this_iter_s: 9.89090895652771\n",
      "  time_total_s: 2871.3086519241333\n",
      "  timestamp: 1582123563\n",
      "  timesteps_since_restore: 28300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28300\n",
      "  training_iteration: 283\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         2871.31</td><td style=\"text-align: right;\">      28300</td><td style=\"text-align: right;\">   3.203</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 33.02\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.202000047713518\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1278\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 44.0\n",
      "    episode_reward_max: 4.300000064074993\n",
      "    episode_reward_mean: 4.300000064074993\n",
      "    episode_reward_min: 4.300000064074993\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.19211925527195\n",
      "      mean_inference_ms: 0.6769883502241218\n",
      "      mean_processing_ms: 4.06749772490631\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.511\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.626847267150879\n",
      "        critic_loss: 0.000769192585721612\n",
      "        max_q: 12.757262229919434\n",
      "        mean_q: 12.615144729614258\n",
      "        min_q: 12.504657745361328\n",
      "        model: {}\n",
      "        td_error: 0.001538385171443224\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28400\n",
      "    num_steps_trained: 6886400\n",
      "    num_target_updates: 28400\n",
      "    opt_peak_throughput: 56747.483\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.21\n",
      "    sample_time_ms: 76.623\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.070588235294117\n",
      "    ram_util_percent: 69.97058823529412\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.1147904492947\n",
      "    mean_inference_ms: 0.5918662963629981\n",
      "    mean_processing_ms: 4.80006436015846\n",
      "  time_since_restore: 2881.310912847519\n",
      "  time_this_iter_s: 10.00226092338562\n",
      "  time_total_s: 2881.310912847519\n",
      "  timestamp: 1582123574\n",
      "  timesteps_since_restore: 28400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28400\n",
      "  training_iteration: 284\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         2881.31</td><td style=\"text-align: right;\">      28400</td><td style=\"text-align: right;\">   3.202</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 33.0\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.200000047683716\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1280\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.18410981448768\n",
      "      mean_inference_ms: 0.677225735265884\n",
      "      mean_processing_ms: 4.069782133031306\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.745\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.595937728881836\n",
      "        critic_loss: 0.0005616114940494299\n",
      "        max_q: 12.692614555358887\n",
      "        mean_q: 12.584577560424805\n",
      "        min_q: 12.469291687011719\n",
      "        model: {}\n",
      "        td_error: 0.0011232229880988598\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28500\n",
      "    num_steps_trained: 6912000\n",
      "    num_target_updates: 28500\n",
      "    opt_peak_throughput: 68359.849\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.601\n",
      "    sample_time_ms: 80.816\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.804999999999996\n",
      "    ram_util_percent: 69.965\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.1187378571639\n",
      "    mean_inference_ms: 0.5916850750346215\n",
      "    mean_processing_ms: 4.795942776342182\n",
      "  time_since_restore: 2891.200947999954\n",
      "  time_this_iter_s: 9.890035152435303\n",
      "  time_total_s: 2891.200947999954\n",
      "  timestamp: 1582123588\n",
      "  timesteps_since_restore: 28500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28500\n",
      "  training_iteration: 285\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">          2891.2</td><td style=\"text-align: right;\">      28500</td><td style=\"text-align: right;\">     3.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 33.56\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.256000048518181\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1283\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 31.0\n",
      "    episode_reward_max: 3.0000000447034836\n",
      "    episode_reward_mean: 3.0000000447034836\n",
      "    episode_reward_min: 3.0000000447034836\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.19477098817168\n",
      "      mean_inference_ms: 0.6767378767336905\n",
      "      mean_processing_ms: 4.0666873789744935\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.159\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.588020324707031\n",
      "        critic_loss: 0.0006214427994564176\n",
      "        max_q: 12.70059585571289\n",
      "        mean_q: 12.578311920166016\n",
      "        min_q: 12.450618743896484\n",
      "        model: {}\n",
      "        td_error: 0.0012428854824975133\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28600\n",
      "    num_steps_trained: 6937600\n",
      "    num_target_updates: 28600\n",
      "    opt_peak_throughput: 61550.119\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.971\n",
      "    sample_time_ms: 80.233\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.841176470588238\n",
      "    ram_util_percent: 70.09411764705882\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.12457518665504\n",
      "    mean_inference_ms: 0.5914155941606826\n",
      "    mean_processing_ms: 4.789773444595649\n",
      "  time_since_restore: 2901.1911430358887\n",
      "  time_this_iter_s: 9.990195035934448\n",
      "  time_total_s: 2901.1911430358887\n",
      "  timestamp: 1582123600\n",
      "  timesteps_since_restore: 28600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28600\n",
      "  training_iteration: 286\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         2901.19</td><td style=\"text-align: right;\">      28600</td><td style=\"text-align: right;\">   3.256</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 33.16\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.2160000479221345\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1286\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 30.0\n",
      "    episode_reward_max: 2.9000000432133675\n",
      "    episode_reward_mean: 2.9000000432133675\n",
      "    episode_reward_min: 2.9000000432133675\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.20343439451362\n",
      "      mean_inference_ms: 0.6763539738023704\n",
      "      mean_processing_ms: 4.064185740473367\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.034\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.545694351196289\n",
      "        critic_loss: 0.0003417748666834086\n",
      "        max_q: 12.630434036254883\n",
      "        mean_q: 12.534603118896484\n",
      "        min_q: 12.43588924407959\n",
      "        model: {}\n",
      "        td_error: 0.0006835496751591563\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28700\n",
      "    num_steps_trained: 6963200\n",
      "    num_target_updates: 28700\n",
      "    opt_peak_throughput: 63462.544\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.404\n",
      "    sample_time_ms: 80.895\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.87777777777778\n",
      "    ram_util_percent: 70.01111111111112\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.13038004556205\n",
      "    mean_inference_ms: 0.5911479925682883\n",
      "    mean_processing_ms: 4.783688671333013\n",
      "  time_since_restore: 2911.1808018684387\n",
      "  time_this_iter_s: 9.989658832550049\n",
      "  time_total_s: 2911.1808018684387\n",
      "  timestamp: 1582123613\n",
      "  timesteps_since_restore: 28700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28700\n",
      "  training_iteration: 287\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         2911.18</td><td style=\"text-align: right;\">      28700</td><td style=\"text-align: right;\">   3.216</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 33.5\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.250000048428774\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1290\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 53.0\n",
      "    episode_reward_max: 5.200000077486038\n",
      "    episode_reward_mean: 5.200000077486038\n",
      "    episode_reward_min: 5.200000077486038\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.25295208873406\n",
      "      mean_inference_ms: 0.6757144567563262\n",
      "      mean_processing_ms: 4.0491736408418255\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.063\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.545499801635742\n",
      "        critic_loss: 0.0005348370177671313\n",
      "        max_q: 12.632726669311523\n",
      "        mean_q: 12.535332679748535\n",
      "        min_q: 12.421489715576172\n",
      "        model: {}\n",
      "        td_error: 0.0010696740355342627\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28800\n",
      "    num_steps_trained: 6988800\n",
      "    num_target_updates: 28800\n",
      "    opt_peak_throughput: 63005.993\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.535\n",
      "    sample_time_ms: 80.881\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.442105263157895\n",
      "    ram_util_percent: 70.02631578947368\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.13811413432191\n",
      "    mean_inference_ms: 0.590794902538248\n",
      "    mean_processing_ms: 4.775828059406943\n",
      "  time_since_restore: 2921.271707057953\n",
      "  time_this_iter_s: 10.09090518951416\n",
      "  time_total_s: 2921.271707057953\n",
      "  timestamp: 1582123626\n",
      "  timesteps_since_restore: 28800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28800\n",
      "  training_iteration: 288\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         2921.27</td><td style=\"text-align: right;\">      28800</td><td style=\"text-align: right;\">    3.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 33.68\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.2680000486969947\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1294\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 86.2437003734572\n",
      "      mean_inference_ms: 0.675377934977186\n",
      "      mean_processing_ms: 4.0520389529991565\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.123\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.501816749572754\n",
      "        critic_loss: 0.0005283640930429101\n",
      "        max_q: 12.594576835632324\n",
      "        mean_q: 12.490426063537598\n",
      "        min_q: 12.330718040466309\n",
      "        model: {}\n",
      "        td_error: 0.001056728302501142\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28900\n",
      "    num_steps_trained: 7014400\n",
      "    num_target_updates: 28900\n",
      "    opt_peak_throughput: 62093.638\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.417\n",
      "    sample_time_ms: 90.946\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.757142857142856\n",
      "    ram_util_percent: 69.79047619047618\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.14591622366788\n",
      "    mean_inference_ms: 0.5904423349006008\n",
      "    mean_processing_ms: 4.767891565591865\n",
      "  time_since_restore: 2931.3629648685455\n",
      "  time_this_iter_s: 10.091257810592651\n",
      "  time_total_s: 2931.3629648685455\n",
      "  timestamp: 1582123641\n",
      "  timesteps_since_restore: 28900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 28900\n",
      "  training_iteration: 289\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         2931.36</td><td style=\"text-align: right;\">      28900</td><td style=\"text-align: right;\">   3.268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-47-33\n",
      "  done: false\n",
      "  episode_len_mean: 33.68\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.2680000486969947\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1296\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 87.76451446606983\n",
      "      mean_inference_ms: 0.6648529509532531\n",
      "      mean_processing_ms: 3.591679798042421\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.279\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.50105094909668\n",
      "        critic_loss: 0.00038390373811125755\n",
      "        max_q: 12.597278594970703\n",
      "        mean_q: 12.49218463897705\n",
      "        min_q: 12.382518768310547\n",
      "        model: {}\n",
      "        td_error: 0.000767807534430176\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 7040000\n",
      "    num_target_updates: 29000\n",
      "    opt_peak_throughput: 59832.819\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.731\n",
      "    sample_time_ms: 79.251\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.711764705882356\n",
      "    ram_util_percent: 69.82941176470588\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.14996108061436\n",
      "    mean_inference_ms: 0.5902629811808334\n",
      "    mean_processing_ms: 4.763737479161056\n",
      "  time_since_restore: 2941.253148794174\n",
      "  time_this_iter_s: 9.890183925628662\n",
      "  time_total_s: 2941.253148794174\n",
      "  timestamp: 1582123653\n",
      "  timesteps_since_restore: 29000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 290\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         2941.25</td><td style=\"text-align: right;\">      29000</td><td style=\"text-align: right;\">   3.268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-49-23\n",
      "  done: false\n",
      "  episode_len_mean: 33.81\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.28100004889071\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1301\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 88.95593655292491\n",
      "      mean_inference_ms: 0.6607154530469875\n",
      "      mean_processing_ms: 3.230289336764365\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.182\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.493168830871582\n",
      "        critic_loss: 0.0005053691565990448\n",
      "        max_q: 12.639599800109863\n",
      "        mean_q: 12.485682487487793\n",
      "        min_q: 12.379112243652344\n",
      "        model: {}\n",
      "        td_error: 0.0010107383131980896\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29100\n",
      "    num_steps_trained: 7065600\n",
      "    num_target_updates: 29100\n",
      "    opt_peak_throughput: 61220.242\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.265\n",
      "    sample_time_ms: 80.927\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.807692307692308\n",
      "    ram_util_percent: 69.99038461538463\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.16026709889664\n",
      "    mean_inference_ms: 0.5898154312939166\n",
      "    mean_processing_ms: 4.7537354323711725\n",
      "  time_since_restore: 2951.4432938098907\n",
      "  time_this_iter_s: 10.190145015716553\n",
      "  time_total_s: 2951.4432938098907\n",
      "  timestamp: 1582123763\n",
      "  timesteps_since_restore: 29100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29100\n",
      "  training_iteration: 291\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         2951.44</td><td style=\"text-align: right;\">      29100</td><td style=\"text-align: right;\">   3.281</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 33.52\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.252000048458576\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1304\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 80.0\n",
      "    episode_reward_max: 7.900000117719173\n",
      "    episode_reward_mean: 7.900000117719173\n",
      "    episode_reward_min: 7.900000117719173\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.0087538261704\n",
      "      mean_inference_ms: 0.6600436321168179\n",
      "      mean_processing_ms: 3.214330035821234\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.01\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.4630765914917\n",
      "        critic_loss: 0.0004597446823026985\n",
      "        max_q: 12.564811706542969\n",
      "        mean_q: 12.45141887664795\n",
      "        min_q: 12.35080623626709\n",
      "        model: {}\n",
      "        td_error: 0.0009194893063977361\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29200\n",
      "    num_steps_trained: 7091200\n",
      "    num_target_updates: 29200\n",
      "    opt_peak_throughput: 63839.484\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.267\n",
      "    sample_time_ms: 80.985\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.971153846153843\n",
      "    ram_util_percent: 69.96346153846153\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.16654467385663\n",
      "    mean_inference_ms: 0.5895473091370809\n",
      "    mean_processing_ms: 4.747872888082909\n",
      "  time_since_restore: 2961.434040784836\n",
      "  time_this_iter_s: 9.990746974945068\n",
      "  time_total_s: 2961.434040784836\n",
      "  timestamp: 1582123873\n",
      "  timesteps_since_restore: 29200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29200\n",
      "  training_iteration: 292\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         2961.43</td><td style=\"text-align: right;\">      29200</td><td style=\"text-align: right;\">   3.252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-51-30\n",
      "  done: false\n",
      "  episode_len_mean: 33.53\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.2530000484734773\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1307\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.00337386179056\n",
      "      mean_inference_ms: 0.659770959542168\n",
      "      mean_processing_ms: 3.2158980298668327\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.211\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.4486722946167\n",
      "        critic_loss: 0.00043427819036878645\n",
      "        max_q: 12.52202320098877\n",
      "        mean_q: 12.437477111816406\n",
      "        min_q: 12.279098510742188\n",
      "        model: {}\n",
      "        td_error: 0.000868556322529912\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29300\n",
      "    num_steps_trained: 7116800\n",
      "    num_target_updates: 29300\n",
      "    opt_peak_throughput: 60799.061\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.45\n",
      "    sample_time_ms: 80.778\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.304000000000002\n",
      "    ram_util_percent: 70.42800000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.17283132159542\n",
      "    mean_inference_ms: 0.5892807666513923\n",
      "    mean_processing_ms: 4.742109214126714\n",
      "  time_since_restore: 2971.422836780548\n",
      "  time_this_iter_s: 9.98879599571228\n",
      "  time_total_s: 2971.422836780548\n",
      "  timestamp: 1582123890\n",
      "  timesteps_since_restore: 29300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29300\n",
      "  training_iteration: 293\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         2971.42</td><td style=\"text-align: right;\">      29300</td><td style=\"text-align: right;\">   3.253</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-51-43\n",
      "  done: false\n",
      "  episode_len_mean: 33.8\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.2800000488758085\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1309\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.0\n",
      "    episode_reward_max: 2.0000000298023224\n",
      "    episode_reward_mean: 2.0000000298023224\n",
      "    episode_reward_min: 2.0000000298023224\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 88.99139783498815\n",
      "      mean_inference_ms: 0.6596650238390322\n",
      "      mean_processing_ms: 3.2194816454091058\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.112\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.418015480041504\n",
      "        critic_loss: 0.000526237185113132\n",
      "        max_q: 12.494627952575684\n",
      "        mean_q: 12.409311294555664\n",
      "        min_q: 12.284802436828613\n",
      "        model: {}\n",
      "        td_error: 0.001052474370226264\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29400\n",
      "    num_steps_trained: 7142400\n",
      "    num_target_updates: 29400\n",
      "    opt_peak_throughput: 62253.842\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.494\n",
      "    sample_time_ms: 80.768\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.26111111111111\n",
      "    ram_util_percent: 70.53333333333333\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.17713393849692\n",
      "    mean_inference_ms: 0.5890991692203553\n",
      "    mean_processing_ms: 4.738119032214714\n",
      "  time_since_restore: 2981.313018798828\n",
      "  time_this_iter_s: 9.89018201828003\n",
      "  time_total_s: 2981.313018798828\n",
      "  timestamp: 1582123903\n",
      "  timesteps_since_restore: 29400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29400\n",
      "  training_iteration: 294\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         2981.31</td><td style=\"text-align: right;\">      29400</td><td style=\"text-align: right;\">    3.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 34.36\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.3360000497102735\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1311\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 88.97418439889155\n",
      "      mean_inference_ms: 0.6595220135039642\n",
      "      mean_processing_ms: 3.2247573888602377\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.949\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.41946029663086\n",
      "        critic_loss: 0.000554967497009784\n",
      "        max_q: 12.513887405395508\n",
      "        mean_q: 12.408205032348633\n",
      "        min_q: 12.300565719604492\n",
      "        model: {}\n",
      "        td_error: 0.001109934994019568\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29500\n",
      "    num_steps_trained: 7168000\n",
      "    num_target_updates: 29500\n",
      "    opt_peak_throughput: 64828.129\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.519\n",
      "    sample_time_ms: 80.945\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.8375\n",
      "    ram_util_percent: 70.525\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.18150811849411\n",
      "    mean_inference_ms: 0.5889179212760098\n",
      "    mean_processing_ms: 4.733944828812864\n",
      "  time_since_restore: 2991.202651977539\n",
      "  time_this_iter_s: 9.889633178710938\n",
      "  time_total_s: 2991.202651977539\n",
      "  timestamp: 1582123915\n",
      "  timesteps_since_restore: 29500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29500\n",
      "  training_iteration: 295\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">          2991.2</td><td style=\"text-align: right;\">      29500</td><td style=\"text-align: right;\">   3.336</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 34.63\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.363000050112605\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1315\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 29.0\n",
      "    episode_reward_max: 2.8000000417232513\n",
      "    episode_reward_mean: 2.8000000417232513\n",
      "    episode_reward_min: 2.8000000417232513\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 88.97109148527068\n",
      "      mean_inference_ms: 0.659279429309592\n",
      "      mean_processing_ms: 3.2256906451274117\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.009\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.400729179382324\n",
      "        critic_loss: 0.0004049359995406121\n",
      "        max_q: 12.47492504119873\n",
      "        mean_q: 12.389730453491211\n",
      "        min_q: 12.25492000579834\n",
      "        model: {}\n",
      "        td_error: 0.0008098720572888851\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29600\n",
      "    num_steps_trained: 7193600\n",
      "    num_target_updates: 29600\n",
      "    opt_peak_throughput: 63851.633\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.675\n",
      "    sample_time_ms: 90.655\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.647058823529413\n",
      "    ram_util_percent: 70.52352941176471\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.1905127413809\n",
      "    mean_inference_ms: 0.5885477848568993\n",
      "    mean_processing_ms: 4.725587811248558\n",
      "  time_since_restore: 3001.2923731803894\n",
      "  time_this_iter_s: 10.089721202850342\n",
      "  time_total_s: 3001.2923731803894\n",
      "  timestamp: 1582123926\n",
      "  timesteps_since_restore: 29600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29600\n",
      "  training_iteration: 296\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         3001.29</td><td style=\"text-align: right;\">      29600</td><td style=\"text-align: right;\">   3.363</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 34.85\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.3850000504404307\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1316\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 88.95393552047322\n",
      "      mean_inference_ms: 0.6590935427420935\n",
      "      mean_processing_ms: 3.2308458097929487\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.205\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.355079650878906\n",
      "        critic_loss: 0.000570646661799401\n",
      "        max_q: 12.442733764648438\n",
      "        mean_q: 12.344669342041016\n",
      "        min_q: 12.24467945098877\n",
      "        model: {}\n",
      "        td_error: 0.001141293323598802\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29700\n",
      "    num_steps_trained: 7219200\n",
      "    num_target_updates: 29700\n",
      "    opt_peak_throughput: 60886.631\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.723\n",
      "    sample_time_ms: 80.482\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.211764705882354\n",
      "    ram_util_percent: 70.5529411764706\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.19281919492538\n",
      "    mean_inference_ms: 0.5884543164751261\n",
      "    mean_processing_ms: 4.723395991208637\n",
      "  time_since_restore: 3011.0822002887726\n",
      "  time_this_iter_s: 9.789827108383179\n",
      "  time_total_s: 3011.0822002887726\n",
      "  timestamp: 1582123939\n",
      "  timesteps_since_restore: 29700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29700\n",
      "  training_iteration: 297\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         3011.08</td><td style=\"text-align: right;\">      29700</td><td style=\"text-align: right;\">   3.385</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 35.1\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.4100000508129598\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1320\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.90298964883613\n",
      "      mean_inference_ms: 0.6524695754333294\n",
      "      mean_processing_ms: 2.9435552018432327\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.133\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.335718154907227\n",
      "        critic_loss: 0.0005365803954191506\n",
      "        max_q: 12.428895950317383\n",
      "        mean_q: 12.326614379882812\n",
      "        min_q: 12.166631698608398\n",
      "        model: {}\n",
      "        td_error: 0.0010731606744229794\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29800\n",
      "    num_steps_trained: 7244800\n",
      "    num_target_updates: 29800\n",
      "    opt_peak_throughput: 61937.472\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.46\n",
      "    sample_time_ms: 90.726\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.352941176470583\n",
      "    ram_util_percent: 70.55882352941177\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.20226800999662\n",
      "    mean_inference_ms: 0.588077086913871\n",
      "    mean_processing_ms: 4.714763116505325\n",
      "  time_since_restore: 3021.172197341919\n",
      "  time_this_iter_s: 10.089997053146362\n",
      "  time_total_s: 3021.172197341919\n",
      "  timestamp: 1582123950\n",
      "  timesteps_since_restore: 29800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29800\n",
      "  training_iteration: 298\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         3021.17</td><td style=\"text-align: right;\">      29800</td><td style=\"text-align: right;\">    3.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 34.92\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.392000050544739\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1323\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 22.0\n",
      "    episode_reward_max: 2.1000000312924385\n",
      "    episode_reward_mean: 2.1000000312924385\n",
      "    episode_reward_min: 2.1000000312924385\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.8915090450115\n",
      "      mean_inference_ms: 0.6522894738877338\n",
      "      mean_processing_ms: 2.946990311196452\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.344579696655273\n",
      "        critic_loss: 0.0004156450740993023\n",
      "        max_q: 12.466463088989258\n",
      "        mean_q: 12.332326889038086\n",
      "        min_q: 12.214580535888672\n",
      "        model: {}\n",
      "        td_error: 0.0008312901481986046\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29900\n",
      "    num_steps_trained: 7270400\n",
      "    num_target_updates: 29900\n",
      "    opt_peak_throughput: 62227.145\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.111\n",
      "    sample_time_ms: 91.117\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.924358974358974\n",
      "    ram_util_percent: 70.27884615384616\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.20954210077221\n",
      "    mean_inference_ms: 0.5877943759626288\n",
      "    mean_processing_ms: 4.7083882217953805\n",
      "  time_since_restore: 3031.163029193878\n",
      "  time_this_iter_s: 9.990831851959229\n",
      "  time_total_s: 3031.163029193878\n",
      "  timestamp: 1582124060\n",
      "  timesteps_since_restore: 29900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 29900\n",
      "  training_iteration: 299\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         3031.16</td><td style=\"text-align: right;\">      29900</td><td style=\"text-align: right;\">   3.392</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-54-31\n",
      "  done: false\n",
      "  episode_len_mean: 34.92\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.392000050544739\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1323\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 38.0\n",
      "    episode_reward_max: 3.7000000551342964\n",
      "    episode_reward_mean: 3.7000000551342964\n",
      "    episode_reward_min: 3.7000000551342964\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.89424393597771\n",
      "      mean_inference_ms: 0.6519672393798828\n",
      "      mean_processing_ms: 2.9462718290441177\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.954\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.3422212600708\n",
      "        critic_loss: 0.0003854219685308635\n",
      "        max_q: 12.421192169189453\n",
      "        mean_q: 12.333372116088867\n",
      "        min_q: 12.219621658325195\n",
      "        model: {}\n",
      "        td_error: 0.000770843937061727\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 7296000\n",
      "    num_target_updates: 30000\n",
      "    opt_peak_throughput: 64737.841\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.267\n",
      "    sample_time_ms: 81.202\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.84375\n",
      "    ram_util_percent: 70.31875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.20954210077223\n",
      "    mean_inference_ms: 0.5877943759626288\n",
      "    mean_processing_ms: 4.70838822179538\n",
      "  time_since_restore: 3040.8521811962128\n",
      "  time_this_iter_s: 9.689152002334595\n",
      "  time_total_s: 3040.8521811962128\n",
      "  timestamp: 1582124071\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 300\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         3040.85</td><td style=\"text-align: right;\">      30000</td><td style=\"text-align: right;\">   3.392</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-54-45\n",
      "  done: false\n",
      "  episode_len_mean: 34.92\n",
      "  episode_reward_max: 9.500000141561031\n",
      "  episode_reward_mean: 3.392000050544739\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1323\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.87650167673154\n",
      "      mean_inference_ms: 0.6518483386003882\n",
      "      mean_processing_ms: 2.951690352948985\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.02\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.316551208496094\n",
      "        critic_loss: 0.00033970968797802925\n",
      "        max_q: 12.402063369750977\n",
      "        mean_q: 12.304372787475586\n",
      "        min_q: 12.19097900390625\n",
      "        model: {}\n",
      "        td_error: 0.0006794193759560585\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30100\n",
      "    num_steps_trained: 7321600\n",
      "    num_target_updates: 30100\n",
      "    opt_peak_throughput: 63679.334\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.409\n",
      "    sample_time_ms: 80.936\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.178947368421053\n",
      "    ram_util_percent: 70.3263157894737\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.20954210077223\n",
      "    mean_inference_ms: 0.5877943759626288\n",
      "    mean_processing_ms: 4.70838822179538\n",
      "  time_since_restore: 3050.5426063537598\n",
      "  time_this_iter_s: 9.690425157546997\n",
      "  time_total_s: 3050.5426063537598\n",
      "  timestamp: 1582124085\n",
      "  timesteps_since_restore: 30100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30100\n",
      "  training_iteration: 301\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         3050.54</td><td style=\"text-align: right;\">      30100</td><td style=\"text-align: right;\">   3.392</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-54-56\n",
      "  done: false\n",
      "  episode_len_mean: 36.85\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.585000053420663\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1326\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.8597298173217\n",
      "      mean_inference_ms: 0.6517163581318326\n",
      "      mean_processing_ms: 2.9568424946195013\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.03\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.285629272460938\n",
      "        critic_loss: 0.00043465750059112906\n",
      "        max_q: 12.378199577331543\n",
      "        mean_q: 12.27902603149414\n",
      "        min_q: 12.152700424194336\n",
      "        model: {}\n",
      "        td_error: 0.0008693150011822581\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30200\n",
      "    num_steps_trained: 7347200\n",
      "    num_target_updates: 30200\n",
      "    opt_peak_throughput: 63527.501\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.611\n",
      "    sample_time_ms: 90.778\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.95\n",
      "    ram_util_percent: 70.4\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.21730394811323\n",
      "    mean_inference_ms: 0.5874950691082282\n",
      "    mean_processing_ms: 4.701398337754146\n",
      "  time_since_restore: 3060.5331423282623\n",
      "  time_this_iter_s: 9.990535974502563\n",
      "  time_total_s: 3060.5331423282623\n",
      "  timestamp: 1582124096\n",
      "  timesteps_since_restore: 30200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30200\n",
      "  training_iteration: 302\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         3060.53</td><td style=\"text-align: right;\">      30200</td><td style=\"text-align: right;\">   3.585</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 36.38\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.5380000527203084\n",
      "  episode_reward_min: 0.9000000134110451\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1329\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 14.0\n",
      "    episode_reward_max: 1.3000000193715096\n",
      "    episode_reward_mean: 1.3000000193715096\n",
      "    episode_reward_min: 1.3000000193715096\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.84115505844159\n",
      "      mean_inference_ms: 0.6515751179960585\n",
      "      mean_processing_ms: 2.962532195401393\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.099\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.28017807006836\n",
      "        critic_loss: 0.0003214957832824439\n",
      "        max_q: 12.36453914642334\n",
      "        mean_q: 12.270787239074707\n",
      "        min_q: 12.136604309082031\n",
      "        model: {}\n",
      "        td_error: 0.0006429915665648878\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30300\n",
      "    num_steps_trained: 7372800\n",
      "    num_target_updates: 30300\n",
      "    opt_peak_throughput: 62450.814\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.353\n",
      "    sample_time_ms: 80.916\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.00625\n",
      "    ram_util_percent: 70.48125\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.22502797339668\n",
      "    mean_inference_ms: 0.5871996383875966\n",
      "    mean_processing_ms: 4.694512006584589\n",
      "  time_since_restore: 3070.522699356079\n",
      "  time_this_iter_s: 9.989557027816772\n",
      "  time_total_s: 3070.522699356079\n",
      "  timestamp: 1582124107\n",
      "  timesteps_since_restore: 30300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30300\n",
      "  training_iteration: 303\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         3070.52</td><td style=\"text-align: right;\">      30300</td><td style=\"text-align: right;\">   3.538</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-55-18\n",
      "  done: false\n",
      "  episode_len_mean: 37.33\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.6330000541359184\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1330\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 20.0\n",
      "    episode_reward_max: 1.9000000283122063\n",
      "    episode_reward_mean: 1.9000000283122063\n",
      "    episode_reward_min: 1.9000000283122063\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.82797817608483\n",
      "      mean_inference_ms: 0.6513971831889995\n",
      "      mean_processing_ms: 2.9665166580086004\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.167\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.275074005126953\n",
      "        critic_loss: 0.00035721552558243275\n",
      "        max_q: 12.368230819702148\n",
      "        mean_q: 12.267422676086426\n",
      "        min_q: 12.12386703491211\n",
      "        model: {}\n",
      "        td_error: 0.0007144310511648655\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30400\n",
      "    num_steps_trained: 7398400\n",
      "    num_target_updates: 30400\n",
      "    opt_peak_throughput: 61436.719\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.377\n",
      "    sample_time_ms: 79.928\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.375\n",
      "    ram_util_percent: 70.53125\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.22766908319545\n",
      "    mean_inference_ms: 0.5870992154063577\n",
      "    mean_processing_ms: 4.69211458702426\n",
      "  time_since_restore: 3080.3131453990936\n",
      "  time_this_iter_s: 9.790446043014526\n",
      "  time_total_s: 3080.3131453990936\n",
      "  timestamp: 1582124118\n",
      "  timesteps_since_restore: 30400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30400\n",
      "  training_iteration: 304\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         3080.31</td><td style=\"text-align: right;\">      30400</td><td style=\"text-align: right;\">   3.633</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-55-30\n",
      "  done: false\n",
      "  episode_len_mean: 37.33\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.6330000541359184\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1330\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 19.0\n",
      "    episode_reward_max: 1.8000000268220901\n",
      "    episode_reward_mean: 1.8000000268220901\n",
      "    episode_reward_min: 1.8000000268220901\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 89.81407175145125\n",
      "      mean_inference_ms: 0.6512253766901877\n",
      "      mean_processing_ms: 2.9707854174133863\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.067\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.252756118774414\n",
      "        critic_loss: 0.00037473085103556514\n",
      "        max_q: 12.341493606567383\n",
      "        mean_q: 12.244054794311523\n",
      "        min_q: 12.099392890930176\n",
      "        model: {}\n",
      "        td_error: 0.0007494616438634694\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30500\n",
      "    num_steps_trained: 7424000\n",
      "    num_target_updates: 30500\n",
      "    opt_peak_throughput: 62944.311\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.385\n",
      "    sample_time_ms: 81.055\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.53125\n",
      "    ram_util_percent: 70.5125\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.22766908319547\n",
      "    mean_inference_ms: 0.5870992154063577\n",
      "    mean_processing_ms: 4.69211458702426\n",
      "  time_since_restore: 3090.003283262253\n",
      "  time_this_iter_s: 9.69013786315918\n",
      "  time_total_s: 3090.003283262253\n",
      "  timestamp: 1582124130\n",
      "  timesteps_since_restore: 30500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30500\n",
      "  training_iteration: 305\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">            3090</td><td style=\"text-align: right;\">      30500</td><td style=\"text-align: right;\">   3.633</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 38.69\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.7690000561624766\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1331\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 90.59671230040868\n",
      "      mean_inference_ms: 0.646084519045106\n",
      "      mean_processing_ms: 2.733801439301405\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.932\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.24846076965332\n",
      "        critic_loss: 0.0005225220811553299\n",
      "        max_q: 12.33073616027832\n",
      "        mean_q: 12.240415573120117\n",
      "        min_q: 12.083820343017578\n",
      "        model: {}\n",
      "        td_error: 0.0010450441623106599\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30600\n",
      "    num_steps_trained: 7449600\n",
      "    num_target_updates: 30600\n",
      "    opt_peak_throughput: 65101.697\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.65\n",
      "    sample_time_ms: 80.967\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.541176470588237\n",
      "    ram_util_percent: 70.51176470588236\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.23046644282232\n",
      "    mean_inference_ms: 0.5869929313198753\n",
      "    mean_processing_ms: 4.689468891406187\n",
      "  time_since_restore: 3099.794558286667\n",
      "  time_this_iter_s: 9.791275024414062\n",
      "  time_total_s: 3099.794558286667\n",
      "  timestamp: 1582124141\n",
      "  timesteps_since_restore: 30600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30600\n",
      "  training_iteration: 306\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         3099.79</td><td style=\"text-align: right;\">      30600</td><td style=\"text-align: right;\">   3.769</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-57-31\n",
      "  done: false\n",
      "  episode_len_mean: 39.91\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.8910000579804183\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1332\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 59.0\n",
      "    episode_reward_max: 5.800000086426735\n",
      "    episode_reward_mean: 5.800000086426735\n",
      "    episode_reward_min: 5.800000086426735\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 90.6124164424406\n",
      "      mean_inference_ms: 0.6457557085012595\n",
      "      mean_processing_ms: 2.729081422759107\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.012\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.22545051574707\n",
      "        critic_loss: 0.0005269116954877973\n",
      "        max_q: 12.30971908569336\n",
      "        mean_q: 12.215787887573242\n",
      "        min_q: 12.097448348999023\n",
      "        model: {}\n",
      "        td_error: 0.0010538233909755945\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30700\n",
      "    num_steps_trained: 7475200\n",
      "    num_target_updates: 30700\n",
      "    opt_peak_throughput: 63801.551\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.142\n",
      "    sample_time_ms: 91.261\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.964516129032258\n",
      "    ram_util_percent: 70.27161290322582\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.23325764441196\n",
      "    mean_inference_ms: 0.5868863362644455\n",
      "    mean_processing_ms: 4.686805271839833\n",
      "  time_since_restore: 3109.5842514038086\n",
      "  time_this_iter_s: 9.789693117141724\n",
      "  time_total_s: 3109.5842514038086\n",
      "  timestamp: 1582124251\n",
      "  timesteps_since_restore: 30700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30700\n",
      "  training_iteration: 307\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         3109.58</td><td style=\"text-align: right;\">      30700</td><td style=\"text-align: right;\">   3.891</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-57-47\n",
      "  done: false\n",
      "  episode_len_mean: 39.73\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.8730000577121975\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1334\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 91.26753042589752\n",
      "      mean_inference_ms: 0.6415803629652899\n",
      "      mean_processing_ms: 2.530658640957118\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.952\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.177253723144531\n",
      "        critic_loss: 0.0005942441639490426\n",
      "        max_q: 12.28797435760498\n",
      "        mean_q: 12.169205665588379\n",
      "        min_q: 12.046630859375\n",
      "        model: {}\n",
      "        td_error: 0.0011884883278980851\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30800\n",
      "    num_steps_trained: 7500800\n",
      "    num_target_updates: 30800\n",
      "    opt_peak_throughput: 64769.472\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.54\n",
      "    sample_time_ms: 80.849\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.890909090909089\n",
      "    ram_util_percent: 70.41818181818184\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.23899763776014\n",
      "    mean_inference_ms: 0.5866666409814741\n",
      "    mean_processing_ms: 4.681331065194649\n",
      "  time_since_restore: 3119.473354578018\n",
      "  time_this_iter_s: 9.889103174209595\n",
      "  time_total_s: 3119.473354578018\n",
      "  timestamp: 1582124267\n",
      "  timesteps_since_restore: 30800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30800\n",
      "  training_iteration: 308\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         3119.47</td><td style=\"text-align: right;\">      30800</td><td style=\"text-align: right;\">   3.873</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 40.81\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9810000593215227\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1335\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 16.0\n",
      "    episode_reward_max: 1.5000000223517418\n",
      "    episode_reward_mean: 1.5000000223517418\n",
      "    episode_reward_min: 1.5000000223517418\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 91.25176083310525\n",
      "      mean_inference_ms: 0.6415834452243562\n",
      "      mean_processing_ms: 2.5354343675104936\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.949\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.174134254455566\n",
      "        critic_loss: 0.0003787492460105568\n",
      "        max_q: 12.27391242980957\n",
      "        mean_q: 12.16390609741211\n",
      "        min_q: 12.038301467895508\n",
      "        model: {}\n",
      "        td_error: 0.0007574984338134527\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30900\n",
      "    num_steps_trained: 7526400\n",
      "    num_target_updates: 30900\n",
      "    opt_peak_throughput: 64830.086\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.185\n",
      "    sample_time_ms: 91.267\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.855128205128203\n",
      "    ram_util_percent: 70.15384615384616\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.24189562756001\n",
      "    mean_inference_ms: 0.5865571167786524\n",
      "    mean_processing_ms: 4.678544792311522\n",
      "  time_since_restore: 3129.264521598816\n",
      "  time_this_iter_s: 9.79116702079773\n",
      "  time_total_s: 3129.264521598816\n",
      "  timestamp: 1582124376\n",
      "  timesteps_since_restore: 30900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 30900\n",
      "  training_iteration: 309\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         3129.26</td><td style=\"text-align: right;\">      30900</td><td style=\"text-align: right;\">   3.981</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_09-59-47\n",
      "  done: false\n",
      "  episode_len_mean: 40.62\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9620000590384006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1337\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 51.0\n",
      "    episode_reward_max: 5.000000074505806\n",
      "    episode_reward_mean: 5.000000074505806\n",
      "    episode_reward_min: 5.000000074505806\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 91.25810041007803\n",
      "      mean_inference_ms: 0.641343005677815\n",
      "      mean_processing_ms: 2.5335484829030706\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.895\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.184629440307617\n",
      "        critic_loss: 0.00048177287681028247\n",
      "        max_q: 12.274067878723145\n",
      "        mean_q: 12.17591667175293\n",
      "        min_q: 12.021787643432617\n",
      "        model: {}\n",
      "        td_error: 0.0009635457536205649\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 7552000\n",
      "    num_target_updates: 31000\n",
      "    opt_peak_throughput: 65724.943\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.435\n",
      "    sample_time_ms: 81.153\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.49375\n",
      "    ram_util_percent: 69.6875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.24782751455014\n",
      "    mean_inference_ms: 0.5863320031106989\n",
      "    mean_processing_ms: 4.672827942834147\n",
      "  time_since_restore: 3139.1541028022766\n",
      "  time_this_iter_s: 9.889581203460693\n",
      "  time_total_s: 3139.1541028022766\n",
      "  timestamp: 1582124387\n",
      "  timesteps_since_restore: 31000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 310\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         3139.15</td><td style=\"text-align: right;\">      31000</td><td style=\"text-align: right;\">   3.962</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 40.88\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.988000059425831\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1340\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 91.8163024349509\n",
      "      mean_inference_ms: 0.6375507710425484\n",
      "      mean_processing_ms: 2.364548354705491\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.988\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.138168334960938\n",
      "        critic_loss: 0.0004538314533419907\n",
      "        max_q: 12.214391708374023\n",
      "        mean_q: 12.12770938873291\n",
      "        min_q: 11.990400314331055\n",
      "        model: {}\n",
      "        td_error: 0.0009076629066839814\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31100\n",
      "    num_steps_trained: 7577600\n",
      "    num_target_updates: 31100\n",
      "    opt_peak_throughput: 64198.659\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.495\n",
      "    sample_time_ms: 90.895\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.19047619047619\n",
      "    ram_util_percent: 69.59047619047617\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.25676793383792\n",
      "    mean_inference_ms: 0.5859965590825893\n",
      "    mean_processing_ms: 4.664464153348128\n",
      "  time_since_restore: 3149.143435716629\n",
      "  time_this_iter_s: 9.989332914352417\n",
      "  time_total_s: 3149.143435716629\n",
      "  timestamp: 1582124402\n",
      "  timesteps_since_restore: 31100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31100\n",
      "  training_iteration: 311\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         3149.14</td><td style=\"text-align: right;\">      31100</td><td style=\"text-align: right;\">   3.988</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-01-52\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9470000588148832\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1342\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 14.0\n",
      "    episode_reward_max: 1.3000000193715096\n",
      "    episode_reward_mean: 1.3000000193715096\n",
      "    episode_reward_min: 1.3000000193715096\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 91.8000663899456\n",
      "      mean_inference_ms: 0.6374775334741848\n",
      "      mean_processing_ms: 2.3694745951485947\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.019\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.156326293945312\n",
      "        critic_loss: 0.0008225534111261368\n",
      "        max_q: 12.234872817993164\n",
      "        mean_q: 12.146625518798828\n",
      "        min_q: 11.983515739440918\n",
      "        model: {}\n",
      "        td_error: 0.0016451067058369517\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31200\n",
      "    num_steps_trained: 7603200\n",
      "    num_target_updates: 31200\n",
      "    opt_peak_throughput: 63695.955\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.211\n",
      "    sample_time_ms: 81.145\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.508974358974356\n",
      "    ram_util_percent: 69.57628205128206\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26280610251177\n",
      "    mean_inference_ms: 0.5857712201343164\n",
      "    mean_processing_ms: 4.658818115205232\n",
      "  time_since_restore: 3159.0338518619537\n",
      "  time_this_iter_s: 9.890416145324707\n",
      "  time_total_s: 3159.0338518619537\n",
      "  timestamp: 1582124512\n",
      "  timesteps_since_restore: 31200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31200\n",
      "  training_iteration: 312\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         3159.03</td><td style=\"text-align: right;\">      31200</td><td style=\"text-align: right;\">   3.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-02-03\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9470000588148832\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1342\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.28273535972573\n",
      "      mean_inference_ms: 0.6348758893780808\n",
      "      mean_processing_ms: 2.2232989460797863\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.118\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.11153793334961\n",
      "        critic_loss: 0.0008552513318136334\n",
      "        max_q: 12.194753646850586\n",
      "        mean_q: 12.10335922241211\n",
      "        min_q: 11.973923683166504\n",
      "        model: {}\n",
      "        td_error: 0.0017105026636272669\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31300\n",
      "    num_steps_trained: 7628800\n",
      "    num_target_updates: 31300\n",
      "    opt_peak_throughput: 62165.538\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.329\n",
      "    sample_time_ms: 80.976\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.60666666666667\n",
      "    ram_util_percent: 69.6\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26280610251177\n",
      "    mean_inference_ms: 0.5857712201343164\n",
      "    mean_processing_ms: 4.658818115205233\n",
      "  time_since_restore: 3168.7238578796387\n",
      "  time_this_iter_s: 9.690006017684937\n",
      "  time_total_s: 3168.7238578796387\n",
      "  timestamp: 1582124523\n",
      "  timesteps_since_restore: 31300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31300\n",
      "  training_iteration: 313\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         3168.72</td><td style=\"text-align: right;\">      31300</td><td style=\"text-align: right;\">   3.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-03-52\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9470000588148832\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1342\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.70422759060003\n",
      "      mean_inference_ms: 0.6328797038026138\n",
      "      mean_processing_ms: 2.095584455999533\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.152\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.109783172607422\n",
      "        critic_loss: 0.0003393280494492501\n",
      "        max_q: 12.188863754272461\n",
      "        mean_q: 12.099753379821777\n",
      "        min_q: 12.007600784301758\n",
      "        model: {}\n",
      "        td_error: 0.0006786560988985002\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31400\n",
      "    num_steps_trained: 7654400\n",
      "    num_target_updates: 31400\n",
      "    opt_peak_throughput: 61661.814\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 13.994\n",
      "    sample_time_ms: 81.272\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.392307692307691\n",
      "    ram_util_percent: 69.63461538461539\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26280610251177\n",
      "    mean_inference_ms: 0.5857712201343164\n",
      "    mean_processing_ms: 4.658818115205233\n",
      "  time_since_restore: 3178.414083957672\n",
      "  time_this_iter_s: 9.690226078033447\n",
      "  time_total_s: 3178.414083957672\n",
      "  timestamp: 1582124632\n",
      "  timesteps_since_restore: 31400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31400\n",
      "  training_iteration: 314\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         3178.41</td><td style=\"text-align: right;\">      31400</td><td style=\"text-align: right;\">   3.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9470000588148832\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1342\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 45.0\n",
      "    episode_reward_max: 4.400000065565109\n",
      "    episode_reward_mean: 4.400000065565109\n",
      "    episode_reward_min: 4.400000065565109\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.70223145276717\n",
      "      mean_inference_ms: 0.6327129271370339\n",
      "      mean_processing_ms: 2.0961311308410884\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.902\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.08523178100586\n",
      "        critic_loss: 0.0003674519248306751\n",
      "        max_q: 12.16533374786377\n",
      "        mean_q: 12.076276779174805\n",
      "        min_q: 11.926623344421387\n",
      "        model: {}\n",
      "        td_error: 0.0007349038496613503\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31500\n",
      "    num_steps_trained: 7680000\n",
      "    num_target_updates: 31500\n",
      "    opt_peak_throughput: 65601.666\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.188\n",
      "    sample_time_ms: 81.364\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.47483870967742\n",
      "    ram_util_percent: 70.06516129032258\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26280610251177\n",
      "    mean_inference_ms: 0.5857712201343164\n",
      "    mean_processing_ms: 4.658818115205233\n",
      "  time_since_restore: 3188.1056768894196\n",
      "  time_this_iter_s: 9.691592931747437\n",
      "  time_total_s: 3188.1056768894196\n",
      "  timestamp: 1582124742\n",
      "  timesteps_since_restore: 31500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31500\n",
      "  training_iteration: 315\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         3188.11</td><td style=\"text-align: right;\">      31500</td><td style=\"text-align: right;\">   3.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-05-56\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9470000588148832\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1342\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 91.0\n",
      "    episode_reward_max: 9.00000013411045\n",
      "    episode_reward_mean: 9.00000013411045\n",
      "    episode_reward_min: 9.00000013411045\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.71918488622718\n",
      "      mean_inference_ms: 0.6324873665626771\n",
      "      mean_processing_ms: 2.0910534666298255\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.08881950378418\n",
      "        critic_loss: 0.0004368447116576135\n",
      "        max_q: 12.192564010620117\n",
      "        mean_q: 12.081167221069336\n",
      "        min_q: 11.974690437316895\n",
      "        model: {}\n",
      "        td_error: 0.0008736893651075661\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31600\n",
      "    num_steps_trained: 7705600\n",
      "    num_target_updates: 31600\n",
      "    opt_peak_throughput: 63597.864\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.236\n",
      "    sample_time_ms: 81.03\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.07\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26280610251177\n",
      "    mean_inference_ms: 0.5857712201343164\n",
      "    mean_processing_ms: 4.658818115205233\n",
      "  time_since_restore: 3197.7938809394836\n",
      "  time_this_iter_s: 9.688204050064087\n",
      "  time_total_s: 3197.7938809394836\n",
      "  timestamp: 1582124756\n",
      "  timesteps_since_restore: 31600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31600\n",
      "  training_iteration: 316\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         3197.79</td><td style=\"text-align: right;\">      31600</td><td style=\"text-align: right;\">   3.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_reward_max: 22.100000329315662\n",
      "  episode_reward_mean: 3.9470000588148832\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1342\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 167.0\n",
      "    episode_reward_max: 16.600000247359276\n",
      "    episode_reward_mean: 16.600000247359276\n",
      "    episode_reward_min: 16.600000247359276\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.7670159187572\n",
      "      mean_inference_ms: 0.6319472198146237\n",
      "      mean_processing_ms: 2.0765677249449324\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.198\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.090004920959473\n",
      "        critic_loss: 0.0004746120248455554\n",
      "        max_q: 12.1785249710083\n",
      "        mean_q: 12.080774307250977\n",
      "        min_q: 11.953733444213867\n",
      "        model: {}\n",
      "        td_error: 0.0009492239914834499\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31700\n",
      "    num_steps_trained: 7731200\n",
      "    num_target_updates: 31700\n",
      "    opt_peak_throughput: 60987.96\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.118\n",
      "    sample_time_ms: 80.091\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.11111111111111\n",
      "    ram_util_percent: 69.54444444444442\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26280610251177\n",
      "    mean_inference_ms: 0.5857712201343164\n",
      "    mean_processing_ms: 4.658818115205233\n",
      "  time_since_restore: 3207.483777284622\n",
      "  time_this_iter_s: 9.68989634513855\n",
      "  time_total_s: 3207.483777284622\n",
      "  timestamp: 1582124774\n",
      "  timesteps_since_restore: 31700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31700\n",
      "  training_iteration: 317\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         3207.48</td><td style=\"text-align: right;\">      31700</td><td style=\"text-align: right;\">   3.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-06-41\n",
      "  done: false\n",
      "  episode_len_mean: 46.58\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.558000067919493\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1343\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 34.0\n",
      "    episode_reward_max: 3.300000049173832\n",
      "    episode_reward_mean: 3.300000049173832\n",
      "    episode_reward_min: 3.300000049173832\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.76034822278982\n",
      "      mean_inference_ms: 0.6318434080470113\n",
      "      mean_processing_ms: 2.0786535897508887\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.055\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.065223693847656\n",
      "        critic_loss: 0.00046683105756528676\n",
      "        max_q: 12.140368461608887\n",
      "        mean_q: 12.055221557617188\n",
      "        min_q: 11.941040992736816\n",
      "        model: {}\n",
      "        td_error: 0.0009336621733382344\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31800\n",
      "    num_steps_trained: 7756800\n",
      "    num_target_updates: 31800\n",
      "    opt_peak_throughput: 63124.894\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.469\n",
      "    sample_time_ms: 80.919\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.118918918918919\n",
      "    ram_util_percent: 69.63243243243241\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.26617746454552\n",
      "    mean_inference_ms: 0.585646594830575\n",
      "    mean_processing_ms: 4.655321459864749\n",
      "  time_since_restore: 3217.2745871543884\n",
      "  time_this_iter_s: 9.790809869766235\n",
      "  time_total_s: 3217.2745871543884\n",
      "  timestamp: 1582124801\n",
      "  timesteps_since_restore: 31800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31800\n",
      "  training_iteration: 318\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         3217.27</td><td style=\"text-align: right;\">      31800</td><td style=\"text-align: right;\">   4.558</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 46.47\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.5470000677555795\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1344\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.74601639071318\n",
      "      mean_inference_ms: 0.6317791250942445\n",
      "      mean_processing_ms: 2.083017225508616\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.9\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.9951171875\n",
      "        critic_loss: 0.0011149811325594783\n",
      "        max_q: 12.093551635742188\n",
      "        mean_q: 11.985567092895508\n",
      "        min_q: 11.833541870117188\n",
      "        model: {}\n",
      "        td_error: 0.002229962032288313\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31900\n",
      "    num_steps_trained: 7782400\n",
      "    num_target_updates: 31900\n",
      "    opt_peak_throughput: 65634.147\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.504\n",
      "    sample_time_ms: 80.956\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.0\n",
      "    ram_util_percent: 69.70555555555558\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.2696165509679\n",
      "    mean_inference_ms: 0.5855192364915139\n",
      "    mean_processing_ms: 4.65172582406426\n",
      "  time_since_restore: 3227.0639493465424\n",
      "  time_this_iter_s: 9.78936219215393\n",
      "  time_total_s: 3227.0639493465424\n",
      "  timestamp: 1582124814\n",
      "  timesteps_since_restore: 31900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 31900\n",
      "  training_iteration: 319\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         3227.06</td><td style=\"text-align: right;\">      31900</td><td style=\"text-align: right;\">   4.547</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-07-05\n",
      "  done: false\n",
      "  episode_len_mean: 47.52\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.652000069320202\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1345\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 36.0\n",
      "    episode_reward_max: 3.500000052154064\n",
      "    episode_reward_mean: 3.500000052154064\n",
      "    episode_reward_min: 3.500000052154064\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 92.74016559039562\n",
      "      mean_inference_ms: 0.6317233939457756\n",
      "      mean_processing_ms: 2.0848086456319503\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.228\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.006674766540527\n",
      "        critic_loss: 0.0003673092578537762\n",
      "        max_q: 12.083772659301758\n",
      "        mean_q: 11.997665405273438\n",
      "        min_q: 11.873648643493652\n",
      "        model: {}\n",
      "        td_error: 0.0007346185157075524\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 7808000\n",
      "    num_target_updates: 32000\n",
      "    opt_peak_throughput: 60551.172\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.819\n",
      "    sample_time_ms: 80.396\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.549999999999997\n",
      "    ram_util_percent: 69.775\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.27302401001828\n",
      "    mean_inference_ms: 0.585391954674146\n",
      "    mean_processing_ms: 4.648159093048972\n",
      "  time_since_restore: 3236.854421377182\n",
      "  time_this_iter_s: 9.790472030639648\n",
      "  time_total_s: 3236.854421377182\n",
      "  timestamp: 1582124825\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 320\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         3236.85</td><td style=\"text-align: right;\">      32000</td><td style=\"text-align: right;\">   4.652</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-07-18\n",
      "  done: false\n",
      "  episode_len_mean: 47.52\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.652000069320202\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1345\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 93.10149106333702\n",
      "      mean_inference_ms: 0.6300221949032525\n",
      "      mean_processing_ms: 1.9751795692220326\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.074\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.040675163269043\n",
      "        critic_loss: 0.0005641431780532002\n",
      "        max_q: 12.127449035644531\n",
      "        mean_q: 12.03255558013916\n",
      "        min_q: 11.918716430664062\n",
      "        model: {}\n",
      "        td_error: 0.0011282863561064005\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32100\n",
      "    num_steps_trained: 7833600\n",
      "    num_target_updates: 32100\n",
      "    opt_peak_throughput: 62835.647\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.348\n",
      "    sample_time_ms: 81.104\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.394444444444442\n",
      "    ram_util_percent: 70.19444444444444\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.27302401001828\n",
      "    mean_inference_ms: 0.5853919546741461\n",
      "    mean_processing_ms: 4.648159093048972\n",
      "  time_since_restore: 3246.5432155132294\n",
      "  time_this_iter_s: 9.688794136047363\n",
      "  time_total_s: 3246.5432155132294\n",
      "  timestamp: 1582124838\n",
      "  timesteps_since_restore: 32100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32100\n",
      "  training_iteration: 321\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         3246.54</td><td style=\"text-align: right;\">      32100</td><td style=\"text-align: right;\">   4.652</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-09-07\n",
      "  done: false\n",
      "  episode_len_mean: 47.52\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.652000069320202\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1345\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 15.0\n",
      "    episode_reward_max: 1.4000000208616257\n",
      "    episode_reward_mean: 1.4000000208616257\n",
      "    episode_reward_min: 1.4000000208616257\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 93.08771822586873\n",
      "      mean_inference_ms: 0.6299553131963216\n",
      "      mean_processing_ms: 1.9793191615042098\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.899\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -12.00706672668457\n",
      "        critic_loss: 0.0003980792826041579\n",
      "        max_q: 12.099910736083984\n",
      "        mean_q: 11.998650550842285\n",
      "        min_q: 11.841075897216797\n",
      "        model: {}\n",
      "        td_error: 0.0007961585652083158\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32200\n",
      "    num_steps_trained: 7859200\n",
      "    num_target_updates: 32200\n",
      "    opt_peak_throughput: 65658.228\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.256\n",
      "    sample_time_ms: 81.166\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.185806451612905\n",
      "    ram_util_percent: 70.0367741935484\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.27302401001828\n",
      "    mean_inference_ms: 0.5853919546741461\n",
      "    mean_processing_ms: 4.648159093048972\n",
      "  time_since_restore: 3256.2329285144806\n",
      "  time_this_iter_s: 9.68971300125122\n",
      "  time_total_s: 3256.2329285144806\n",
      "  timestamp: 1582124947\n",
      "  timesteps_since_restore: 32200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32200\n",
      "  training_iteration: 322\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         3256.23</td><td style=\"text-align: right;\">      32200</td><td style=\"text-align: right;\">   4.652</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 47.52\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.652000069320202\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1345\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 93.41000938478228\n",
      "      mean_inference_ms: 0.628349722515928\n",
      "      mean_processing_ms: 1.8815531062891733\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.986\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.976943969726562\n",
      "        critic_loss: 0.0003808249020949006\n",
      "        max_q: 12.055899620056152\n",
      "        mean_q: 11.969106674194336\n",
      "        min_q: 11.869519233703613\n",
      "        model: {}\n",
      "        td_error: 0.0007616498041898012\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32300\n",
      "    num_steps_trained: 7884800\n",
      "    num_target_updates: 32300\n",
      "    opt_peak_throughput: 64228.996\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.638\n",
      "    sample_time_ms: 80.825\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.725\n",
      "    ram_util_percent: 70.08749999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.27302401001828\n",
      "    mean_inference_ms: 0.5853919546741461\n",
      "    mean_processing_ms: 4.648159093048972\n",
      "  time_since_restore: 3265.923801422119\n",
      "  time_this_iter_s: 9.69087290763855\n",
      "  time_total_s: 3265.923801422119\n",
      "  timestamp: 1582124958\n",
      "  timesteps_since_restore: 32300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32300\n",
      "  training_iteration: 323\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         3265.92</td><td style=\"text-align: right;\">      32300</td><td style=\"text-align: right;\">   4.652</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 50.4\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.940000073611737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1348\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 93.69921701602868\n",
      "      mean_inference_ms: 0.626439342819775\n",
      "      mean_processing_ms: 1.7940167971566836\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.951\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.936315536499023\n",
      "        critic_loss: 0.0007006382220424712\n",
      "        max_q: 12.031576156616211\n",
      "        mean_q: 11.9280424118042\n",
      "        min_q: 11.792428970336914\n",
      "        model: {}\n",
      "        td_error: 0.0014012763276696205\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32400\n",
      "    num_steps_trained: 7910400\n",
      "    num_target_updates: 32400\n",
      "    opt_peak_throughput: 64797.223\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.33\n",
      "    sample_time_ms: 79.908\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.248387096774193\n",
      "    ram_util_percent: 70.22967741935484\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.28388073668214\n",
      "    mean_inference_ms: 0.5849853374792909\n",
      "    mean_processing_ms: 4.636241094696771\n",
      "  time_since_restore: 3275.9131932258606\n",
      "  time_this_iter_s: 9.989391803741455\n",
      "  time_total_s: 3275.9131932258606\n",
      "  timestamp: 1582125068\n",
      "  timesteps_since_restore: 32400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32400\n",
      "  training_iteration: 324\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         3275.91</td><td style=\"text-align: right;\">      32400</td><td style=\"text-align: right;\">    4.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 50.4\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.940000073611737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1348\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 93.95945360602632\n",
      "      mean_inference_ms: 0.6251169856819456\n",
      "      mean_processing_ms: 1.7151211693944748\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.016\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.968284606933594\n",
      "        critic_loss: 0.0004582161200232804\n",
      "        max_q: 12.045280456542969\n",
      "        mean_q: 11.958301544189453\n",
      "        min_q: 11.792010307312012\n",
      "        model: {}\n",
      "        td_error: 0.0009164321818388999\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32500\n",
      "    num_steps_trained: 7936000\n",
      "    num_target_updates: 32500\n",
      "    opt_peak_throughput: 63744.357\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.378\n",
      "    sample_time_ms: 81.063\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.562820512820513\n",
      "    ram_util_percent: 69.42884615384615\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.28388073668212\n",
      "    mean_inference_ms: 0.5849853374792908\n",
      "    mean_processing_ms: 4.6362410946967705\n",
      "  time_since_restore: 3285.6042742729187\n",
      "  time_this_iter_s: 9.691081047058105\n",
      "  time_total_s: 3285.6042742729187\n",
      "  timestamp: 1582125177\n",
      "  timesteps_since_restore: 32500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32500\n",
      "  training_iteration: 325\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">          3285.6</td><td style=\"text-align: right;\">      32500</td><td style=\"text-align: right;\">    4.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-14-47\n",
      "  done: false\n",
      "  episode_len_mean: 50.4\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.940000073611737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1348\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 59.0\n",
      "    episode_reward_max: 5.800000086426735\n",
      "    episode_reward_mean: 5.800000086426735\n",
      "    episode_reward_min: 5.800000086426735\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 93.95868797831882\n",
      "      mean_inference_ms: 0.625051662562828\n",
      "      mean_processing_ms: 1.7153716593266215\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.1\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.921985626220703\n",
      "        critic_loss: 0.000590264389757067\n",
      "        max_q: 12.020295143127441\n",
      "        mean_q: 11.914741516113281\n",
      "        min_q: 11.748390197753906\n",
      "        model: {}\n",
      "        td_error: 0.001180528663098812\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32600\n",
      "    num_steps_trained: 7961600\n",
      "    num_target_updates: 32600\n",
      "    opt_peak_throughput: 62433.747\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.187\n",
      "    sample_time_ms: 81.206\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.863870967741933\n",
      "    ram_util_percent: 69.44193548387096\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.28388073668212\n",
      "    mean_inference_ms: 0.5849853374792908\n",
      "    mean_processing_ms: 4.6362410946967705\n",
      "  time_since_restore: 3295.295229434967\n",
      "  time_this_iter_s: 9.69095516204834\n",
      "  time_total_s: 3295.295229434967\n",
      "  timestamp: 1582125287\n",
      "  timesteps_since_restore: 32600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32600\n",
      "  training_iteration: 326\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">          3295.3</td><td style=\"text-align: right;\">      32600</td><td style=\"text-align: right;\">    4.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 50.4\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.940000073611737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1348\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 94.19431704749003\n",
      "      mean_inference_ms: 0.6234377229640521\n",
      "      mean_processing_ms: 1.6440039735795589\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.29\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.914837837219238\n",
      "        critic_loss: 0.0004136672941967845\n",
      "        max_q: 12.007784843444824\n",
      "        mean_q: 11.907342910766602\n",
      "        min_q: 11.739574432373047\n",
      "        model: {}\n",
      "        td_error: 0.0008273346466012299\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32700\n",
      "    num_steps_trained: 7987200\n",
      "    num_target_updates: 32700\n",
      "    opt_peak_throughput: 59669.561\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.569\n",
      "    sample_time_ms: 80.048\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.563636363636364\n",
      "    ram_util_percent: 69.05909090909087\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.28388073668212\n",
      "    mean_inference_ms: 0.5849853374792908\n",
      "    mean_processing_ms: 4.6362410946967705\n",
      "  time_since_restore: 3304.9903721809387\n",
      "  time_this_iter_s: 9.69514274597168\n",
      "  time_total_s: 3304.9903721809387\n",
      "  timestamp: 1582125302\n",
      "  timesteps_since_restore: 32700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32700\n",
      "  training_iteration: 327\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         3304.99</td><td style=\"text-align: right;\">      32700</td><td style=\"text-align: right;\">    4.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-16-52\n",
      "  done: false\n",
      "  episode_len_mean: 50.4\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 4.940000073611737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1348\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 34.0\n",
      "    episode_reward_max: 3.300000049173832\n",
      "    episode_reward_mean: 3.300000049173832\n",
      "    episode_reward_min: 3.300000049173832\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 94.18697196333156\n",
      "      mean_inference_ms: 0.623373427919382\n",
      "      mean_processing_ms: 1.646219804432722\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.188\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.92648696899414\n",
      "        critic_loss: 0.0003076099674217403\n",
      "        max_q: 12.021327018737793\n",
      "        mean_q: 11.917881965637207\n",
      "        min_q: 11.802209854125977\n",
      "        model: {}\n",
      "        td_error: 0.0006152198766358197\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32800\n",
      "    num_steps_trained: 8012800\n",
      "    num_target_updates: 32800\n",
      "    opt_peak_throughput: 61121.619\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.082\n",
      "    sample_time_ms: 81.101\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.059354838709679\n",
      "    ram_util_percent: 69.37290322580645\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.28388073668212\n",
      "    mean_inference_ms: 0.5849853374792908\n",
      "    mean_processing_ms: 4.6362410946967705\n",
      "  time_since_restore: 3314.6805181503296\n",
      "  time_this_iter_s: 9.69014596939087\n",
      "  time_total_s: 3314.6805181503296\n",
      "  timestamp: 1582125412\n",
      "  timesteps_since_restore: 32800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32800\n",
      "  training_iteration: 328\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         3314.68</td><td style=\"text-align: right;\">      32800</td><td style=\"text-align: right;\">    4.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 54.02\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.302000079005957\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1352\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 94.40131352822182\n",
      "      mean_inference_ms: 0.6221382749723519\n",
      "      mean_processing_ms: 1.581238494620384\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.079\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.920451164245605\n",
      "        critic_loss: 0.0003982611815445125\n",
      "        max_q: 11.992801666259766\n",
      "        mean_q: 11.90733814239502\n",
      "        min_q: 11.805201530456543\n",
      "        model: {}\n",
      "        td_error: 0.000796522363089025\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32900\n",
      "    num_steps_trained: 8038400\n",
      "    num_target_updates: 32900\n",
      "    opt_peak_throughput: 62765.492\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.143\n",
      "    sample_time_ms: 80.171\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.673684210526316\n",
      "    ram_util_percent: 69.40526315789475\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.29964185601555\n",
      "    mean_inference_ms: 0.5844047630116144\n",
      "    mean_processing_ms: 4.618367502181984\n",
      "  time_since_restore: 3324.7694380283356\n",
      "  time_this_iter_s: 10.088919878005981\n",
      "  time_total_s: 3324.7694380283356\n",
      "  timestamp: 1582125425\n",
      "  timesteps_since_restore: 32900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 32900\n",
      "  training_iteration: 329\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         3324.77</td><td style=\"text-align: right;\">      32900</td><td style=\"text-align: right;\">   5.302</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 94.59742592856317\n",
      "      mean_inference_ms: 0.6209089557372367\n",
      "      mean_processing_ms: 1.5217525337564024\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.141\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.888554573059082\n",
      "        critic_loss: 0.00029875364270992577\n",
      "        max_q: 11.982151985168457\n",
      "        mean_q: 11.879953384399414\n",
      "        min_q: 11.744948387145996\n",
      "        model: {}\n",
      "        td_error: 0.0005975072854198515\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 8064000\n",
      "    num_target_updates: 33000\n",
      "    opt_peak_throughput: 61818.726\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.127\n",
      "    sample_time_ms: 81.017\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.141935483870968\n",
      "    ram_util_percent: 69.25741935483872\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.5842585921058149\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3334.5594520568848\n",
      "  time_this_iter_s: 9.790014028549194\n",
      "  time_total_s: 3334.5594520568848\n",
      "  timestamp: 1582125535\n",
      "  timesteps_since_restore: 33000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 330\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         3334.56</td><td style=\"text-align: right;\">      33000</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-20-44\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 94.77753799831777\n",
      "      mean_inference_ms: 0.6196359609720227\n",
      "      mean_processing_ms: 1.4672032663968584\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.248\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.866158485412598\n",
      "        critic_loss: 0.0003279118682257831\n",
      "        max_q: 11.950639724731445\n",
      "        mean_q: 11.85924243927002\n",
      "        min_q: 11.73575210571289\n",
      "        model: {}\n",
      "        td_error: 0.0006558237364515662\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33100\n",
      "    num_steps_trained: 8089600\n",
      "    num_target_updates: 33100\n",
      "    opt_peak_throughput: 60265.694\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.646\n",
      "    sample_time_ms: 80.674\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.906451612903226\n",
      "    ram_util_percent: 69.74064516129032\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3344.2509260177612\n",
      "  time_this_iter_s: 9.691473960876465\n",
      "  time_total_s: 3344.2509260177612\n",
      "  timestamp: 1582125644\n",
      "  timesteps_since_restore: 33100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33100\n",
      "  training_iteration: 331\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         3344.25</td><td style=\"text-align: right;\">      33100</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 94.94283688940882\n",
      "      mean_inference_ms: 0.618961177697241\n",
      "      mean_processing_ms: 1.4170193535779727\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.168\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.872674942016602\n",
      "        critic_loss: 0.00035189941991120577\n",
      "        max_q: 11.956079483032227\n",
      "        mean_q: 11.864827156066895\n",
      "        min_q: 11.757978439331055\n",
      "        model: {}\n",
      "        td_error: 0.0007037987234070897\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33200\n",
      "    num_steps_trained: 8115200\n",
      "    num_target_updates: 33200\n",
      "    opt_peak_throughput: 61420.202\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.645\n",
      "    sample_time_ms: 80.391\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.578709677419354\n",
      "    ram_util_percent: 69.23806451612901\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3353.9398193359375\n",
      "  time_this_iter_s: 9.68889331817627\n",
      "  time_total_s: 3353.9398193359375\n",
      "  timestamp: 1582125754\n",
      "  timesteps_since_restore: 33200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33200\n",
      "  training_iteration: 332\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         3353.94</td><td style=\"text-align: right;\">      33200</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.09260210553798\n",
      "      mean_inference_ms: 0.6214160307418872\n",
      "      mean_processing_ms: 1.3705453602994817\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.225\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.828686714172363\n",
      "        critic_loss: 0.000451456755399704\n",
      "        max_q: 11.916340827941895\n",
      "        mean_q: 11.819183349609375\n",
      "        min_q: 11.638564109802246\n",
      "        model: {}\n",
      "        td_error: 0.000902913510799408\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33300\n",
      "    num_steps_trained: 8140800\n",
      "    num_target_updates: 33300\n",
      "    opt_peak_throughput: 60592.859\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.16\n",
      "    sample_time_ms: 80.913\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.697402597402599\n",
      "    ram_util_percent: 69.15844155844155\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3363.63095331192\n",
      "  time_this_iter_s: 9.691133975982666\n",
      "  time_total_s: 3363.63095331192\n",
      "  timestamp: 1582125863\n",
      "  timesteps_since_restore: 33300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33300\n",
      "  training_iteration: 333\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         3363.63</td><td style=\"text-align: right;\">      33300</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.23454049013284\n",
      "      mean_inference_ms: 0.6205314821768863\n",
      "      mean_processing_ms: 1.3275642590323102\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.132\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.843534469604492\n",
      "        critic_loss: 0.00025674773496575654\n",
      "        max_q: 11.912107467651367\n",
      "        mean_q: 11.833047866821289\n",
      "        min_q: 11.690666198730469\n",
      "        model: {}\n",
      "        td_error: 0.0005134954699315131\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33400\n",
      "    num_steps_trained: 8166400\n",
      "    num_target_updates: 33400\n",
      "    opt_peak_throughput: 61952.124\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.082\n",
      "    sample_time_ms: 81.045\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.576129032258063\n",
      "    ram_util_percent: 69.35548387096773\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3373.3204352855682\n",
      "  time_this_iter_s: 9.689481973648071\n",
      "  time_total_s: 3373.3204352855682\n",
      "  timestamp: 1582125973\n",
      "  timesteps_since_restore: 33400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33400\n",
      "  training_iteration: 334\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         3373.32</td><td style=\"text-align: right;\">      33400</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 406.0\n",
      "    episode_reward_max: 40.50000060349703\n",
      "    episode_reward_mean: 40.50000060349703\n",
      "    episode_reward_min: 40.50000060349703\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.28210507543713\n",
      "      mean_inference_ms: 0.6202705262746945\n",
      "      mean_processing_ms: 1.3130848474156505\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.052\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.821306228637695\n",
      "        critic_loss: 0.0002832336467690766\n",
      "        max_q: 11.908894538879395\n",
      "        mean_q: 11.81375789642334\n",
      "        min_q: 11.673453330993652\n",
      "        model: {}\n",
      "        td_error: 0.0005664672935381532\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33500\n",
      "    num_steps_trained: 8192000\n",
      "    num_target_updates: 33500\n",
      "    opt_peak_throughput: 63178.007\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.397\n",
      "    sample_time_ms: 81.125\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.4058064516129\n",
      "    ram_util_percent: 69.05999999999999\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3383.0126333236694\n",
      "  time_this_iter_s: 9.692198038101196\n",
      "  time_total_s: 3383.0126333236694\n",
      "  timestamp: 1582126082\n",
      "  timesteps_since_restore: 33500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33500\n",
      "  training_iteration: 335\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         3383.01</td><td style=\"text-align: right;\">      33500</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-28-52\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 49.0\n",
      "    episode_reward_max: 4.800000071525574\n",
      "    episode_reward_mean: 4.800000071525574\n",
      "    episode_reward_min: 4.800000071525574\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.27735300201306\n",
      "      mean_inference_ms: 0.6202202392139023\n",
      "      mean_processing_ms: 1.3145381083591379\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.994\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.833230972290039\n",
      "        critic_loss: 0.0005482520209625363\n",
      "        max_q: 11.911667823791504\n",
      "        mean_q: 11.823203086853027\n",
      "        min_q: 11.604283332824707\n",
      "        model: {}\n",
      "        td_error: 0.0010965039255097508\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33600\n",
      "    num_steps_trained: 8217600\n",
      "    num_target_updates: 33600\n",
      "    opt_peak_throughput: 64102.841\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.097\n",
      "    sample_time_ms: 81.224\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.343661971830986\n",
      "    ram_util_percent: 69.46901408450705\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3392.702762365341\n",
      "  time_this_iter_s: 9.690129041671753\n",
      "  time_total_s: 3392.702762365341\n",
      "  timestamp: 1582126132\n",
      "  timesteps_since_restore: 33600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33600\n",
      "  training_iteration: 336\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">          3392.7</td><td style=\"text-align: right;\">      33600</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-29-07\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.40523612664805\n",
      "      mean_inference_ms: 0.6198136756817499\n",
      "      mean_processing_ms: 1.2757025079594717\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.13\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.807899475097656\n",
      "        critic_loss: 0.0002930399205069989\n",
      "        max_q: 11.897562026977539\n",
      "        mean_q: 11.797182083129883\n",
      "        min_q: 11.638053894042969\n",
      "        model: {}\n",
      "        td_error: 0.0005860798410139978\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33700\n",
      "    num_steps_trained: 8243200\n",
      "    num_target_updates: 33700\n",
      "    opt_peak_throughput: 61981.806\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.666\n",
      "    sample_time_ms: 79.847\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.649999999999997\n",
      "    ram_util_percent: 69.81500000000001\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3402.396963119507\n",
      "  time_this_iter_s: 9.69420075416565\n",
      "  time_total_s: 3402.396963119507\n",
      "  timestamp: 1582126147\n",
      "  timesteps_since_restore: 33700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33700\n",
      "  training_iteration: 337\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">          3402.4</td><td style=\"text-align: right;\">      33700</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.52378729685842\n",
      "      mean_inference_ms: 0.6200915775043052\n",
      "      mean_processing_ms: 1.2395999735633798\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.262\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.78714370727539\n",
      "        critic_loss: 0.00027569750091060996\n",
      "        max_q: 11.875732421875\n",
      "        mean_q: 11.780485153198242\n",
      "        min_q: 11.657459259033203\n",
      "        model: {}\n",
      "        td_error: 0.0005513950018212199\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33800\n",
      "    num_steps_trained: 8268800\n",
      "    num_target_updates: 33800\n",
      "    opt_peak_throughput: 60063.76\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.957\n",
      "    sample_time_ms: 80.195\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.961935483870967\n",
      "    ram_util_percent: 69.89290322580644\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3412.08687210083\n",
      "  time_this_iter_s: 9.689908981323242\n",
      "  time_total_s: 3412.08687210083\n",
      "  timestamp: 1582126256\n",
      "  timesteps_since_restore: 33800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33800\n",
      "  training_iteration: 338\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         3412.09</td><td style=\"text-align: right;\">      33800</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-32-46\n",
      "  done: false\n",
      "  episode_len_mean: 54.65\n",
      "  episode_reward_max: 64.10000095516443\n",
      "  episode_reward_mean: 5.36500007994473\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1353\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 41.0\n",
      "    episode_reward_max: 4.000000059604645\n",
      "    episode_reward_mean: 4.000000059604645\n",
      "    episode_reward_min: 4.000000059604645\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.51791896937037\n",
      "      mean_inference_ms: 0.6200454297475922\n",
      "      mean_processing_ms: 1.2413484194272246\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.164\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.785318374633789\n",
      "        critic_loss: 0.00032184936571866274\n",
      "        max_q: 11.861082077026367\n",
      "        mean_q: 11.777963638305664\n",
      "        min_q: 11.686058044433594\n",
      "        model: {}\n",
      "        td_error: 0.0006436987314373255\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33900\n",
      "    num_steps_trained: 8294400\n",
      "    num_target_updates: 33900\n",
      "    opt_peak_throughput: 61480.691\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.23\n",
      "    sample_time_ms: 81.097\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.446451612903225\n",
      "    ram_util_percent: 69.90645161290324\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30361865887454\n",
      "    mean_inference_ms: 0.584258592105815\n",
      "    mean_processing_ms: 4.613834053203593\n",
      "  time_since_restore: 3421.777368068695\n",
      "  time_this_iter_s: 9.69049596786499\n",
      "  time_total_s: 3421.777368068695\n",
      "  timestamp: 1582126366\n",
      "  timesteps_since_restore: 33900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 33900\n",
      "  training_iteration: 339\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         3421.78</td><td style=\"text-align: right;\">      33900</td><td style=\"text-align: right;\">   5.365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-32-59\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.63034510226733\n",
      "      mean_inference_ms: 0.6189986636776942\n",
      "      mean_processing_ms: 1.2073415961735119\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.1\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.767906188964844\n",
      "        critic_loss: 0.00030847114976495504\n",
      "        max_q: 11.852396011352539\n",
      "        mean_q: 11.758899688720703\n",
      "        min_q: 11.580549240112305\n",
      "        model: {}\n",
      "        td_error: 0.0006169422995299101\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 8320000\n",
      "    num_target_updates: 34000\n",
      "    opt_peak_throughput: 62433.021\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.38\n",
      "    sample_time_ms: 80.988\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.905263157894737\n",
      "    ram_util_percent: 69.09999999999998\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038813\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3431.567202091217\n",
      "  time_this_iter_s: 9.789834022521973\n",
      "  time_total_s: 3431.567202091217\n",
      "  timestamp: 1582126379\n",
      "  timesteps_since_restore: 34000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 340\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         3431.57</td><td style=\"text-align: right;\">      34000</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.73543439771485\n",
      "      mean_inference_ms: 0.618262325548159\n",
      "      mean_processing_ms: 1.1755266342625532\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.062\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.750734329223633\n",
      "        critic_loss: 0.00030822379630990326\n",
      "        max_q: 11.835759162902832\n",
      "        mean_q: 11.741620063781738\n",
      "        min_q: 11.621722221374512\n",
      "        model: {}\n",
      "        td_error: 0.0006164476508274674\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34100\n",
      "    num_steps_trained: 8345600\n",
      "    num_target_updates: 34100\n",
      "    opt_peak_throughput: 63024.114\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.136\n",
      "    sample_time_ms: 80.259\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.403225806451612\n",
      "    ram_util_percent: 69.29677419354839\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3441.2581310272217\n",
      "  time_this_iter_s: 9.690928936004639\n",
      "  time_total_s: 3441.2581310272217\n",
      "  timestamp: 1582126489\n",
      "  timesteps_since_restore: 34100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34100\n",
      "  training_iteration: 341\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         3441.26</td><td style=\"text-align: right;\">      34100</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.83425343985868\n",
      "      mean_inference_ms: 0.6174369015293941\n",
      "      mean_processing_ms: 1.1456323807591124\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.115\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.754348754882812\n",
      "        critic_loss: 0.00032223781454376876\n",
      "        max_q: 11.832989692687988\n",
      "        mean_q: 11.744512557983398\n",
      "        min_q: 11.629199028015137\n",
      "        model: {}\n",
      "        td_error: 0.0006444756290875375\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34200\n",
      "    num_steps_trained: 8371200\n",
      "    num_target_updates: 34200\n",
      "    opt_peak_throughput: 62213.083\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.48\n",
      "    sample_time_ms: 80.796\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.820645161290324\n",
      "    ram_util_percent: 69.1432258064516\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3450.948857307434\n",
      "  time_this_iter_s: 9.690726280212402\n",
      "  time_total_s: 3450.948857307434\n",
      "  timestamp: 1582126598\n",
      "  timesteps_since_restore: 34200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34200\n",
      "  training_iteration: 342\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         3450.95</td><td style=\"text-align: right;\">      34200</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 95.92690175738072\n",
      "      mean_inference_ms: 0.6169431623332217\n",
      "      mean_processing_ms: 1.1175434476873214\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.181\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.727039337158203\n",
      "        critic_loss: 0.00029578845715150237\n",
      "        max_q: 11.815032005310059\n",
      "        mean_q: 11.718436241149902\n",
      "        min_q: 11.575772285461426\n",
      "        model: {}\n",
      "        td_error: 0.0005915769143030047\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34300\n",
      "    num_steps_trained: 8396800\n",
      "    num_target_updates: 34300\n",
      "    opt_peak_throughput: 61230.366\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.193\n",
      "    sample_time_ms: 80.019\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.570322580645165\n",
      "    ram_util_percent: 69.57354838709678\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3460.6407911777496\n",
      "  time_this_iter_s: 9.691933870315552\n",
      "  time_total_s: 3460.6407911777496\n",
      "  timestamp: 1582126708\n",
      "  timesteps_since_restore: 34300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34300\n",
      "  training_iteration: 343\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         3460.64</td><td style=\"text-align: right;\">      34300</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-40-17\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.01420062317962\n",
      "      mean_inference_ms: 0.6165160387194166\n",
      "      mean_processing_ms: 1.0910478567119746\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.051\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.690912246704102\n",
      "        critic_loss: 0.0003714428166858852\n",
      "        max_q: 11.780933380126953\n",
      "        mean_q: 11.685258865356445\n",
      "        min_q: 11.570867538452148\n",
      "        model: {}\n",
      "        td_error: 0.0007428855751641095\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34400\n",
      "    num_steps_trained: 8422400\n",
      "    num_target_updates: 34400\n",
      "    opt_peak_throughput: 63199.947\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.163\n",
      "    sample_time_ms: 81.236\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.75844155844156\n",
      "    ram_util_percent: 68.8987012987013\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3470.3314802646637\n",
      "  time_this_iter_s: 9.690689086914062\n",
      "  time_total_s: 3470.3314802646637\n",
      "  timestamp: 1582126817\n",
      "  timesteps_since_restore: 34400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34400\n",
      "  training_iteration: 344\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         3470.33</td><td style=\"text-align: right;\">      34400</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-42-07\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.09674887214001\n",
      "      mean_inference_ms: 0.6159992576699808\n",
      "      mean_processing_ms: 1.0660413100428687\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.178\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.684122085571289\n",
      "        critic_loss: 0.00042864371789619327\n",
      "        max_q: 11.809828758239746\n",
      "        mean_q: 11.674907684326172\n",
      "        min_q: 11.528026580810547\n",
      "        model: {}\n",
      "        td_error: 0.0008572873193770647\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34500\n",
      "    num_steps_trained: 8448000\n",
      "    num_target_updates: 34500\n",
      "    opt_peak_throughput: 61275.092\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.396\n",
      "    sample_time_ms: 80.805\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.827096774193548\n",
      "    ram_util_percent: 69.2658064516129\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3480.0218024253845\n",
      "  time_this_iter_s: 9.690322160720825\n",
      "  time_total_s: 3480.0218024253845\n",
      "  timestamp: 1582126927\n",
      "  timesteps_since_restore: 34500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34500\n",
      "  training_iteration: 345\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         3480.02</td><td style=\"text-align: right;\">      34500</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.17484252352736\n",
      "      mean_inference_ms: 0.6154920485278481\n",
      "      mean_processing_ms: 1.042355246220436\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.036\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.674705505371094\n",
      "        critic_loss: 0.00044657973921857774\n",
      "        max_q: 11.780899047851562\n",
      "        mean_q: 11.66677188873291\n",
      "        min_q: 11.540266036987305\n",
      "        model: {}\n",
      "        td_error: 0.0008931594784371555\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34600\n",
      "    num_steps_trained: 8473600\n",
      "    num_target_updates: 34600\n",
      "    opt_peak_throughput: 63431.427\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.225\n",
      "    sample_time_ms: 81.125\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.569032258064516\n",
      "    ram_util_percent: 69.61161290322582\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3489.713488340378\n",
      "  time_this_iter_s: 9.691685914993286\n",
      "  time_total_s: 3489.713488340378\n",
      "  timestamp: 1582127036\n",
      "  timesteps_since_restore: 34600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34600\n",
      "  training_iteration: 346\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         3489.71</td><td style=\"text-align: right;\">      34600</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.24797617235187\n",
      "      mean_inference_ms: 0.6157238656504677\n",
      "      mean_processing_ms: 1.0200828784005007\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.199\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.667858123779297\n",
      "        critic_loss: 0.00033981094020418823\n",
      "        max_q: 11.759831428527832\n",
      "        mean_q: 11.659780502319336\n",
      "        min_q: 11.496901512145996\n",
      "        model: {}\n",
      "        td_error: 0.0006796218804083765\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34700\n",
      "    num_steps_trained: 8499200\n",
      "    num_target_updates: 34700\n",
      "    opt_peak_throughput: 60965.105\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.202\n",
      "    sample_time_ms: 80.893\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.782580645161291\n",
      "    ram_util_percent: 69.32129032258064\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3499.403077363968\n",
      "  time_this_iter_s: 9.689589023590088\n",
      "  time_total_s: 3499.403077363968\n",
      "  timestamp: 1582127146\n",
      "  timesteps_since_restore: 34700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34700\n",
      "  training_iteration: 347\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">          3499.4</td><td style=\"text-align: right;\">      34700</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-47-35\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 25.0\n",
      "    episode_reward_max: 2.400000035762787\n",
      "    episode_reward_mean: 2.400000035762787\n",
      "    episode_reward_min: 2.400000035762787\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.24128563810955\n",
      "      mean_inference_ms: 0.6157059000315772\n",
      "      mean_processing_ms: 1.0220942781597764\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.002\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.655926704406738\n",
      "        critic_loss: 0.00030387379229068756\n",
      "        max_q: 11.75123119354248\n",
      "        mean_q: 11.64920425415039\n",
      "        min_q: 11.513114929199219\n",
      "        model: {}\n",
      "        td_error: 0.0006077475845813751\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34800\n",
      "    num_steps_trained: 8524800\n",
      "    num_target_updates: 34800\n",
      "    opt_peak_throughput: 63964.985\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.197\n",
      "    sample_time_ms: 80.087\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.029677419354842\n",
      "    ram_util_percent: 68.59612903225806\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3509.094036579132\n",
      "  time_this_iter_s: 9.690959215164185\n",
      "  time_total_s: 3509.094036579132\n",
      "  timestamp: 1582127255\n",
      "  timesteps_since_restore: 34800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34800\n",
      "  training_iteration: 348\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         3509.09</td><td style=\"text-align: right;\">      34800</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-47-47\n",
      "  done: false\n",
      "  episode_len_mean: 64.47\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 6.34700009457767\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.30912317518333\n",
      "      mean_inference_ms: 0.6172985570009815\n",
      "      mean_processing_ms: 1.001205537823797\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.072\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.662825584411621\n",
      "        critic_loss: 0.00033170453389175236\n",
      "        max_q: 11.751107215881348\n",
      "        mean_q: 11.654598236083984\n",
      "        min_q: 11.5455322265625\n",
      "        model: {}\n",
      "        td_error: 0.0006634090095758438\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34900\n",
      "    num_steps_trained: 8550400\n",
      "    num_target_updates: 34900\n",
      "    opt_peak_throughput: 62866.919\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.784\n",
      "    sample_time_ms: 79.523\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.41176470588235\n",
      "    ram_util_percent: 66.48823529411764\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.30813887344884\n",
      "    mean_inference_ms: 0.5840937782038814\n",
      "    mean_processing_ms: 4.6081620369288165\n",
      "  time_since_restore: 3518.786169528961\n",
      "  time_this_iter_s: 9.692132949829102\n",
      "  time_total_s: 3518.786169528961\n",
      "  timestamp: 1582127267\n",
      "  timesteps_since_restore: 34900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 34900\n",
      "  training_iteration: 349\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         3518.79</td><td style=\"text-align: right;\">      34900</td><td style=\"text-align: right;\">   6.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.3704169068652\n",
      "      mean_inference_ms: 0.6214762298567909\n",
      "      mean_processing_ms: 0.9818125170615507\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.276\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.649009704589844\n",
      "        critic_loss: 0.0002498343528714031\n",
      "        max_q: 11.740838050842285\n",
      "        mean_q: 11.641639709472656\n",
      "        min_q: 11.529261589050293\n",
      "        model: {}\n",
      "        td_error: 0.0004996687057428062\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 8576000\n",
      "    num_target_updates: 35000\n",
      "    opt_peak_throughput: 48519.301\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 21.052\n",
      "    sample_time_ms: 73.538\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.797435897435896\n",
      "    ram_util_percent: 67.61089743589743\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296743\n",
      "    mean_processing_ms: 4.601402799563274\n",
      "  time_since_restore: 3528.593212366104\n",
      "  time_this_iter_s: 9.807042837142944\n",
      "  time_total_s: 3528.593212366104\n",
      "  timestamp: 1582127377\n",
      "  timesteps_since_restore: 35000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 350\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         3528.59</td><td style=\"text-align: right;\">      35000</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.43120508516482\n",
      "      mean_inference_ms: 0.6233285610262637\n",
      "      mean_processing_ms: 0.962982261512841\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.627175331115723\n",
      "        critic_loss: 0.0002369313733652234\n",
      "        max_q: 11.725398063659668\n",
      "        mean_q: 11.616547584533691\n",
      "        min_q: 11.486377716064453\n",
      "        model: {}\n",
      "        td_error: 0.00047386277583427727\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35100\n",
      "    num_steps_trained: 8601600\n",
      "    num_target_updates: 35100\n",
      "    opt_peak_throughput: 66232.933\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.12\n",
      "    sample_time_ms: 79.921\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.65806451612903\n",
      "    ram_util_percent: 69.46645161290323\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3538.2775444984436\n",
      "  time_this_iter_s: 9.684332132339478\n",
      "  time_total_s: 3538.2775444984436\n",
      "  timestamp: 1582127486\n",
      "  timesteps_since_restore: 35100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35100\n",
      "  training_iteration: 351\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         3538.28</td><td style=\"text-align: right;\">      35100</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.48870447433349\n",
      "      mean_inference_ms: 0.6254454471819754\n",
      "      mean_processing_ms: 0.9451008262155003\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.57\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.62346076965332\n",
      "        critic_loss: 0.0002855191705748439\n",
      "        max_q: 11.720458030700684\n",
      "        mean_q: 11.613895416259766\n",
      "        min_q: 11.47341537475586\n",
      "        model: {}\n",
      "        td_error: 0.0005710383411496878\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35200\n",
      "    num_steps_trained: 8627200\n",
      "    num_target_updates: 35200\n",
      "    opt_peak_throughput: 56021.215\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.932\n",
      "    sample_time_ms: 75.378\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.58269230769231\n",
      "    ram_util_percent: 69.68333333333335\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3547.972193479538\n",
      "  time_this_iter_s: 9.69464898109436\n",
      "  time_total_s: 3547.972193479538\n",
      "  timestamp: 1582127596\n",
      "  timesteps_since_restore: 35200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35200\n",
      "  training_iteration: 352\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         3547.97</td><td style=\"text-align: right;\">      35200</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.54293144621907\n",
      "      mean_inference_ms: 0.6280412021199661\n",
      "      mean_processing_ms: 0.928147649073108\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.142\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.602241516113281\n",
      "        critic_loss: 0.0004359978483989835\n",
      "        max_q: 11.694692611694336\n",
      "        mean_q: 11.59351921081543\n",
      "        min_q: 11.461735725402832\n",
      "        model: {}\n",
      "        td_error: 0.000871995696797967\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35300\n",
      "    num_steps_trained: 8652800\n",
      "    num_target_updates: 35300\n",
      "    opt_peak_throughput: 61800.58\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.534\n",
      "    sample_time_ms: 78.748\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.388387096774196\n",
      "    ram_util_percent: 70.63419354838709\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3557.6659483909607\n",
      "  time_this_iter_s: 9.69375491142273\n",
      "  time_total_s: 3557.6659483909607\n",
      "  timestamp: 1582127705\n",
      "  timesteps_since_restore: 35300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35300\n",
      "  training_iteration: 353\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         3557.67</td><td style=\"text-align: right;\">      35300</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 32.0\n",
      "    episode_reward_max: 3.1000000461935997\n",
      "    episode_reward_mean: 3.1000000461935997\n",
      "    episode_reward_min: 3.1000000461935997\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.53713681035467\n",
      "      mean_inference_ms: 0.6281090503005949\n",
      "      mean_processing_ms: 0.9298787383111942\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.757\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.587177276611328\n",
      "        critic_loss: 0.000489570782519877\n",
      "        max_q: 11.681623458862305\n",
      "        mean_q: 11.577658653259277\n",
      "        min_q: 11.414999008178711\n",
      "        model: {}\n",
      "        td_error: 0.0009791416814550757\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35400\n",
      "    num_steps_trained: 8678400\n",
      "    num_target_updates: 35400\n",
      "    opt_peak_throughput: 53819.487\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.306\n",
      "    sample_time_ms: 75.225\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.52051282051282\n",
      "    ram_util_percent: 71.04294871794872\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3567.3739235401154\n",
      "  time_this_iter_s: 9.707975149154663\n",
      "  time_total_s: 3567.3739235401154\n",
      "  timestamp: 1582127815\n",
      "  timesteps_since_restore: 35400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35400\n",
      "  training_iteration: 354\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         3567.37</td><td style=\"text-align: right;\">      35400</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-57-07\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.58958596451971\n",
      "      mean_inference_ms: 0.6300029062052741\n",
      "      mean_processing_ms: 0.9135553693330349\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.637\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.605401039123535\n",
      "        critic_loss: 0.0003621266223490238\n",
      "        max_q: 11.694360733032227\n",
      "        mean_q: 11.599071502685547\n",
      "        min_q: 11.489622116088867\n",
      "        model: {}\n",
      "        td_error: 0.0007242533029057086\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35500\n",
      "    num_steps_trained: 8704000\n",
      "    num_target_updates: 35500\n",
      "    opt_peak_throughput: 55206.654\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 18.22\n",
      "    sample_time_ms: 76.508\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.65555555555556\n",
      "    ram_util_percent: 71.51666666666668\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3577.0600035190582\n",
      "  time_this_iter_s: 9.686079978942871\n",
      "  time_total_s: 3577.0600035190582\n",
      "  timestamp: 1582127827\n",
      "  timesteps_since_restore: 35500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35500\n",
      "  training_iteration: 355\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         3577.06</td><td style=\"text-align: right;\">      35500</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_10-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.64261150988838\n",
      "      mean_inference_ms: 0.6292797875641194\n",
      "      mean_processing_ms: 0.8976236632939856\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.006\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.622804641723633\n",
      "        critic_loss: 0.0006933388649486005\n",
      "        max_q: 11.706971168518066\n",
      "        mean_q: 11.614843368530273\n",
      "        min_q: 11.510523796081543\n",
      "        model: {}\n",
      "        td_error: 0.0013866776134818792\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35600\n",
      "    num_steps_trained: 8729600\n",
      "    num_target_updates: 35600\n",
      "    opt_peak_throughput: 63906.357\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.757\n",
      "    sample_time_ms: 80.683\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.90516129032258\n",
      "    ram_util_percent: 71.59419354838711\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3586.7508676052094\n",
      "  time_this_iter_s: 9.690864086151123\n",
      "  time_total_s: 3586.7508676052094\n",
      "  timestamp: 1582127937\n",
      "  timesteps_since_restore: 35600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35600\n",
      "  training_iteration: 356\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         3586.75</td><td style=\"text-align: right;\">      35600</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-00-46\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.69373375049338\n",
      "      mean_inference_ms: 0.6282246446831823\n",
      "      mean_processing_ms: 0.8823245931302308\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.04\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.56890869140625\n",
      "        critic_loss: 0.00027454362134449184\n",
      "        max_q: 11.658108711242676\n",
      "        mean_q: 11.561346054077148\n",
      "        min_q: 11.442474365234375\n",
      "        model: {}\n",
      "        td_error: 0.0005490872426889837\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35700\n",
      "    num_steps_trained: 8755200\n",
      "    num_target_updates: 35700\n",
      "    opt_peak_throughput: 63364.797\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.277\n",
      "    sample_time_ms: 81.202\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.179487179487179\n",
      "    ram_util_percent: 69.25128205128206\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3596.442447900772\n",
      "  time_this_iter_s: 9.691580295562744\n",
      "  time_total_s: 3596.442447900772\n",
      "  timestamp: 1582128046\n",
      "  timesteps_since_restore: 35700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35700\n",
      "  training_iteration: 357\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         3596.44</td><td style=\"text-align: right;\">      35700</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-02-36\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.74258727580785\n",
      "      mean_inference_ms: 0.6272940042100332\n",
      "      mean_processing_ms: 0.8676795543823737\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.149\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.57602310180664\n",
      "        critic_loss: 0.0002562882727943361\n",
      "        max_q: 11.669413566589355\n",
      "        mean_q: 11.567865371704102\n",
      "        min_q: 11.421756744384766\n",
      "        model: {}\n",
      "        td_error: 0.0005125765455886722\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35800\n",
      "    num_steps_trained: 8780800\n",
      "    num_target_updates: 35800\n",
      "    opt_peak_throughput: 61696.535\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.82\n",
      "    sample_time_ms: 80.143\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.297419354838713\n",
      "    ram_util_percent: 69.12645161290324\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3606.1320338249207\n",
      "  time_this_iter_s: 9.68958592414856\n",
      "  time_total_s: 3606.1320338249207\n",
      "  timestamp: 1582128156\n",
      "  timesteps_since_restore: 35800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35800\n",
      "  training_iteration: 358\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         3606.13</td><td style=\"text-align: right;\">      35800</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 74.25\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 7.325000109151006\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.78933668138588\n",
      "      mean_inference_ms: 0.6264915963673503\n",
      "      mean_processing_ms: 0.8536248153532974\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.026\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.542207717895508\n",
      "        critic_loss: 0.0003049786319024861\n",
      "        max_q: 11.641042709350586\n",
      "        mean_q: 11.533699035644531\n",
      "        min_q: 11.413238525390625\n",
      "        model: {}\n",
      "        td_error: 0.0006099572638049722\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35900\n",
      "    num_steps_trained: 8806400\n",
      "    num_target_updates: 35900\n",
      "    opt_peak_throughput: 63590.331\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.378\n",
      "    sample_time_ms: 81.01\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.864743589743593\n",
      "    ram_util_percent: 69.32948717948716\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31297371724976\n",
      "    mean_inference_ms: 0.5839139786296744\n",
      "    mean_processing_ms: 4.6014027995632745\n",
      "  time_since_restore: 3615.8225095272064\n",
      "  time_this_iter_s: 9.690475702285767\n",
      "  time_total_s: 3615.8225095272064\n",
      "  timestamp: 1582128265\n",
      "  timesteps_since_restore: 35900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 35900\n",
      "  training_iteration: 359\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         3615.82</td><td style=\"text-align: right;\">      35900</td><td style=\"text-align: right;\">   7.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-06-15\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.83406703081778\n",
      "      mean_inference_ms: 0.6258318509942032\n",
      "      mean_processing_ms: 0.8401428733097663\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.119\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.539210319519043\n",
      "        critic_loss: 0.0002989626373164356\n",
      "        max_q: 11.630720138549805\n",
      "        mean_q: 11.530767440795898\n",
      "        min_q: 11.42711067199707\n",
      "        model: {}\n",
      "        td_error: 0.0005979252746328712\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 8832000\n",
      "    num_target_updates: 36000\n",
      "    opt_peak_throughput: 62152.584\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.163\n",
      "    sample_time_ms: 81.068\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.885806451612902\n",
      "    ram_util_percent: 69.32451612903225\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339508\n",
      "  time_since_restore: 3625.6123666763306\n",
      "  time_this_iter_s: 9.789857149124146\n",
      "  time_total_s: 3625.6123666763306\n",
      "  timestamp: 1582128375\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 360\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         3625.61</td><td style=\"text-align: right;\">      36000</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-08-04\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.87700459959895\n",
      "      mean_inference_ms: 0.6252090516866933\n",
      "      mean_processing_ms: 0.8271911387080596\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.079\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.503512382507324\n",
      "        critic_loss: 0.00036169978557154536\n",
      "        max_q: 11.619297981262207\n",
      "        mean_q: 11.493345260620117\n",
      "        min_q: 11.3699951171875\n",
      "        model: {}\n",
      "        td_error: 0.0007233995711430907\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36100\n",
      "    num_steps_trained: 8857600\n",
      "    num_target_updates: 36100\n",
      "    opt_peak_throughput: 62758.155\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.346\n",
      "    sample_time_ms: 80.848\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.246451612903227\n",
      "    ram_util_percent: 69.40387096774192\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3635.3028585910797\n",
      "  time_this_iter_s: 9.690491914749146\n",
      "  time_total_s: 3635.3028585910797\n",
      "  timestamp: 1582128484\n",
      "  timesteps_since_restore: 36100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36100\n",
      "  training_iteration: 361\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">          3635.3</td><td style=\"text-align: right;\">      36100</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.91817328392993\n",
      "      mean_inference_ms: 0.6246676336895005\n",
      "      mean_processing_ms: 0.8147913953673278\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.065\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.51524543762207\n",
      "        critic_loss: 0.0002778953639790416\n",
      "        max_q: 11.607609748840332\n",
      "        mean_q: 11.506101608276367\n",
      "        min_q: 11.373894691467285\n",
      "        model: {}\n",
      "        td_error: 0.0005557907279580832\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36200\n",
      "    num_steps_trained: 8883200\n",
      "    num_target_updates: 36200\n",
      "    opt_peak_throughput: 62969.412\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.23\n",
      "    sample_time_ms: 81.078\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.821428571428571\n",
      "    ram_util_percent: 69.61363636363636\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3644.993132352829\n",
      "  time_this_iter_s: 9.690273761749268\n",
      "  time_total_s: 3644.993132352829\n",
      "  timestamp: 1582128594\n",
      "  timesteps_since_restore: 36200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36200\n",
      "  training_iteration: 362\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         3644.99</td><td style=\"text-align: right;\">      36200</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-11-43\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.95804253254438\n",
      "      mean_inference_ms: 0.623876223688553\n",
      "      mean_processing_ms: 0.802872938854193\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.149\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.497809410095215\n",
      "        critic_loss: 0.0002929585753008723\n",
      "        max_q: 11.60477352142334\n",
      "        mean_q: 11.49124526977539\n",
      "        min_q: 11.370500564575195\n",
      "        model: {}\n",
      "        td_error: 0.0005859172088094056\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36300\n",
      "    num_steps_trained: 8908800\n",
      "    num_target_updates: 36300\n",
      "    opt_peak_throughput: 61697.599\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.145\n",
      "    sample_time_ms: 80.995\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.320645161290322\n",
      "    ram_util_percent: 69.77096774193548\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3654.800522327423\n",
      "  time_this_iter_s: 9.807389974594116\n",
      "  time_total_s: 3654.800522327423\n",
      "  timestamp: 1582128703\n",
      "  timesteps_since_restore: 36300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36300\n",
      "  training_iteration: 363\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">          3654.8</td><td style=\"text-align: right;\">      36300</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-13-33\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 96.99616021499538\n",
      "      mean_inference_ms: 0.62335666994854\n",
      "      mean_processing_ms: 0.7913917479990489\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.14\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.480873107910156\n",
      "        critic_loss: 0.0002331914729438722\n",
      "        max_q: 11.590588569641113\n",
      "        mean_q: 11.47220230102539\n",
      "        min_q: 11.334258079528809\n",
      "        model: {}\n",
      "        td_error: 0.0004663829458877444\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36400\n",
      "    num_steps_trained: 8934400\n",
      "    num_target_updates: 36400\n",
      "    opt_peak_throughput: 61841.157\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.218\n",
      "    sample_time_ms: 81.07\n",
      "    update_time_ms: 0.005\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.08974358974359\n",
      "    ram_util_percent: 70.54487179487182\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3664.491997718811\n",
      "  time_this_iter_s: 9.69147539138794\n",
      "  time_total_s: 3664.491997718811\n",
      "  timestamp: 1582128813\n",
      "  timesteps_since_restore: 36400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36400\n",
      "  training_iteration: 364\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         3664.49</td><td style=\"text-align: right;\">      36400</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-15-22\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.03315680064784\n",
      "      mean_inference_ms: 0.6225998890263247\n",
      "      mean_processing_ms: 0.7803301151406854\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.088\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.456619262695312\n",
      "        critic_loss: 0.00038036241312511265\n",
      "        max_q: 11.56582260131836\n",
      "        mean_q: 11.448984146118164\n",
      "        min_q: 11.321227073669434\n",
      "        model: {}\n",
      "        td_error: 0.0007607248262502253\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36500\n",
      "    num_steps_trained: 8960000\n",
      "    num_target_updates: 36500\n",
      "    opt_peak_throughput: 62621.633\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.224\n",
      "    sample_time_ms: 80.918\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.77548387096774\n",
      "    ram_util_percent: 70.51870967741937\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3674.1812438964844\n",
      "  time_this_iter_s: 9.68924617767334\n",
      "  time_total_s: 3674.1812438964844\n",
      "  timestamp: 1582128922\n",
      "  timesteps_since_restore: 36500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36500\n",
      "  training_iteration: 365\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         3674.18</td><td style=\"text-align: right;\">      36500</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-17-12\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.06696079373945\n",
      "      mean_inference_ms: 0.623724900020216\n",
      "      mean_processing_ms: 0.7696491090529316\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.132\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.475790023803711\n",
      "        critic_loss: 0.00023737290757708251\n",
      "        max_q: 11.577999114990234\n",
      "        mean_q: 11.467231750488281\n",
      "        min_q: 11.355146408081055\n",
      "        model: {}\n",
      "        td_error: 0.00047474581515416503\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36600\n",
      "    num_steps_trained: 8985600\n",
      "    num_target_updates: 36600\n",
      "    opt_peak_throughput: 61956.771\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.194\n",
      "    sample_time_ms: 80.887\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.655769230769232\n",
      "    ram_util_percent: 70.27307692307691\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3683.8711318969727\n",
      "  time_this_iter_s: 9.689888000488281\n",
      "  time_total_s: 3683.8711318969727\n",
      "  timestamp: 1582129032\n",
      "  timesteps_since_restore: 36600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36600\n",
      "  training_iteration: 366\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         3683.87</td><td style=\"text-align: right;\">      36600</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.10125030611891\n",
      "      mean_inference_ms: 0.6231425243471725\n",
      "      mean_processing_ms: 0.7593293212011097\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.02\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.441346168518066\n",
      "        critic_loss: 0.00032875011675059795\n",
      "        max_q: 11.539668083190918\n",
      "        mean_q: 11.434650421142578\n",
      "        min_q: 11.306575775146484\n",
      "        model: {}\n",
      "        td_error: 0.0006575002917088568\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36700\n",
      "    num_steps_trained: 9011200\n",
      "    num_target_updates: 36700\n",
      "    opt_peak_throughput: 63675.18\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.264\n",
      "    sample_time_ms: 81.051\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.083870967741936\n",
      "    ram_util_percent: 70.23935483870967\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3693.562021970749\n",
      "  time_this_iter_s: 9.690890073776245\n",
      "  time_total_s: 3693.562021970749\n",
      "  timestamp: 1582129141\n",
      "  timesteps_since_restore: 36700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36700\n",
      "  training_iteration: 367\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         3693.56</td><td style=\"text-align: right;\">      36700</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-20-51\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.1342629341129\n",
      "      mean_inference_ms: 0.6226398156327755\n",
      "      mean_processing_ms: 0.7493904954440364\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.065\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.473685264587402\n",
      "        critic_loss: 0.0007140992674976587\n",
      "        max_q: 11.580551147460938\n",
      "        mean_q: 11.465085983276367\n",
      "        min_q: 11.344109535217285\n",
      "        model: {}\n",
      "        td_error: 0.0014281986514106393\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36800\n",
      "    num_steps_trained: 9036800\n",
      "    num_target_updates: 36800\n",
      "    opt_peak_throughput: 62972.736\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.464\n",
      "    sample_time_ms: 80.626\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.685714285714282\n",
      "    ram_util_percent: 70.38571428571427\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3703.2516198158264\n",
      "  time_this_iter_s: 9.689597845077515\n",
      "  time_total_s: 3703.2516198158264\n",
      "  timestamp: 1582129251\n",
      "  timesteps_since_restore: 36800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36800\n",
      "  training_iteration: 368\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         3703.25</td><td style=\"text-align: right;\">      36800</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-22-40\n",
      "  done: false\n",
      "  episode_len_mean: 83.99\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 8.299000123664737\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1356\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 37.0\n",
      "    episode_reward_max: 3.6000000536441803\n",
      "    episode_reward_mean: 3.6000000536441803\n",
      "    episode_reward_min: 3.6000000536441803\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.12988292824409\n",
      "      mean_inference_ms: 0.6226139406958151\n",
      "      mean_processing_ms: 0.750722417800433\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.167\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.418634414672852\n",
      "        critic_loss: 0.0004064065869897604\n",
      "        max_q: 11.52872085571289\n",
      "        mean_q: 11.410690307617188\n",
      "        min_q: 11.273143768310547\n",
      "        model: {}\n",
      "        td_error: 0.0008128131739795208\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36900\n",
      "    num_steps_trained: 9062400\n",
      "    num_target_updates: 36900\n",
      "    opt_peak_throughput: 61428.283\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.385\n",
      "    sample_time_ms: 80.893\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.264516129032257\n",
      "    ram_util_percent: 70.38709677419352\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.31776699989204\n",
      "    mean_inference_ms: 0.5837291568104507\n",
      "    mean_processing_ms: 4.593624316339507\n",
      "  time_since_restore: 3712.9426445961\n",
      "  time_this_iter_s: 9.691024780273438\n",
      "  time_total_s: 3712.9426445961\n",
      "  timestamp: 1582129360\n",
      "  timesteps_since_restore: 36900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 36900\n",
      "  training_iteration: 369\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         3712.94</td><td style=\"text-align: right;\">      36900</td><td style=\"text-align: right;\">   8.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-22-54\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.1617113518182\n",
      "      mean_inference_ms: 0.622226416812606\n",
      "      mean_processing_ms: 0.7411018421623801\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.054\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.42726993560791\n",
      "        critic_loss: 0.0002831367892213166\n",
      "        max_q: 11.526673316955566\n",
      "        mean_q: 11.417741775512695\n",
      "        min_q: 11.290118217468262\n",
      "        model: {}\n",
      "        td_error: 0.0005662735784426332\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 9088000\n",
      "    num_target_updates: 37000\n",
      "    opt_peak_throughput: 63141.598\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.221\n",
      "    sample_time_ms: 81.101\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.33157894736842\n",
      "    ram_util_percent: 70.40000000000002\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.584871985013089\n",
      "  time_since_restore: 3722.733252763748\n",
      "  time_this_iter_s: 9.790608167648315\n",
      "  time_total_s: 3722.733252763748\n",
      "  timestamp: 1582129374\n",
      "  timesteps_since_restore: 37000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 370\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         3722.73</td><td style=\"text-align: right;\">      37000</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-24-43\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.19253761972433\n",
      "      mean_inference_ms: 0.6217792823618682\n",
      "      mean_processing_ms: 0.7318101423209664\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.034\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.425321578979492\n",
      "        critic_loss: 0.0002976260147988796\n",
      "        max_q: 11.520528793334961\n",
      "        mean_q: 11.41844367980957\n",
      "        min_q: 11.303079605102539\n",
      "        model: {}\n",
      "        td_error: 0.0005952520878054202\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37100\n",
      "    num_steps_trained: 9113600\n",
      "    num_target_updates: 37100\n",
      "    opt_peak_throughput: 63463.669\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.467\n",
      "    sample_time_ms: 80.774\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.859354838709676\n",
      "    ram_util_percent: 70.41032258064516\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3732.4230484962463\n",
      "  time_this_iter_s: 9.689795732498169\n",
      "  time_total_s: 3732.4230484962463\n",
      "  timestamp: 1582129483\n",
      "  timesteps_since_restore: 37100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37100\n",
      "  training_iteration: 371\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         3732.42</td><td style=\"text-align: right;\">      37100</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.22231250892462\n",
      "      mean_inference_ms: 0.6213393393952763\n",
      "      mean_processing_ms: 0.72284595298242\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.41343879699707\n",
      "        critic_loss: 0.00019928242545574903\n",
      "        max_q: 11.515796661376953\n",
      "        mean_q: 11.403486251831055\n",
      "        min_q: 11.280537605285645\n",
      "        model: {}\n",
      "        td_error: 0.00039856485091149807\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37200\n",
      "    num_steps_trained: 9139200\n",
      "    num_target_updates: 37200\n",
      "    opt_peak_throughput: 61455.707\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.554\n",
      "    sample_time_ms: 80.949\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.561290322580644\n",
      "    ram_util_percent: 70.40064516129031\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3742.1162147521973\n",
      "  time_this_iter_s: 9.693166255950928\n",
      "  time_total_s: 3742.1162147521973\n",
      "  timestamp: 1582129593\n",
      "  timesteps_since_restore: 37200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37200\n",
      "  training_iteration: 372\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         3742.12</td><td style=\"text-align: right;\">      37200</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.25110936401572\n",
      "      mean_inference_ms: 0.6209335873357245\n",
      "      mean_processing_ms: 0.7141667432777992\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.079\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.410869598388672\n",
      "        critic_loss: 0.000292705197352916\n",
      "        max_q: 11.518921852111816\n",
      "        mean_q: 11.40421199798584\n",
      "        min_q: 11.26546859741211\n",
      "        model: {}\n",
      "        td_error: 0.0005854103364981711\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37300\n",
      "    num_steps_trained: 9164800\n",
      "    num_target_updates: 37300\n",
      "    opt_peak_throughput: 62758.155\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.27\n",
      "    sample_time_ms: 80.995\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.788387096774196\n",
      "    ram_util_percent: 70.57354838709678\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3751.806396484375\n",
      "  time_this_iter_s: 9.690181732177734\n",
      "  time_total_s: 3751.806396484375\n",
      "  timestamp: 1582129702\n",
      "  timesteps_since_restore: 37300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37300\n",
      "  training_iteration: 373\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         3751.81</td><td style=\"text-align: right;\">      37300</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 27.0\n",
      "    episode_reward_max: 2.600000038743019\n",
      "    episode_reward_mean: 2.600000038743019\n",
      "    episode_reward_min: 2.600000038743019\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.24660100971619\n",
      "      mean_inference_ms: 0.6209249035907947\n",
      "      mean_processing_ms: 0.7155439812290493\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.263\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.401342391967773\n",
      "        critic_loss: 0.0003747942973859608\n",
      "        max_q: 11.499467849731445\n",
      "        mean_q: 11.392953872680664\n",
      "        min_q: 11.25120735168457\n",
      "        model: {}\n",
      "        td_error: 0.0007495885365642607\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37400\n",
      "    num_steps_trained: 9190400\n",
      "    num_target_updates: 37400\n",
      "    opt_peak_throughput: 60056.705\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.348\n",
      "    sample_time_ms: 80.823\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.934415584415582\n",
      "    ram_util_percent: 70.78571428571429\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3761.496348619461\n",
      "  time_this_iter_s: 9.68995213508606\n",
      "  time_total_s: 3761.496348619461\n",
      "  timestamp: 1582129812\n",
      "  timesteps_since_restore: 37400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37400\n",
      "  training_iteration: 374\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">          3761.5</td><td style=\"text-align: right;\">      37400</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-30-24\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.27447385234801\n",
      "      mean_inference_ms: 0.620570223895003\n",
      "      mean_processing_ms: 0.7071515026334481\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.226\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.357149124145508\n",
      "        critic_loss: 0.00040217279456555843\n",
      "        max_q: 11.47213363647461\n",
      "        mean_q: 11.349449157714844\n",
      "        min_q: 11.202143669128418\n",
      "        model: {}\n",
      "        td_error: 0.0008043455891311169\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37500\n",
      "    num_steps_trained: 9216000\n",
      "    num_target_updates: 37500\n",
      "    opt_peak_throughput: 60574.059\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.574\n",
      "    sample_time_ms: 80.976\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.155555555555557\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3771.1897604465485\n",
      "  time_this_iter_s: 9.693411827087402\n",
      "  time_total_s: 3771.1897604465485\n",
      "  timestamp: 1582129824\n",
      "  timesteps_since_restore: 37500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37500\n",
      "  training_iteration: 375\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         3771.19</td><td style=\"text-align: right;\">      37500</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.3009951084891\n",
      "      mean_inference_ms: 0.6206025995780596\n",
      "      mean_processing_ms: 0.6990970375766513\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.018\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.373661041259766\n",
      "        critic_loss: 0.0002636699937283993\n",
      "        max_q: 11.486631393432617\n",
      "        mean_q: 11.365339279174805\n",
      "        min_q: 11.231743812561035\n",
      "        model: {}\n",
      "        td_error: 0.0005273399292491376\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37600\n",
      "    num_steps_trained: 9241600\n",
      "    num_target_updates: 37600\n",
      "    opt_peak_throughput: 63720.903\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.236\n",
      "    sample_time_ms: 81.257\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.485714285714288\n",
      "    ram_util_percent: 71.06428571428572\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3780.8818533420563\n",
      "  time_this_iter_s: 9.692092895507812\n",
      "  time_total_s: 3780.8818533420563\n",
      "  timestamp: 1582129933\n",
      "  timesteps_since_restore: 37600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37600\n",
      "  training_iteration: 376\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         3780.88</td><td style=\"text-align: right;\">      37600</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-34-03\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.32660056390598\n",
      "      mean_inference_ms: 0.6207330884206161\n",
      "      mean_processing_ms: 0.6913043168840479\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.107\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.361873626708984\n",
      "        critic_loss: 0.00024124246556311846\n",
      "        max_q: 11.469152450561523\n",
      "        mean_q: 11.35467529296875\n",
      "        min_q: 11.198650360107422\n",
      "        model: {}\n",
      "        td_error: 0.00048248496023006737\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37700\n",
      "    num_steps_trained: 9267200\n",
      "    num_target_updates: 37700\n",
      "    opt_peak_throughput: 62333.35\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.413\n",
      "    sample_time_ms: 80.623\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.51483870967742\n",
      "    ram_util_percent: 71.1025806451613\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3790.5712213516235\n",
      "  time_this_iter_s: 9.68936800956726\n",
      "  time_total_s: 3790.5712213516235\n",
      "  timestamp: 1582130043\n",
      "  timesteps_since_restore: 37700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37700\n",
      "  training_iteration: 377\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         3790.57</td><td style=\"text-align: right;\">      37700</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-35-52\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.35220382044841\n",
      "      mean_inference_ms: 0.6201592452097202\n",
      "      mean_processing_ms: 0.6838313229825894\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.151\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.367399215698242\n",
      "        critic_loss: 0.000423085642978549\n",
      "        max_q: 11.479779243469238\n",
      "        mean_q: 11.356090545654297\n",
      "        min_q: 11.216594696044922\n",
      "        model: {}\n",
      "        td_error: 0.000846171285957098\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37800\n",
      "    num_steps_trained: 9292800\n",
      "    num_target_updates: 37800\n",
      "    opt_peak_throughput: 61667.126\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.77\n",
      "    sample_time_ms: 80.345\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.443870967741933\n",
      "    ram_util_percent: 71.12000000000002\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3800.2625362873077\n",
      "  time_this_iter_s: 9.691314935684204\n",
      "  time_total_s: 3800.2625362873077\n",
      "  timestamp: 1582130152\n",
      "  timesteps_since_restore: 37800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37800\n",
      "  training_iteration: 378\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         3800.26</td><td style=\"text-align: right;\">      37800</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-37-42\n",
      "  done: false\n",
      "  episode_len_mean: 93.79\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 9.279000138267875\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.37543795028625\n",
      "      mean_inference_ms: 0.6207940844558989\n",
      "      mean_processing_ms: 0.6766312624252189\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.342\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.362628936767578\n",
      "        critic_loss: 0.00026574297226034105\n",
      "        max_q: 11.467944145202637\n",
      "        mean_q: 11.353212356567383\n",
      "        min_q: 11.197102546691895\n",
      "        model: {}\n",
      "        td_error: 0.0005314859445206821\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37900\n",
      "    num_steps_trained: 9318400\n",
      "    num_target_updates: 37900\n",
      "    opt_peak_throughput: 58954.369\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 17.155\n",
      "    sample_time_ms: 78.037\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.168589743589742\n",
      "    ram_util_percent: 71.27051282051282\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.3231598016223\n",
      "    mean_inference_ms: 0.5835248571717867\n",
      "    mean_processing_ms: 4.5848719850130895\n",
      "  time_since_restore: 3809.955879211426\n",
      "  time_this_iter_s: 9.693342924118042\n",
      "  time_total_s: 3809.955879211426\n",
      "  timestamp: 1582130262\n",
      "  timesteps_since_restore: 37900\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 37900\n",
      "  training_iteration: 379\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         3809.96</td><td style=\"text-align: right;\">      37900</td><td style=\"text-align: right;\">   9.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-39-31\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.3974196081474\n",
      "      mean_inference_ms: 0.622041482288227\n",
      "      mean_processing_ms: 0.6697341919458025\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 3.996\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.329133987426758\n",
      "        critic_loss: 0.00018839616677723825\n",
      "        max_q: 11.435237884521484\n",
      "        mean_q: 11.321083068847656\n",
      "        min_q: 11.150308609008789\n",
      "        model: {}\n",
      "        td_error: 0.0003767923335544765\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 9344000\n",
      "    num_target_updates: 38000\n",
      "    opt_peak_throughput: 64056.951\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.537\n",
      "    sample_time_ms: 80.759\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.703225806451613\n",
      "    ram_util_percent: 70.72709677419356\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247382\n",
      "    mean_processing_ms: 4.5752522298734055\n",
      "  time_since_restore: 3819.746294260025\n",
      "  time_this_iter_s: 9.790415048599243\n",
      "  time_total_s: 3819.746294260025\n",
      "  timestamp: 1582130371\n",
      "  timesteps_since_restore: 38000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 380\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         3819.75</td><td style=\"text-align: right;\">      38000</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-41-21\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.41894047318029\n",
      "      mean_inference_ms: 0.6231376840699144\n",
      "      mean_processing_ms: 0.66300478773279\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.266\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.322882652282715\n",
      "        critic_loss: 0.0002560216234996915\n",
      "        max_q: 11.437990188598633\n",
      "        mean_q: 11.314340591430664\n",
      "        min_q: 11.181733131408691\n",
      "        model: {}\n",
      "        td_error: 0.000512043246999383\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38100\n",
      "    num_steps_trained: 9369600\n",
      "    num_target_updates: 38100\n",
      "    opt_peak_throughput: 60012.733\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.812\n",
      "    sample_time_ms: 77.973\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.393589743589743\n",
      "    ram_util_percent: 69.08141025641027\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3829.4396171569824\n",
      "  time_this_iter_s: 9.693322896957397\n",
      "  time_total_s: 3829.4396171569824\n",
      "  timestamp: 1582130481\n",
      "  timesteps_since_restore: 38100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38100\n",
      "  training_iteration: 381\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         3829.44</td><td style=\"text-align: right;\">      38100</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-43-10\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.43972275954852\n",
      "      mean_inference_ms: 0.6244441534374088\n",
      "      mean_processing_ms: 0.6563032431605382\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.529\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.310632705688477\n",
      "        critic_loss: 0.00024069039500318468\n",
      "        max_q: 11.42032241821289\n",
      "        mean_q: 11.302494049072266\n",
      "        min_q: 11.154727935791016\n",
      "        model: {}\n",
      "        td_error: 0.00048138079000636935\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38200\n",
      "    num_steps_trained: 9395200\n",
      "    num_target_updates: 38200\n",
      "    opt_peak_throughput: 56530.579\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 19.097\n",
      "    sample_time_ms: 75.914\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.76387096774194\n",
      "    ram_util_percent: 69.49741935483873\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3839.180052280426\n",
      "  time_this_iter_s: 9.740435123443604\n",
      "  time_total_s: 3839.180052280426\n",
      "  timestamp: 1582130590\n",
      "  timesteps_since_restore: 38200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38200\n",
      "  training_iteration: 382\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         3839.18</td><td style=\"text-align: right;\">      38200</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-45-00\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.45883774325954\n",
      "      mean_inference_ms: 0.626456641758297\n",
      "      mean_processing_ms: 0.6501475498533921\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 5.525\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.304276466369629\n",
      "        critic_loss: 0.0002895853540394455\n",
      "        max_q: 11.409820556640625\n",
      "        mean_q: 11.291942596435547\n",
      "        min_q: 11.113727569580078\n",
      "        model: {}\n",
      "        td_error: 0.000579170766286552\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38300\n",
      "    num_steps_trained: 9420800\n",
      "    num_target_updates: 38300\n",
      "    opt_peak_throughput: 46338.9\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 23.352\n",
      "    sample_time_ms: 71.437\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.059615384615388\n",
      "    ram_util_percent: 68.875\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3848.899977207184\n",
      "  time_this_iter_s: 9.719924926757812\n",
      "  time_total_s: 3848.899977207184\n",
      "  timestamp: 1582130700\n",
      "  timesteps_since_restore: 38300\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38300\n",
      "  training_iteration: 383\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">          3848.9</td><td style=\"text-align: right;\">      38300</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-46-49\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.48005731604198\n",
      "      mean_inference_ms: 0.626135196993818\n",
      "      mean_processing_ms: 0.6437910970224106\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.137\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.294778823852539\n",
      "        critic_loss: 0.0003535184951033443\n",
      "        max_q: 11.405725479125977\n",
      "        mean_q: 11.285894393920898\n",
      "        min_q: 11.147116661071777\n",
      "        model: {}\n",
      "        td_error: 0.0007070369902066886\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38400\n",
      "    num_steps_trained: 9446400\n",
      "    num_target_updates: 38400\n",
      "    opt_peak_throughput: 61874.655\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 16.253\n",
      "    sample_time_ms: 79.147\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.694230769230767\n",
      "    ram_util_percent: 71.91410256410258\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3858.59375333786\n",
      "  time_this_iter_s: 9.69377613067627\n",
      "  time_total_s: 3858.59375333786\n",
      "  timestamp: 1582130809\n",
      "  timesteps_since_restore: 38400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38400\n",
      "  training_iteration: 384\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         3858.59</td><td style=\"text-align: right;\">      38400</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-48-39\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.50083986315586\n",
      "      mean_inference_ms: 0.6256986627159692\n",
      "      mean_processing_ms: 0.6375855493862266\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.155\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.292716979980469\n",
      "        critic_loss: 0.00024285001563839614\n",
      "        max_q: 11.396818161010742\n",
      "        mean_q: 11.283345222473145\n",
      "        min_q: 11.147441864013672\n",
      "        model: {}\n",
      "        td_error: 0.0004857000894844532\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38500\n",
      "    num_steps_trained: 9472000\n",
      "    num_target_updates: 38500\n",
      "    opt_peak_throughput: 61612.986\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 15.454\n",
      "    sample_time_ms: 79.804\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.95290322580645\n",
      "    ram_util_percent: 71.02516129032257\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3868.285991191864\n",
      "  time_this_iter_s: 9.692237854003906\n",
      "  time_total_s: 3868.285991191864\n",
      "  timestamp: 1582130919\n",
      "  timesteps_since_restore: 38500\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38500\n",
      "  training_iteration: 385\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         3868.29</td><td style=\"text-align: right;\">      38500</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-50-28\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.52103421729971\n",
      "      mean_inference_ms: 0.6253077094320476\n",
      "      mean_processing_ms: 0.6315394623728359\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.101\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.279114723205566\n",
      "        critic_loss: 0.00023352079733740538\n",
      "        max_q: 11.390700340270996\n",
      "        mean_q: 11.27153491973877\n",
      "        min_q: 11.126302719116211\n",
      "        model: {}\n",
      "        td_error: 0.0004670415655709803\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38600\n",
      "    num_steps_trained: 9497600\n",
      "    num_target_updates: 38600\n",
      "    opt_peak_throughput: 62425.761\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.041\n",
      "    sample_time_ms: 81.277\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.187179487179487\n",
      "    ram_util_percent: 71.87820512820511\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3877.976843357086\n",
      "  time_this_iter_s: 9.690852165222168\n",
      "  time_total_s: 3877.976843357086\n",
      "  timestamp: 1582131028\n",
      "  timesteps_since_restore: 38600\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38600\n",
      "  training_iteration: 386\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         3877.98</td><td style=\"text-align: right;\">      38600</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_myBalanceBall_4cff1564:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-02-19_11-52-18\n",
      "  done: false\n",
      "  episode_len_mean: 103.42\n",
      "  episode_reward_max: 99.900001488626\n",
      "  episode_reward_mean: 10.242000152617694\n",
      "  episode_reward_min: 1.0000000149011612\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_reward_max: 99.900001488626\n",
      "    episode_reward_mean: 99.900001488626\n",
      "    episode_reward_min: 99.900001488626\n",
      "    episodes_this_iter: 1\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 97.54015403248917\n",
      "      mean_inference_ms: 0.6253621173405904\n",
      "      mean_processing_ms: 0.6257443471750591\n",
      "  experiment_id: abdeb6248efa461ca8a859e3c98b15fe\n",
      "  experiment_tag: '0'\n",
      "  hostname: Marks-MacBook-Pro.local\n",
      "  info:\n",
      "    grad_time_ms: 4.118\n",
      "    learner:\n",
      "      default_policy:\n",
      "        actor_loss: -11.294050216674805\n",
      "        critic_loss: 0.0002789166755974293\n",
      "        max_q: 11.407363891601562\n",
      "        mean_q: 11.286369323730469\n",
      "        min_q: 11.148345947265625\n",
      "        model: {}\n",
      "        td_error: 0.0005578334094025195\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38700\n",
      "    num_steps_trained: 9523200\n",
      "    num_target_updates: 38700\n",
      "    opt_peak_throughput: 62171.297\n",
      "    opt_samples: 256.0\n",
      "    replay_time_ms: 14.185\n",
      "    sample_time_ms: 80.951\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.053548387096775\n",
      "    ram_util_percent: 71.65483870967745\n",
      "  pid: 46675\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 74.32896745732116\n",
      "    mean_inference_ms: 0.5833061396247383\n",
      "    mean_processing_ms: 4.575252229873405\n",
      "  time_since_restore: 3887.665693283081\n",
      "  time_this_iter_s: 9.688849925994873\n",
      "  time_total_s: 3887.665693283081\n",
      "  timestamp: 1582131138\n",
      "  timesteps_since_restore: 38700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 38700\n",
      "  training_iteration: 387\n",
      "  trial_id: 4cff1564\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /Users/mettinger/ray_results/SAC<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_myBalanceBall_4cff1564</td><td>RUNNING </td><td>192.168.1.12:46675</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         3887.67</td><td style=\"text-align: right;\">      38700</td><td style=\"text-align: right;\">  10.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    config={\n",
    "        \"env\": myBalanceBall,\n",
    "        \"num_workers\": 0,\n",
    "        \"eager\": False,\n",
    "        \"Q_model\": {\"hidden_activation\": \"relu\", \"hidden_layer_sizes\": [256, 256]},\n",
    "        \"policy_model\":{\"hidden_activation\": \"relu\", \"hidden_layer_sizes\": [256, 256]},\n",
    "        \"env_config\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondae85c44cab95f4015a8bcd7aebb80cc75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
